{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples\n",
    "\n",
    "[**(quick-start)**](#quick-start) Adding a `@costly()` decorator to your function automatically adds the arguments `simulate: bool` and `cost_log: costly.Costlog` to it (and also `description: list[str]` which is useful for tracing the sources of costs in the breakdown).\n",
    "\n",
    "By default, this uses [`LLM_Simulator_Faker.simulate_llm_call()`](costly/simulators/llm_simulator_faker.py) to simulate your function and [`LLM_API_Estimation.get_cost_real()`](costly/estimators/llm_api_estimation.py) as the estimator -- see [**(assumptions)**](#assumptions) for some brief comments on how these work by default. \n",
    "\n",
    "You can also use your own simulator and estimator, as shown in Example [**(customators)**](#customators). In particular you can subclass the existing simulator and estimator objects.\n",
    "\n",
    "The default simulator and estimator assume that your function takes arguments `input_string: str, model: str, response_model: str | BaseModel)`, and supplies these arguments to the simulator and estimator. Regardless of which simulator and estimator you use, if your function does not take the same arguments as them (e.g. if your function adds a system prompt to `input_string`, or you use a different naming system for `model`s from that used in `LLM_API_Estimation.PRICES`), you can pass parameter mappings as shown in Example [**(param-mappings)**](#param-mappings). \n",
    "\n",
    "In particular see the example parameter mappings for [**(messages-and-instructor)**](#messages-and-instructor).\n",
    "\n",
    "For more accurate cost tracking, you might want to calculate costs within the function, e.g. use info returned by the API call itself. For this, your function must response a `CostlyResponse` object as shown in Example [**(costly-response)**](#costly-response). In usage, your function will still return the normal output contained in `CostlyResponse.output`, and everything else will be passed to the simulator and estimator.\n",
    "\n",
    "Finally, see [**(costlog-notes)**](#costlog-notes) for some notes on the `Costlog` class itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## quick-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abhim\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\requests\\__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.18) or chardet (5.2.0)/charset_normalizer (2.0.10) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n",
      "c:\\Users\\abhim\\Google Drive\\Gittable\\Code\\costly\\costly\\estimators\\llm_api_estimation.py:192: UserWarning: messages_to_input_tokens: gpt-4 may update over time. Returning num tokens assuming gpt-4-0613.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly! Here is the standard \"Lorem ipsum\" text:\n",
      "\n",
      "```\n",
      "Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\n",
      "```\n",
      "\n",
      "Feel free to ask if you need more variations or specific lengths! \n",
      "---\n",
      " Responsibility heart Republican herself. Exactly catch country red. Population until listen rest.\n",
      "Quite bill result group many. Break husband voice.\n",
      "Respond represent as economy ball policy. Decade establish visit build table.\n",
      "Land brother red yeah example itself. Stage meet defense stage. Firm expert dog peace oil wait coach. Shake beautiful development high particularly.\n",
      "Employee project or allow. Great a another democratic. Board officer college tax deal leg must why.\n",
      "Conference standard big fine scientist type impact. Modern few style brother artist myself. Nearly nor draw shake.\n",
      "Take stage house camera political. Baby really loss wife performance stage eight.\n",
      "Few break little between sort. Guess listen price behind force specific street create. Whole people end medical its nice.\n",
      "News within common he its. Artist while less PM lose want picture. Public answer method house.\n",
      "Church nation sometimes exist serious almost today. Center environment somebody place himself finally himself.\n",
      "Common force three should.\n",
      "Lay cost glass how type trial. Mention national cause when.\n",
      "Relate by down west human. Work direction improve start no foot. Admit threat know produce door.\n",
      "Generation bar information ok particularly region. Mind both hour turn onto garden. Sure outside item voice.\n",
      "Which staff participant various season wait. Age need which bad easy and example. Build blood near center even single similar take.\n",
      "Wrong consumer audience product product improve position. Show determine consumer another. Stop certain sell kitchen avoid agency thus.\n",
      "Child page cover serve. Identify sit wall claim million stock up. By true back relate focus.\n",
      "Marriage agree reality. Join serve next care mission. Top task game back. Agree in nearly media.\n",
      "Imagine common now send return future. Anything economic room hospital trouble natural. Read skin believe now guess themselves.\n",
      "Modern sometimes kind the. Decide wife above item brother join father. South well street rise.\n",
      "Up run carry school sea church decide. Paper discussion thank so. Through rule reflect model like fight.\n",
      "High meet source describe thought network. Grow just policy push part success.\n",
      "Decision according trial drive. Nothing their old film Mrs crime environment. Collection none rock recognize another charge throughout do.\n",
      "Never suddenly back bring blood during. Send identify beat among Democrat avoid. Likely when moment try side up.\n",
      "Various set its another discuss. Worry east despite easy any machine. Blood value beautiful argue education either national.\n",
      "Small poor could life air scene garden. Eat author hour game.\n",
      "More view foreign glass.\n",
      "Appear consumer expert these deep cultural next. \n",
      "---\n",
      " {'time_min': 1.376594200002728, 'cost_min': 7.86e-05, 'cost_max': 0.0013073999999999998, 'time_max': 19.808594200002727, 'calls': 2.0} \n",
      "---\n",
      " {'cost_min': 7.68e-05, 'cost_max': 7.68e-05, 'time_min': 1.376594200002728, 'time_max': 1.376594200002728, 'input_tokens': 12, 'output_tokens': 125, 'output_tokens_min': 125, 'output_tokens_max': 125, 'calls': 1, 'model': 'gpt-4o-mini', 'simulated': False, 'input_string': None, 'messages': [{'content': 'Write the Lorem ipsum text', 'role': 'user'}], 'output_string': 'Certainly! Here is the standard \"Lorem ipsum\" text:\\n\\n```\\nLorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\\n```\\n\\nFeel free to ask if you need more variations or specific lengths!', 'description': None} \n",
      "---\n",
      " {'cost_min': 1.8e-06, 'cost_max': 0.0012305999999999999, 'time_min': 0.0, 'time_max': 18.432, 'input_tokens': 12, 'output_tokens_min': 0, 'output_tokens_max': 2048, 'calls': 1, 'model': 'gpt-4o-mini', 'simulated': True, 'input_string': None, 'messages': [{'content': 'Write the Lorem ipsum text', 'role': 'user'}], 'output_string': None, 'description': None}\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from costly import Costlog, costly\n",
    "\n",
    "\n",
    "@costly()\n",
    "def chatgpt(messages: dict, model: str) -> str:\n",
    "    from openai import OpenAI\n",
    "\n",
    "    client = OpenAI()\n",
    "    response = client.chat.completions.create(\n",
    "        model=model, messages=messages,\n",
    "    )\n",
    "    output_string = response.choices[0].message.content\n",
    "    return output_string\n",
    "\n",
    "\n",
    "cl = Costlog()\n",
    "x = chatgpt(\n",
    "    messages=[{\"content\": \"Write the Lorem ipsum text\", \"role\": \"user\"}],\n",
    "    model=\"gpt-4o-mini\",\n",
    "    cost_log=cl,\n",
    "    simulate=False,\n",
    ")\n",
    "y = chatgpt(\n",
    "    messages=[{\"content\": \"Write the Lorem ipsum text\", \"role\": \"user\"}],\n",
    "    model=\"gpt-4o-mini\",\n",
    "    cost_log=cl,\n",
    "    simulate=True,\n",
    ")\n",
    "print(\n",
    "    x,\n",
    "    \"\\n---\\n\",\n",
    "    y,\n",
    "    \"\\n---\\n\",\n",
    "    cl.totals,\n",
    "    \"\\n---\\n\",\n",
    "    cl.items[0],\n",
    "    \"\\n---\\n\",\n",
    "    cl.items[1],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's basically what's happening under the hood, courtesy of `costly.decorator.costly`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from costly.simulators.llm_simulator_faker import LLM_Simulator_Faker\n",
    "from costly.estimators.llm_api_estimation import LLM_API_Estimation\n",
    "\n",
    "def _chatgpt(messages: list[dict[str, str]], model: str) -> str:\n",
    "    from openai import OpenAI\n",
    "    client = OpenAI()\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages\n",
    "    )\n",
    "    output_string = response.choices[0].message.content\n",
    "    return output_string\n",
    "\n",
    "def chatgpt(messages: list[dict[str,str]], model: str, cost_log: Costlog=None, simulate: bool=False, description: list[str]=None) -> str:\n",
    "    if simulate:\n",
    "        return LLM_Simulator_Faker.simulate_llm_call(\n",
    "            messages=messages,\n",
    "            model=model,\n",
    "            response_model=str,\n",
    "            cost_log=cost_log,\n",
    "            description=description,\n",
    "        )\n",
    "    if cost_log is not None:\n",
    "        with cost_log.new_item() as (item, timer):\n",
    "            output_string = _chatgpt(messages, model)\n",
    "            cost_item = LLM_API_Estimation.get_cost_real(\n",
    "                model=model,\n",
    "                input_string=messages,\n",
    "                output_string=output_string,\n",
    "                description=description,\n",
    "                timer=timer(),\n",
    "            )\n",
    "            item.update(cost_item)\n",
    "    else:\n",
    "        output_string = _chatgpt(input_string, model)\n",
    "    return output_string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## assumptions\n",
    "\n",
    "`LLM_Simulator_Faker`, when producing text, produces text of about `600 * 4.5` characters.\n",
    "\n",
    "Generally we assume that 1 token is about 4.5 characters. Though actual token estimation does use `tiktoken` (unless you subclass `LLM_API_Estimation` and set `tokenize=_tokenize_rough`).\n",
    "\n",
    "Generally we assume, for cost estimation, that output tokens are in the range `[0, 2048]`, and the min and max are computed accordingly. As a rule of thumb for complex projects the true value tends to be about 1/3 the way through, and for projects that receive quite short responses it would be much lower.\n",
    "\n",
    "`LLM_API_Estimation.get_cost_real()` uses the `PRICES` dict to get the cost of a (real of simulated) API call, and to estimate time for a simulated API call. By default the standard OpenAI and Anthropic models are included; you can add more via subclassing.\n",
    "\n",
    "All of this can be overriden by subclassing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## customators\n",
    "\n",
    "The defualt decorator behaviour is\n",
    "\n",
    "```python\n",
    "@costly(\n",
    "    simulator=LLM_Simulator_Faker.simulate_llm_call,\n",
    "    estimator=LLM_API_Estimation.get_cost_real,\n",
    ")\n",
    "```\n",
    "\n",
    "These functions can be replaced by your own custom functions -- the best way to do this is probably to subclass the respective class.\n",
    "\n",
    "[`costly.simulators.llm_simulator_faker`](costly/simulators/llm_simulator_faker.py) has some examples of how to subclass it. One obvious reason to subclass it is to have custom simulating functions for the types you are interested in. Although the default class \"works\" for any Pydantic basemodel etc., you might want to have a custom function -- e.g. if a value needs to be within a certain range, or if its distribution is not uniform, or if you want to use examples from your data, etc.\n",
    "\n",
    "Again [`costly.estimators.llm_api_estimation`](costly/estimators/llm_api_estimation.py) has some examples of how to subclass it. The most obvious reason would be to add prices for other models we don't have listed (right now it's just OpenAI and Anthropic). The `PRICES` dict is like this:\n",
    "\n",
    "```python\n",
    "class LLM_API_Estimation:\n",
    "\n",
    "    PRICES = {\n",
    "        \"gpt-4o\": {\n",
    "            \"input_tokens\": 5.0e-6,\n",
    "            \"output_tokens\": 15.0e-6,\n",
    "            \"time\": 18e-3,\n",
    "        },\n",
    "        \"gpt-4o-mini\": {\n",
    "            \"input_tokens\": 0.15e-6,\n",
    "            \"output_tokens\": 0.6e-6,\n",
    "            \"time\": 9e-3,\n",
    "        },\n",
    "        ...\n",
    "    }\n",
    "```\n",
    "\n",
    "Something like this would be quite natural:\n",
    "\n",
    "```python\n",
    "class My_Estimation(LLM_API_Estimation):\n",
    "    PRICES = LLM_API_Estimation.PRICES | {\"my_model\": LLM_API_Estimation.PRICES[\"gpt-4o\"]}\n",
    "```\n",
    "\n",
    "Note that `LLM_API_Estimation` _can_ handle things like `gpt-4o-2024-05-13` etc. because it `LLM_API_Estimation.get_model()` gets the longest prefix matching model name in `PRICES`. \n",
    "\n",
    "\n",
    "The default simulator and estimator have type signatures:\n",
    "\n",
    "```python\n",
    "class LLM_Simulator_Faker:\n",
    "\n",
    "    @staticmethod\n",
    "    def simulate_llm_call(\n",
    "        input_string: str = None,  # must supply at least one of input_string, input_tokens, messages\n",
    "        input_tokens: int = None,\n",
    "        messages: list[dict[str, str]] = None,\n",
    "        model: str = None,\n",
    "        response_model: type = str,\n",
    "        cost_log: Costlog = None,\n",
    "        description: list[str] = None,\n",
    "    ) -> str | Any:\n",
    "        ...\n",
    "\n",
    "class LLM_API_Estimation:\n",
    "\n",
    "    @staticmethod\n",
    "    def get_cost_real(\n",
    "        model: str,\n",
    "        input_tokens: int = None,\n",
    "        output_tokens: int = None,\n",
    "        input_string: str = None,\n",
    "        messages: list[dict[str, str]] = None,\n",
    "        output_string: str = None,\n",
    "        timer: float = None,\n",
    "        **kwargs,\n",
    "    ) -> dict[str, float]:\n",
    "        ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## param-mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abhim\\Google Drive\\Gittable\\Code\\costly\\costly\\estimators\\llm_api_estimation.py:185: UserWarning: messages_to_input_tokens: gpt-3.5-turbo may update over time. Returning num tokens assuming gpt-3.5-turbo-0613.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cost_min': 4e-06, 'cost_max': 0.003076, 'time_min': 0.0, 'time_max': 73.728, 'input_tokens': 8, 'output_tokens_min': 0, 'output_tokens_max': 2048, 'calls': 1, 'model': 'gpt-3.5-turbo', 'simulated': True, 'input_string': None, 'messages': [{'role': 'user', 'content': 'Hello'}], 'output_string': None, 'description': None} \n",
      "---\n",
      " {'cost_min': 1.75e-05, 'cost_max': 1.75e-05, 'time_min': 0.6297439000045415, 'time_max': 0.6297439000045415, 'input_tokens': 8, 'output_tokens': 9, 'output_tokens_min': 9, 'output_tokens_max': 9, 'calls': 1, 'model': 'gpt-3.5-turbo', 'simulated': False, 'input_string': None, 'messages': [{'role': 'user', 'content': 'Hello'}], 'output_string': 'Hello! How can I assist you today?', 'description': None}\n"
     ]
    }
   ],
   "source": [
    "from costly import Costlog, costly\n",
    "from costly.simulators.llm_simulator_faker import LLM_Simulator_Faker\n",
    "from costly.estimators.llm_api_estimation import LLM_API_Estimation\n",
    "\n",
    "\n",
    "@costly(\n",
    "    messages=(lambda kwargs: [{\"role\": \"user\", \"content\": kwargs[\"prompt\"]}]),\n",
    "    model=(lambda kwargs: kwargs[\"model_name\"]),\n",
    ")\n",
    "def chatgpt2(prompt: str, model_name: str) -> str:\n",
    "    from openai import OpenAI\n",
    "\n",
    "    client = OpenAI()\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name, messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    output_string = response.choices[0].message.content\n",
    "    return output_string\n",
    "\n",
    "\n",
    "cl = Costlog()\n",
    "chatgpt2(prompt=\"Hello\", model_name=\"gpt-3.5-turbo\", simulate=True, cost_log=cl)\n",
    "chatgpt2(prompt=\"Hello\", model_name=\"gpt-3.5-turbo\", simulate=False, cost_log=cl)\n",
    "print(cl.items[0],'\\n---\\n',cl.items[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually for the specific case of just remapping variables you can simply do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abhim\\Google Drive\\Gittable\\Code\\costly\\costly\\estimators\\llm_api_estimation.py:185: UserWarning: messages_to_input_tokens: gpt-3.5-turbo may update over time. Returning num tokens assuming gpt-3.5-turbo-0613.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cost_min': 4e-06, 'cost_max': 0.003076, 'time_min': 0.0, 'time_max': 73.728, 'input_tokens': 8, 'output_tokens_min': 0, 'output_tokens_max': 2048, 'calls': 1, 'model': 'gpt-3.5-turbo', 'simulated': True, 'input_string': None, 'messages': [{'content': 'Hello', 'role': 'user'}], 'output_string': None, 'description': None} \n",
      "---\n",
      " {'cost_min': 1.75e-05, 'cost_max': 1.75e-05, 'time_min': 0.6508535999746528, 'time_max': 0.6508535999746528, 'input_tokens': 8, 'output_tokens': 9, 'output_tokens_min': 9, 'output_tokens_max': 9, 'calls': 1, 'model': 'gpt-3.5-turbo', 'simulated': False, 'input_string': None, 'messages': [{'content': 'Hello', 'role': 'user'}], 'output_string': 'Hello! How can I assist you today?', 'description': None}\n"
     ]
    }
   ],
   "source": [
    "from costly import Costlog, costly\n",
    "from costly.simulators.llm_simulator_faker import LLM_Simulator_Faker\n",
    "from costly.estimators.llm_api_estimation import LLM_API_Estimation\n",
    "\n",
    "\n",
    "@costly(messages=\"message_history\", model=\"model_name\")\n",
    "def chatgpt2(message_history: list[dict[str, str]], model_name: str) -> str:\n",
    "    from openai import OpenAI\n",
    "\n",
    "    client = OpenAI()\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name, messages=message_history\n",
    "    )\n",
    "    output_string = response.choices[0].message.content\n",
    "    return output_string\n",
    "\n",
    "\n",
    "cl = Costlog(\n",
    "    totals_keys={\n",
    "        \"cost_min\",\n",
    "        \"cost_max\",\n",
    "        \"time_min\",\n",
    "        \"time_max\",\n",
    "        \"calls\",\n",
    "        \"input_tokens\",\n",
    "        \"output_tokens_min\",\n",
    "        \"output_tokens_max\",\n",
    "    }\n",
    ")\n",
    "mh = [{\"content\": \"Hello\", \"role\": \"user\"}]\n",
    "chatgpt2(message_history=mh, model_name=\"gpt-3.5-turbo\", simulate=True, cost_log=cl)\n",
    "chatgpt2(message_history=mh, model_name=\"gpt-3.5-turbo\", simulate=False, cost_log=cl)\n",
    "print(cl.items[0],'\\n---\\n',cl.items[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## messages-and-instructor\n",
    "\n",
    "More examples of parameter mappings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'system_prompt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 31\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output_string\n\u001b[0;32m     30\u001b[0m cl \u001b[38;5;241m=\u001b[39m Costlog()\n\u001b[1;32m---> 31\u001b[0m \u001b[43mchatgpt_messages\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHey\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-4o-mini\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43msimulate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcost_log\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m chatgpt_messages(\n\u001b[0;32m     38\u001b[0m     prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHey\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     39\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-4o-mini\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     40\u001b[0m     simulate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     41\u001b[0m     cost_log\u001b[38;5;241m=\u001b[39mcl,\n\u001b[0;32m     42\u001b[0m )\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(cl\u001b[38;5;241m.\u001b[39mitems[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m---\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, cl\u001b[38;5;241m.\u001b[39mitems[\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\abhim\\Google Drive\\Gittable\\Code\\costly\\costly\\decorator.py:29\u001b[0m, in \u001b[0;36mcostly.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     26\u001b[0m simulate \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimulate\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     27\u001b[0m description \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdescription\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m---> 29\u001b[0m costly_kwargs \u001b[38;5;241m=\u001b[39m kwargs \u001b[38;5;241m|\u001b[39m \u001b[43m{\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapping\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mcallable\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmapping\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmapping\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapping\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mparam_mappings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m\u001b[49m\u001b[43m}\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m simulate:\n\u001b[0;32m     35\u001b[0m     simulator_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     36\u001b[0m         k: v\n\u001b[0;32m     37\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m costly_kwargs\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m     38\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m signature(simulator)\u001b[38;5;241m.\u001b[39mparameters\n\u001b[0;32m     39\u001b[0m     } \u001b[38;5;241m|\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcost_log\u001b[39m\u001b[38;5;124m\"\u001b[39m: cost_log, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdescription\u001b[39m\u001b[38;5;124m\"\u001b[39m: description}\n",
      "File \u001b[1;32mc:\\Users\\abhim\\Google Drive\\Gittable\\Code\\costly\\costly\\decorator.py:30\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     26\u001b[0m simulate \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimulate\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     27\u001b[0m description \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdescription\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     29\u001b[0m costly_kwargs \u001b[38;5;241m=\u001b[39m kwargs \u001b[38;5;241m|\u001b[39m {\n\u001b[1;32m---> 30\u001b[0m     key: \u001b[43mmapping\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(mapping) \u001b[38;5;28;01melse\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(mapping)\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, mapping \u001b[38;5;129;01min\u001b[39;00m param_mappings\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m     32\u001b[0m }\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m simulate:\n\u001b[0;32m     35\u001b[0m     simulator_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     36\u001b[0m         k: v\n\u001b[0;32m     37\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m costly_kwargs\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m     38\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m signature(simulator)\u001b[38;5;241m.\u001b[39mparameters\n\u001b[0;32m     39\u001b[0m     } \u001b[38;5;241m|\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcost_log\u001b[39m\u001b[38;5;124m\"\u001b[39m: cost_log, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdescription\u001b[39m\u001b[38;5;124m\"\u001b[39m: description}\n",
      "Cell \u001b[1;32mIn[7], line 9\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(kwargs)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcostly\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msimulators\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllm_simulator_faker\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LLM_Simulator_Faker\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcostly\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mestimators\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllm_api_estimation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LLM_API_Estimation\n\u001b[0;32m      6\u001b[0m \u001b[38;5;129m@costly\u001b[39m(\n\u001b[0;32m      7\u001b[0m     input_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m kwargs: LLM_API_Estimation\u001b[38;5;241m.\u001b[39mprompt_to_input_tokens(\n\u001b[0;32m      8\u001b[0m         prompt\u001b[38;5;241m=\u001b[39mkwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m----> 9\u001b[0m         system_prompt\u001b[38;5;241m=\u001b[39m\u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msystem_prompt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m,\n\u001b[0;32m     10\u001b[0m         model\u001b[38;5;241m=\u001b[39mkwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     11\u001b[0m     ),\n\u001b[0;32m     12\u001b[0m )\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchatgpt_messages\u001b[39m(\n\u001b[0;32m     14\u001b[0m     prompt: \u001b[38;5;28mstr\u001b[39m, model: \u001b[38;5;28mstr\u001b[39m, system_prompt: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are a helpful assistant.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     15\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n\u001b[0;32m     18\u001b[0m     client \u001b[38;5;241m=\u001b[39m OpenAI()\n",
      "\u001b[1;31mKeyError\u001b[0m: 'system_prompt'"
     ]
    }
   ],
   "source": [
    "from costly import Costlog, costly\n",
    "from costly.simulators.llm_simulator_faker import LLM_Simulator_Faker\n",
    "from costly.estimators.llm_api_estimation import LLM_API_Estimation\n",
    "\n",
    "\n",
    "@costly(\n",
    "    input_tokens=lambda kwargs: LLM_API_Estimation.prompt_to_input_tokens(\n",
    "        prompt=kwargs[\"prompt\"],\n",
    "        system_prompt=kwargs[\"system_prompt\"],\n",
    "        model=kwargs[\"model\"],\n",
    "    ),\n",
    ")\n",
    "def chatgpt_messages(\n",
    "    prompt: str, model: str, system_prompt: str = \"You are a helpful assistant.\"\n",
    ") -> str:\n",
    "    from openai import OpenAI\n",
    "\n",
    "    client = OpenAI()\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"content\": prompt, \"role\": \"user\"},\n",
    "            {\"content\": system_prompt, \"role\": \"system\"},\n",
    "        ],\n",
    "    )\n",
    "    output_string = response.choices[0].message.content\n",
    "    return output_string\n",
    "\n",
    "\n",
    "cl = Costlog()\n",
    "chatgpt_messages(\n",
    "    prompt=\"Hey\",\n",
    "    model=\"gpt-4o-mini\",\n",
    "    simulate=True,\n",
    "    cost_log=cl,\n",
    ")\n",
    "chatgpt_messages(\n",
    "    prompt=\"Hey\",\n",
    "    model=\"gpt-4o-mini\",\n",
    "    simulate=False,\n",
    "    cost_log=cl,\n",
    ")\n",
    "print(cl.items[0], \"\\n---\\n\", cl.items[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-05 13:48:13,773 DEBUG instructor: Instructor Request: mode.value='tool_call', response_model=<class '__main__.PersonInfo'>, new_kwargs={'messages': [{'content': 'Hey', 'role': 'user'}], 'model': 'gpt-3.5-turbo', 'tools': [{'type': 'function', 'function': {'name': 'PersonInfo', 'description': 'Correctly extracted `PersonInfo` with all the required parameters with correct types', 'parameters': {'properties': {'name': {'title': 'Name', 'type': 'string'}, 'age': {'title': 'Age', 'type': 'integer'}}, 'required': ['age', 'name'], 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'PersonInfo'}}}\n",
      "2024-09-05 13:48:13,773 DEBUG instructor: max_retries: 3\n",
      "2024-09-05 13:48:13,816 DEBUG instructor: Patching `client.chat.completions.create` with mode=<Mode.TOOLS: 'tool_call'>\n",
      "2024-09-05 13:48:13,820 DEBUG instructor: Instructor Request: mode.value='tool_call', response_model=<class '__main__.PersonInfo'>, new_kwargs={'messages': [{'content': 'Hey', 'role': 'user'}], 'model': 'gpt-3.5-turbo', 'tools': [{'type': 'function', 'function': {'name': 'PersonInfo', 'description': 'Correctly extracted `PersonInfo` with all the required parameters with correct types', 'parameters': {'properties': {'name': {'title': 'Name', 'type': 'string'}, 'age': {'title': 'Age', 'type': 'integer'}}, 'required': ['age', 'name'], 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'PersonInfo'}}}\n",
      "2024-09-05 13:48:13,821 DEBUG instructor: max_retries: 3\n",
      "2024-09-05 13:48:13,845 DEBUG instructor: Instructor Request: mode.value='tool_call', response_model=<class '__main__.PersonInfo'>, new_kwargs={'messages': [{'role': 'user', 'content': 'Hey'}], 'model': 'gpt-3.5-turbo', 'tools': [{'type': 'function', 'function': {'name': 'PersonInfo', 'description': 'Correctly extracted `PersonInfo` with all the required parameters with correct types', 'parameters': {'properties': {'name': {'title': 'Name', 'type': 'string'}, 'age': {'title': 'Age', 'type': 'integer'}}, 'required': ['age', 'name'], 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'PersonInfo'}}}\n",
      "2024-09-05 13:48:13,846 DEBUG instructor: max_retries: 3\n",
      "2024-09-05 13:48:14,514 DEBUG instructor: Instructor Raw Response: ChatCompletion(id='chatcmpl-A46CEMls7YIQ1JA8RvNiJJoFk7hEX', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_03gGs2iuIEPoF6oBUSDTwhRF', function=Function(arguments='{\"name\":\"Alice\",\"age\":30}', name='PersonInfo'), type='function')], refusal=None))], created=1725540494, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=9, prompt_tokens=75, total_tokens=84))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cost_min': 2.05e-05, 'cost_max': 0.0030925, 'time_min': 0.0, 'time_max': 73.728, 'input_tokens': 41, 'output_tokens_min': 0, 'output_tokens_max': 2048, 'calls': 1, 'model': 'gpt-3.5-turbo', 'simulated': True, 'input_string': \"HeyPersonInfoCorrectly extracted `PersonInfo` with all the required parameters with correct typesdict_values(['Name', 'string'])dict_values(['Age', 'integer'])\", 'output_string': None, 'description': None} \n",
      "---\n",
      " {'cost_min': 2.05e-05, 'cost_max': 2.05e-05, 'time_min': 0.6806791000126395, 'time_max': 0.6806791000126395, 'input_tokens': 41, 'output_tokens': 0, 'output_tokens_min': 0, 'output_tokens_max': 0, 'calls': 1, 'model': 'gpt-3.5-turbo', 'simulated': False, 'input_string': \"HeyPersonInfoCorrectly extracted `PersonInfo` with all the required parameters with correct typesdict_values(['Name', 'string'])dict_values(['Age', 'integer'])\", 'output_string': PersonInfo(name='Alice', age=30), 'description': None}\n"
     ]
    }
   ],
   "source": [
    "import instructor\n",
    "from pydantic import BaseModel\n",
    "from openai import OpenAI\n",
    "from instructor import Instructor\n",
    "from costly import costly, Costlog\n",
    "from costly.estimators.llm_api_estimation import LLM_API_Estimation\n",
    "\n",
    "\n",
    "@costly(\n",
    "    input_string=lambda kwargs: LLM_API_Estimation.get_raw_input_string_instructor(**kwargs),\n",
    ")\n",
    "def chatgpt_instructor(\n",
    "    messages: str | list[dict[str, str]],\n",
    "    model: str,\n",
    "    client: Instructor,\n",
    "    response_model: BaseModel,\n",
    ") -> str:\n",
    "    if isinstance(messages, str):\n",
    "        messages = [{\"role\": \"user\", \"content\": messages}]\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        response_model=response_model,\n",
    "    )\n",
    "    return response\n",
    "\n",
    "\n",
    "class PersonInfo(BaseModel):\n",
    "    name: str\n",
    "    age: int\n",
    "\n",
    "\n",
    "cl = Costlog()\n",
    "chatgpt_instructor(\n",
    "    messages=\"Hey\",\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    response_model=PersonInfo,\n",
    "    client=instructor.from_openai(OpenAI()),\n",
    "    simulate=True,\n",
    "    cost_log=cl,\n",
    ")\n",
    "chatgpt_instructor(\n",
    "    messages=\"Hey\",\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    response_model=PersonInfo,\n",
    "    client=instructor.from_openai(OpenAI()),\n",
    "    simulate=False,\n",
    "    cost_log=cl,\n",
    ")\n",
    "print(cl.items[0], '\\n---\\n', cl.items[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## costly-response\n",
    "\n",
    "For more accurate estimation of _real_ (not simulated) costs, we might want to calculate costs within the function.\n",
    "\n",
    "To do this, your function must return a `CostlyResponse` object.\n",
    "\n",
    "```python\n",
    "@dataclass\n",
    "class CostlyResponse:\n",
    "    output: Any\n",
    "    cost_info: dict[str, Any]\n",
    "```\n",
    "\n",
    " In actual usage, your function will only return the `output` field -- the `@costly` decorator will take care of this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perform business feel little subject weight. Real behind summer behind through suffer other their.\n",
      "Pretty population yard religious nearly painting. Sport writer analysis week shoulder. Girl member somebody.\n",
      "Someone job bad write both yes into. Every always before play field.\n",
      "Law executive man rate magazine. Role early leader customer defense scene again.\n",
      "Floor so others. Economic in American himself. Would employee care result city.\n",
      "Pm campaign memory TV not low. Find loss four economy human.\n",
      "Win then computer everyone somebody to. The natural exactly open white adult draw.\n",
      "Red term prevent friend on including resource. Commercial hospital suggest another condition message.\n",
      "Tv seven without week yet environmental western. Product memory himself since later relate. Should medical shake.\n",
      "Believe individual ever feel teacher measure believe cover. Author war girl determine list.\n",
      "Sister usually close authority listen such service. Authority wide rock personal wrong fact message.\n",
      "Stage before possible be throw. Determine old member behavior type easy. Every now prove behavior government work use.\n",
      "Cold especially plant and theory color authority. Value article player section. Including area hundred former capital red.\n",
      "Future growth summer clear. Former first agree peace state oil north.\n",
      "Concern treat analysis movement school analysis hit. Record throw its his forward call to doctor. East western blood today.\n",
      "Sport along certain bill same. Require then serve option cold. Late out military much claim exactly lay. Television sell room such chair enough yet statement.\n",
      "Push rate remember dream religious. Right with her marriage son message. By the such field cultural.\n",
      "Then front short author money. Response drop mean million. Chance paper audience before behind line.\n",
      "Yet account something simply possible. Able candidate prepare new glass shoulder structure. Minute indicate environment act out north similar.\n",
      "Trip lose own property. Hard believe yourself actually crime east. Alone thing fine new beyond benefit.\n",
      "Human control no more after form player research. Eye support admit opportunity since anything continue. Who term community your contain.\n",
      "Gun husband yeah southern room. Music TV sometimes join matter.\n",
      "Only conference grow Congress someone natural region. Modern them true value dinner clearly happen. Act especially each leg.\n",
      "Late compare star never. Small south rich receive late rest serve.\n",
      "Little man religious again task body woman student. Adult sport large alone fill film baby. Someone hour sea professional painting half alone avoid.\n",
      "Avoid administration church. Pay exactly small turn between drop.\n",
      "Entire strategy work cause suddenly course to think. \n",
      "---\n",
      " Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum. \n",
      "---\n",
      " {'cost_min': 0.00036, 'cost_max': 0.12324, 'time_min': 0.0, 'time_max': 73.728, 'input_tokens': 12, 'output_tokens_min': 0, 'output_tokens_max': 2048, 'calls': 1, 'model': 'gpt-4', 'simulated': True, 'input_string': 'Write the Lorem ipsum text', 'output_string': None, 'description': None} \n",
      "---\n",
      " {'cost_min': 0.0061200000000000004, 'cost_max': 0.0061200000000000004, 'time_min': 4.668959399990854, 'time_max': 4.668959399990854, 'input_tokens': 12, 'output_tokens': 96, 'output_tokens_min': 96, 'output_tokens_max': 96, 'calls': 1, 'model': 'gpt-4', 'simulated': False, 'input_string': 'Write the Lorem ipsum text', 'output_string': 'Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.', 'description': None}\n"
     ]
    }
   ],
   "source": [
    "from costly import Costlog, costly, CostlyResponse\n",
    "\n",
    "\n",
    "@costly()\n",
    "def chatgpt(input_string: str, model: str) -> str:\n",
    "    from openai import OpenAI\n",
    "\n",
    "    client = OpenAI()\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": input_string},\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    return CostlyResponse(\n",
    "        output=response.choices[0].message.content,\n",
    "        cost_info={\n",
    "            \"input_tokens\": response.usage.prompt_tokens,\n",
    "            \"output_tokens\": response.usage.completion_tokens,\n",
    "        },\n",
    "    )\n",
    "\n",
    "\n",
    "cl = Costlog()\n",
    "x = chatgpt(\n",
    "    input_string=\"Write the Lorem ipsum text\",\n",
    "    model=\"gpt-4\",\n",
    "    cost_log=cl,\n",
    "    simulate=True,\n",
    ")\n",
    "y = chatgpt(\n",
    "    input_string=\"Write the Lorem ipsum text\",\n",
    "    model=\"gpt-4\",\n",
    "    cost_log=cl,\n",
    "    simulate=False,\n",
    ")\n",
    "print(x, \"\\n---\\n\", y, \"\\n---\\n\", cl.items[0], \"\\n---\\n\", cl.items[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-05 13:48:21,930 DEBUG instructor: Patching `client.chat.completions.create` with mode=<Mode.TOOLS: 'tool_call'>\n",
      "2024-09-05 13:48:21,935 DEBUG instructor: Instructor Request: mode.value='tool_call', response_model=<class '__main__.PersonInfo'>, new_kwargs={'messages': [{'content': 'Hey', 'role': 'user'}], 'model': 'gpt-4', 'tools': [{'type': 'function', 'function': {'name': 'PersonInfo', 'description': 'Correctly extracted `PersonInfo` with all the required parameters with correct types', 'parameters': {'properties': {'name': {'title': 'Name', 'type': 'string'}, 'age': {'title': 'Age', 'type': 'integer'}}, 'required': ['age', 'name'], 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'PersonInfo'}}}\n",
      "2024-09-05 13:48:21,938 DEBUG instructor: max_retries: 3\n",
      "2024-09-05 13:48:21,979 DEBUG instructor: Patching `client.chat.completions.create` with mode=<Mode.TOOLS: 'tool_call'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-05 13:48:21,979 DEBUG instructor: Instructor Request: mode.value='tool_call', response_model=<class '__main__.PersonInfo'>, new_kwargs={'messages': [{'content': 'Hey', 'role': 'user'}], 'model': 'gpt-4', 'tools': [{'type': 'function', 'function': {'name': 'PersonInfo', 'description': 'Correctly extracted `PersonInfo` with all the required parameters with correct types', 'parameters': {'properties': {'name': {'title': 'Name', 'type': 'string'}, 'age': {'title': 'Age', 'type': 'integer'}}, 'required': ['age', 'name'], 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'PersonInfo'}}}\n",
      "2024-09-05 13:48:21,988 DEBUG instructor: max_retries: 3\n",
      "2024-09-05 13:48:22,007 DEBUG instructor: Instructor Request: mode.value='tool_call', response_model=<class '__main__.PersonInfo'>, new_kwargs={'messages': [{'role': 'user', 'content': 'Hey'}], 'model': 'gpt-4', 'tools': [{'type': 'function', 'function': {'name': 'PersonInfo', 'description': 'Correctly extracted `PersonInfo` with all the required parameters with correct types', 'parameters': {'properties': {'name': {'title': 'Name', 'type': 'string'}, 'age': {'title': 'Age', 'type': 'integer'}}, 'required': ['age', 'name'], 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'PersonInfo'}}}\n",
      "2024-09-05 13:48:22,007 DEBUG instructor: max_retries: 3\n",
      "2024-09-05 13:48:23,642 DEBUG instructor: Instructor Raw Response: ChatCompletion(id='chatcmpl-A46CM0CmnzQ3WGO9sPNrNywh7Q9dj', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_SCw0UWe2DjfoeTVhiqgNvfVf', function=Function(arguments='{\\n  \"name\": \"John\",\\n  \"age\": 25\\n}', name='PersonInfo'), type='function')], refusal=None))], created=1725540502, model='gpt-4-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=16, prompt_tokens=66, total_tokens=82))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perform business feel little subject weight. Real behind summer behind through suffer other their.\n",
      "Pretty population yard religious nearly painting. Sport writer analysis week shoulder. Girl member somebody.\n",
      "Someone job bad write both yes into. Every always before play field.\n",
      "Law executive man rate magazine. Role early leader customer defense scene again.\n",
      "Floor so others. Economic in American himself. Would employee care result city.\n",
      "Pm campaign memory TV not low. Find loss four economy human.\n",
      "Win then computer everyone somebody to. The natural exactly open white adult draw.\n",
      "Red term prevent friend on including resource. Commercial hospital suggest another condition message.\n",
      "Tv seven without week yet environmental western. Product memory himself since later relate. Should medical shake.\n",
      "Believe individual ever feel teacher measure believe cover. Author war girl determine list.\n",
      "Sister usually close authority listen such service. Authority wide rock personal wrong fact message.\n",
      "Stage before possible be throw. Determine old member behavior type easy. Every now prove behavior government work use.\n",
      "Cold especially plant and theory color authority. Value article player section. Including area hundred former capital red.\n",
      "Future growth summer clear. Former first agree peace state oil north.\n",
      "Concern treat analysis movement school analysis hit. Record throw its his forward call to doctor. East western blood today.\n",
      "Sport along certain bill same. Require then serve option cold. Late out military much claim exactly lay. Television sell room such chair enough yet statement.\n",
      "Push rate remember dream religious. Right with her marriage son message. By the such field cultural.\n",
      "Then front short author money. Response drop mean million. Chance paper audience before behind line.\n",
      "Yet account something simply possible. Able candidate prepare new glass shoulder structure. Minute indicate environment act out north similar.\n",
      "Trip lose own property. Hard believe yourself actually crime east. Alone thing fine new beyond benefit.\n",
      "Human control no more after form player research. Eye support admit opportunity since anything continue. Who term community your contain.\n",
      "Gun husband yeah southern room. Music TV sometimes join matter.\n",
      "Only conference grow Congress someone natural region. Modern them true value dinner clearly happen. Act especially each leg.\n",
      "Late compare star never. Small south rich receive late rest serve.\n",
      "Little man religious again task body woman student. Adult sport large alone fill film baby. Someone hour sea professional painting half alone avoid.\n",
      "Avoid administration church. Pay exactly small turn between drop.\n",
      "Entire strategy work cause suddenly course to think. \n",
      "---\n",
      " Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum. \n",
      "---\n",
      " {'cost_min': 0.00123, 'cost_max': 0.12411, 'time_min': 0.0, 'time_max': 73.728, 'input_tokens': 41, 'output_tokens_min': 0, 'output_tokens_max': 2048, 'calls': 1, 'model': 'gpt-4', 'simulated': True, 'input_string': \"HeyPersonInfoCorrectly extracted `PersonInfo` with all the required parameters with correct typesdict_values(['Name', 'string'])dict_values(['Age', 'integer'])\", 'output_string': None, 'description': None} \n",
      "---\n",
      " {'cost_min': 0.00294, 'cost_max': 0.00294, 'time_min': 1.639918500004569, 'time_max': 1.639918500004569, 'input_tokens': 66, 'output_tokens': 16, 'output_tokens_min': 16, 'output_tokens_max': 16, 'calls': 1, 'model': 'gpt-4', 'simulated': False, 'input_string': \"HeyPersonInfoCorrectly extracted `PersonInfo` with all the required parameters with correct typesdict_values(['Name', 'string'])dict_values(['Age', 'integer'])\", 'output_string': PersonInfo(name='John', age=25), 'description': None}\n"
     ]
    }
   ],
   "source": [
    "import instructor\n",
    "from pydantic import BaseModel\n",
    "from instructor import Instructor\n",
    "from openai import OpenAI\n",
    "from costly import Costlog, costly, CostlyResponse\n",
    "from costly.estimators.llm_api_estimation import LLM_API_Estimation\n",
    "\n",
    "\n",
    "@costly(\n",
    "    input_string=lambda kwargs: LLM_API_Estimation.get_raw_input_string_instructor(**kwargs),\n",
    ")\n",
    "def chatgpt_instructor(\n",
    "    messages: str | list[dict[str, str]],\n",
    "    model: str,\n",
    "    client: Instructor,\n",
    "    response_model: BaseModel,\n",
    ") -> str:\n",
    "    if isinstance(messages, str):\n",
    "        messages = [{\"role\": \"user\", \"content\": messages}]\n",
    "    response = client.chat.completions.create_with_completion(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        response_model=response_model,\n",
    "    )\n",
    "    output_string, cost_info = response\n",
    "    return CostlyResponse(\n",
    "        output=output_string,\n",
    "        cost_info={\n",
    "            \"input_tokens\": cost_info.usage.prompt_tokens,\n",
    "            \"output_tokens\": cost_info.usage.completion_tokens\n",
    "        }\n",
    "    )\n",
    "    \n",
    "class PersonInfo(BaseModel):\n",
    "    name: str\n",
    "    age: int\n",
    "\n",
    "\n",
    "cl = Costlog()\n",
    "chatgpt_instructor(\n",
    "    messages=\"Hey\",\n",
    "    model=\"gpt-4\",\n",
    "    response_model=PersonInfo,\n",
    "    client=instructor.from_openai(OpenAI()),\n",
    "    simulate=True,\n",
    "    cost_log=cl,\n",
    ")\n",
    "chatgpt_instructor(\n",
    "    messages=\"Hey\",\n",
    "    model=\"gpt-4\",\n",
    "    response_model=PersonInfo,\n",
    "    client=instructor.from_openai(OpenAI()),\n",
    "    simulate=False,\n",
    "    cost_log=cl,\n",
    ")\n",
    "print(x, '\\n---\\n', y, '\\n---\\n', cl.items[0], '\\n---\\n', cl.items[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## costlog-notes\n",
    "\n",
    "The default [`costly.Costlog`](costly/costlog.py) class has two modes: `memory` and `jsonl`. The default is `memory`, but for large projects you may want to use `jsonl`: this dumps the cost log into a `.costly` folder in your working directory.\n",
    "\n",
    "The other thing that can be customized is the `totals_keys` parameter, which is a set of keys to aggregate costs by. By default it is `{\"cost_min\", \"cost_max\", \"time_min\", \"time_max\", \"calls\"}`, i.e. it tracks the range of possible costs and running times (`max` and `min` are usually only different when simulating because then you have to estimate). Out-of-the box you can customize it to also track `input_tokens`, `output_tokens_min`, `output_tokens_max`; any other customizations will only make sense if you are using your own estimator.\n",
    "\n",
    "You will want to change `totals_keys` if you want to use this package for things other than LLM costs, or if you want to track something other than `min` and `max`, e.g. some estimate of the average, or percentiles or whatever."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## token-estimating-weirdness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lake Chad is the fourth largest freshwater lake in Africa and the largest in West Central Africa. Located mainly in the far west of Chad, it spans along the border of four countries, including Chad, Cameroon, Niger, and Nigeria. It is important for water supply, fishing, agriculture, and pastures. However, Lake Chad has significantly shrunk in recent decades due to climate change, population pressure, and irrigation demands. As a result, resource competition among local communities has intensified and the livelihood of millions of people is at risk. 7 106\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "response = client.chat.completions.create(\n",
    "    model='gpt-4',\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"\"},\n",
    "    ],\n",
    ")\n",
    "\n",
    "output=response.choices[0].message.content\n",
    "input_tokens=response.usage.prompt_tokens\n",
    "output_tokens=response.usage.completion_tokens\n",
    "\n",
    "print(output, input_tokens, output_tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
