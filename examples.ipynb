{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Costly\n",
    "\n",
    "Costly adds a `simulate` argument and a cost logger to your function. The idea is this cost logger logs the cost of your function calls if `simulate=False`, and reasonably estimates it if `simulate=True`.\n",
    "\n",
    "The default behaviour is for LLM API calls, and uses `LLM_Simulator_Faker.simulate_llm_call()` as the simulator and `LLM_API_Estimation.get_cost_real()`, `LLM_API_Estimation.get_cost_simulating()` as the estimator.\n",
    "\n",
    "## Quick start\n",
    "\n",
    "Just mark the function responsible for your costs with `@costly()` decorator, as follows.\n",
    "\n",
    "This will only work sensibly out of the box if your function is doing an LLM API call, and takes the arguments `input_string`, `model` (and optionally `response_model` if the response is expected to be a complex object e.g. a Pydantic model) and returns a string (or a `response_model` object if you specified one).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abhim\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\requests\\__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.18) or chardet (5.2.0)/charset_normalizer (2.0.10) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly! Here is the classic \"Lorem Ipsum\" text:\n",
      "\n",
      "```\n",
      "Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\n",
      "```\n",
      "\n",
      "Let me know if you need more text or any variations!\n",
      "Quite rather later challenge parent research important. Recent worry thought of nation past.\n",
      "Knowledge become I together. Analysis series share past here quality. Cell south TV water upon fall.\n",
      "Institution daughter available into collection family. Government lose notice finish less life.\n",
      "Professional executive window whatever. Degree garden collection important each. Environment probably reason very rise pass. President make write believe conference fish quality.\n",
      "Leg executive everyone similar hold every or. About every generation at box page where receive. Kind part hit case growth ten run.\n",
      "There else benefit police. Interview trial appear voice sit identify finish. Worker rock feeling week that.\n",
      "Medical region coach anything perhaps compare guess. It why stand bring drop get machine. Enjoy car opportunity threat training forget space.\n",
      "Military decide cup. Support mouth morning.\n",
      "Who lawyer wonder fine character. Fight play early lead network. Their Congress fund Mrs nearly but organization build.\n",
      "Mother daughter hot computer travel. Call drive send everybody number computer radio.\n",
      "Cover above quite development game eight. Single dark camera alone company cell establish. Job still drop right myself.\n",
      "Impact build be particular interest world add. Station author development order mouth for.\n",
      "Collection indicate president actually candidate gun. Next election college spend government hear. Blue another course American challenge seven real.\n",
      "Reality water section go peace. Charge tax part thing.\n",
      "Base position pretty manager. Which that any drive. Quality debate big management think true.\n",
      "Defense audience production animal. Meet act hair unit risk check. Worker gun three drug every.\n",
      "Word responsibility let own. Fly behavior leg only consider friend.\n",
      "Grow me throughout. Claim race director assume member break least. End husband necessary office.\n",
      "Hand air suddenly suffer degree American. Statement if agreement factor challenge dark.\n",
      "Interview base open surface player follow job. Production city name force population. Understand fall safe military may story whatever. Less enjoy serious war rate her heart.\n",
      "Generation own test military none practice price he. Player travel growth data provide.\n",
      "Agree wall course performance look of chair. Another matter hold occur. Practice course term various customer raise.\n",
      "Way who high always try security maybe drug. Service what eat.\n",
      "Method although my commercial. Group central chance party that those shake. Large example its.\n",
      "Very president because traditional alone although carry check. Car right oil late friend. Bar head technology control.\n",
      "{'time_max': 20.40452720000688, 'cost_min': 7.59e-05, 'time_min': 1.9725272000068799, 'calls': 2.0, 'cost_max': 0.0013047}\n",
      "{'cost_min': 7.515e-05, 'cost_max': 7.515e-05, 'time_min': 1.9725272000068799, 'time_max': 1.9725272000068799, 'input_tokens': 5, 'output_tokens': 124, 'output_tokens_min': 124, 'output_tokens_max': 124, 'calls': 1, 'model': 'gpt-4o-mini', 'simulated': False, 'input_string': 'Write the Lorem ipsum text', 'output_string': 'Certainly! Here is the classic \"Lorem Ipsum\" text:\\n\\n```\\nLorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\\n```\\n\\nLet me know if you need more text or any variations!', 'description': ['chatgpt call']}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'cost_min': 7.5e-07,\n",
       " 'cost_max': 0.00122955,\n",
       " 'time_min': 0.0,\n",
       " 'time_max': 18.432,\n",
       " 'input_tokens': 5,\n",
       " 'output_tokens_min': 0,\n",
       " 'output_tokens_max': 2048,\n",
       " 'calls': 1,\n",
       " 'model': 'gpt-4o-mini',\n",
       " 'simulated': True,\n",
       " 'input_string': 'Write the Lorem ipsum text',\n",
       " 'output_string': None,\n",
       " 'description': ['chatgpt call']}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from costly import Costlog, costly\n",
    "\n",
    "@costly()\n",
    "def chatgpt(input_string: str, model: str) -> str:\n",
    "    from openai import OpenAI\n",
    "    client = OpenAI()\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": input_string}]\n",
    "    )\n",
    "    output_string = response.choices[0].message.content\n",
    "    return output_string\n",
    "\n",
    "\n",
    "    \n",
    "cl = Costlog()\n",
    "x = chatgpt(input_string=\"Write the Lorem ipsum text\", model = \"gpt-4o-mini\", cost_log=cl, simulate=False, description=[\"chatgpt call\"])\n",
    "y = chatgpt(input_string=\"Write the Lorem ipsum text\", model = \"gpt-4o-mini\", cost_log=cl, simulate=True, description=[\"chatgpt call\"])\n",
    "print(x)\n",
    "print(y)\n",
    "print(cl.totals)\n",
    "print(cl.items[0])\n",
    "cl.items[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's basically what's happening under the hood, courtesy of `costly.decorator.costly`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from costly.simulators.llm_simulator_faker import LLM_Simulator_Faker\n",
    "from costly.estimators.llm_api_estimation import LLM_API_Estimation\n",
    "\n",
    "def _chatgpt(input_string: str, model: str) -> str:\n",
    "    from openai import OpenAI\n",
    "    client = OpenAI()\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": input_string}]\n",
    "    )\n",
    "    output_string = response.choices[0].message.content\n",
    "    return output_string\n",
    "\n",
    "def chatgpt(input_string: str, model: str, cost_log: Costlog=None, simulate: bool=False, description: list[str]=None) -> str:\n",
    "    if simulate:\n",
    "        return LLM_Simulator_Faker.simulate_llm_call(\n",
    "            input_string=input_string,\n",
    "            model=model,\n",
    "            response_model=str,\n",
    "            cost_log=cost_log,\n",
    "            description=description,\n",
    "        )\n",
    "    if cost_log is not None:\n",
    "        with cost_log.new_item() as (item, timer):\n",
    "            output_string = _chatgpt(input_string, model)\n",
    "            cost_item = LLM_API_Estimation.get_cost_real(\n",
    "                model=model,\n",
    "                input_string=input_string,\n",
    "                output_string=output_string,\n",
    "                description=description,\n",
    "                timer=timer(),\n",
    "            )\n",
    "            item.update(cost_item)\n",
    "    else:\n",
    "        output_string = _chatgpt(input_string, model)\n",
    "    return output_string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customizations\n",
    "\n",
    "Although the library is designed for LLM API calls, it can be extended to estimating other types of costs with some customization and subclassing.\n",
    "\n",
    "Customizations you can do:\n",
    "\n",
    "### different simulators and estimators\n",
    "\n",
    "The defualt decorator behaviour is\n",
    "\n",
    "```python\n",
    "@costly(\n",
    "    simulator=LLM_Simulator_Faker.simulate_llm_call,\n",
    "    estimator=LLM_API_Estimation.get_cost_real,\n",
    ")\n",
    "```\n",
    "\n",
    "These functions can be replaced by your own custom functions. For reference, you can see how the default ones are implemented in [`costly.simulators.llm_simulator_faker`](costly/simulators/llm_simulator_faker.py) and [`costly.estimators.llm_api_estimation`](costly/estimators/llm_api_estimation.py).\n",
    "\n",
    "Also, the simulator and the estimator both have very specific type signatures:\n",
    "\n",
    "```python\n",
    "class LLM_Simulator_Faker:\n",
    "\n",
    "    @staticmethod\n",
    "    def simulate_llm_call(\n",
    "        input_string: str,\n",
    "        model: str = None,\n",
    "        response_model: type = str,\n",
    "        cost_log: Costlog = None,\n",
    "        description: list[str] = None,\n",
    "    ) -> str | Any:\n",
    "        ...\n",
    "\n",
    "class LLM_API_Estimation:\n",
    "\n",
    "    @staticmethod\n",
    "    def get_cost_real(\n",
    "        model: str,\n",
    "        input_tokens: int = None,\n",
    "        output_tokens_min: int = None,\n",
    "        output_tokens_max: int = None,\n",
    "        input_string: str = None,\n",
    "        output_string: str = None,\n",
    "        timer: float = None,\n",
    "        **kwargs,\n",
    "    ) -> dict[str, float]:\n",
    "        ...\n",
    "```\n",
    "\n",
    "So e.g. if your function takes different argument names -- say `prompt`, `model_name` and `response_type` -- you can change the decorator to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cost_min': 1.4e-05,\n",
       " 'cost_max': 1.4e-05,\n",
       " 'time_min': 0.5816699999850243,\n",
       " 'time_max': 0.5816699999850243,\n",
       " 'input_tokens': 1,\n",
       " 'output_tokens': 9,\n",
       " 'output_tokens_min': 9,\n",
       " 'output_tokens_max': 9,\n",
       " 'calls': 1,\n",
       " 'model': 'gpt-3.5-turbo',\n",
       " 'simulated': False,\n",
       " 'input_string': 'Hello',\n",
       " 'output_string': 'Hello! How can I assist you today?',\n",
       " 'description': None}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from costly.simulators.llm_simulator_faker import LLM_Simulator_Faker\n",
    "from costly.estimators.llm_api_estimation import LLM_API_Estimation\n",
    "\n",
    "@costly(\n",
    "    simulator=lambda prompt, model_name, response_type=str, cost_log=None, description=None: LLM_Simulator_Faker.simulate_llm_call(\n",
    "        input_string=prompt,\n",
    "        model=model_name,\n",
    "        response_model=response_type,\n",
    "        cost_log=cost_log,\n",
    "        description=description,\n",
    "    ),\n",
    "    estimator=lambda model_name, prompt, output_string, description, timer: LLM_API_Estimation.get_cost_real(\n",
    "        model=model_name,\n",
    "        input_string=prompt,\n",
    "        output_string=output_string,\n",
    "        description=description,\n",
    "        timer=timer,\n",
    "    ),\n",
    ")\n",
    "def chatgpt2(prompt: str, model_name: str) -> str:\n",
    "    from openai import OpenAI\n",
    "\n",
    "    client = OpenAI()\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name, messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    output_string = response.choices[0].message.content\n",
    "    return output_string\n",
    "\n",
    "\n",
    "cl = Costlog(totals_keys={\"cost_min\", \"cost_max\", \"time_min\", \"time_max\", \"calls\", \"input_tokens\", \"output_tokens_min\", \"output_tokens_max\"})\n",
    "chatgpt2(prompt=\"Hello\", model_name=\"gpt-3.5-turbo\", simulate=False, cost_log=cl)\n",
    "cl.items[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately, life isn't so hard, and there is a less cumbersome way of doing this in the case of remapping/computing variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cost_min': 1.4e-05,\n",
       " 'cost_max': 1.4e-05,\n",
       " 'time_min': 2.4095481999684125,\n",
       " 'time_max': 2.4095481999684125,\n",
       " 'input_tokens': 1,\n",
       " 'output_tokens': 9,\n",
       " 'output_tokens_min': 9,\n",
       " 'output_tokens_max': 9,\n",
       " 'calls': 1,\n",
       " 'model': 'gpt-3.5-turbo',\n",
       " 'simulated': False,\n",
       " 'input_string': 'Hello',\n",
       " 'output_string': 'Hello! How can I assist you today?',\n",
       " 'description': None}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from costly import Costlog, costly\n",
    "from costly.simulators.llm_simulator_faker import LLM_Simulator_Faker\n",
    "from costly.estimators.llm_api_estimation import LLM_API_Estimation\n",
    "\n",
    "\n",
    "@costly(\n",
    "    input_string=(lambda kwargs: kwargs[\"prompt\"]),\n",
    "    model=(lambda kwargs: kwargs[\"model_name\"]),\n",
    ")\n",
    "def chatgpt2(prompt: str, model_name: str) -> str:\n",
    "    from openai import OpenAI\n",
    "\n",
    "    client = OpenAI()\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name, messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    output_string = response.choices[0].message.content\n",
    "    return output_string\n",
    "\n",
    "\n",
    "cl = Costlog(\n",
    "    totals_keys={\n",
    "        \"cost_min\",\n",
    "        \"cost_max\",\n",
    "        \"time_min\",\n",
    "        \"time_max\",\n",
    "        \"calls\",\n",
    "        \"input_tokens\",\n",
    "        \"output_tokens_min\",\n",
    "        \"output_tokens_max\",\n",
    "    }\n",
    ")\n",
    "chatgpt2(prompt=\"Hello\", model_name=\"gpt-3.5-turbo\", simulate=False, cost_log=cl)\n",
    "cl.items[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually for the specific case of just remapping variables you can simply do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cost_min': 5e-07,\n",
       " 'cost_max': 0.0030725,\n",
       " 'time_min': 0.0,\n",
       " 'time_max': 73.728,\n",
       " 'input_tokens': 1,\n",
       " 'output_tokens_min': 0,\n",
       " 'output_tokens_max': 2048,\n",
       " 'calls': 1,\n",
       " 'model': 'gpt-3.5-turbo',\n",
       " 'simulated': True,\n",
       " 'input_string': 'Hello',\n",
       " 'output_string': None,\n",
       " 'description': None}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from costly import Costlog, costly\n",
    "from costly.simulators.llm_simulator_faker import LLM_Simulator_Faker\n",
    "from costly.estimators.llm_api_estimation import LLM_API_Estimation\n",
    "\n",
    "\n",
    "@costly(input_string=\"prompt\", model=\"model_name\")\n",
    "def chatgpt2(prompt: str, model_name: str) -> str:\n",
    "    from openai import OpenAI\n",
    "\n",
    "    client = OpenAI()\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name, messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    output_string = response.choices[0].message.content\n",
    "    return output_string\n",
    "\n",
    "\n",
    "cl = Costlog(\n",
    "    totals_keys={\n",
    "        \"cost_min\",\n",
    "        \"cost_max\",\n",
    "        \"time_min\",\n",
    "        \"time_max\",\n",
    "        \"calls\",\n",
    "        \"input_tokens\",\n",
    "        \"output_tokens_min\",\n",
    "        \"output_tokens_max\",\n",
    "    }\n",
    ")\n",
    "chatgpt2(prompt=\"Hello\", model_name=\"gpt-3.5-turbo\", simulate=True, cost_log=cl)\n",
    "cl.items[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These hacks will still be supported if you swap `simulator` and `estimator` for something else: just provide a way to calculate whatever parameters your new functions take!\n",
    "\n",
    "Some more examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cost_min': 1.4e-05,\n",
       " 'cost_max': 1.4e-05,\n",
       " 'time_min': 0.5815050000092015,\n",
       " 'time_max': 0.5815050000092015,\n",
       " 'input_tokens': 1,\n",
       " 'output_tokens': 9,\n",
       " 'output_tokens_min': 9,\n",
       " 'output_tokens_max': 9,\n",
       " 'calls': 1,\n",
       " 'model': 'gpt-3.5-turbo',\n",
       " 'simulated': False,\n",
       " 'input_string': 'Hey',\n",
       " 'output_string': 'Hello! How can I assist you today?',\n",
       " 'description': None}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from costly import Costlog, costly\n",
    "from costly.simulators.llm_simulator_faker import LLM_Simulator_Faker\n",
    "from costly.estimators.llm_api_estimation import LLM_API_Estimation\n",
    "\n",
    "\n",
    "@costly(\n",
    "    input_string=lambda kwargs: LLM_API_Estimation.messages_to_input_string(\n",
    "        kwargs[\"messages\"]\n",
    "    ),\n",
    ")\n",
    "def chatgpt_messages(messages: list[dict[str, str]], model: str) -> str:\n",
    "    from openai import OpenAI\n",
    "\n",
    "    client = OpenAI()\n",
    "    response = client.chat.completions.create(model=model, messages=messages)\n",
    "    output_string = response.choices[0].message.content\n",
    "    return output_string\n",
    "\n",
    "\n",
    "cl = Costlog(\n",
    "    totals_keys={\n",
    "        \"cost_min\",\n",
    "        \"cost_max\",\n",
    "        \"time_min\",\n",
    "        \"time_max\",\n",
    "        \"calls\",\n",
    "        \"input_tokens\",\n",
    "        \"output_tokens_min\",\n",
    "        \"output_tokens_max\",\n",
    "    }\n",
    ")\n",
    "chatgpt_messages(\n",
    "    messages=LLM_API_Estimation._input_string_to_messages(\"Hey\"),\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    simulate=False,\n",
    "    cost_log=cl,\n",
    ")\n",
    "cl.items[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-30 03:56:35,564 DEBUG instructor: Patching `client.chat.completions.create` with mode=<Mode.TOOLS: 'tool_call'>\n",
      "2024-08-30 03:56:35,572 DEBUG instructor: Instructor Request: mode.value='tool_call', response_model=<class '__main__.PersonInfo'>, new_kwargs={'messages': [{'content': 'Hey', 'role': 'user'}], 'model': 'gpt-3.5-turbo', 'tools': [{'type': 'function', 'function': {'name': 'PersonInfo', 'description': 'Correctly extracted `PersonInfo` with all the required parameters with correct types', 'parameters': {'properties': {'name': {'title': 'Name', 'type': 'string'}, 'age': {'title': 'Age', 'type': 'integer'}}, 'required': ['age', 'name'], 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'PersonInfo'}}}\n",
      "2024-08-30 03:56:35,574 DEBUG instructor: max_retries: 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'cost_min': 1.7e-05,\n",
       " 'cost_max': 0.003089,\n",
       " 'time_min': 0.0,\n",
       " 'time_max': 73.728,\n",
       " 'input_tokens': 34,\n",
       " 'output_tokens_min': 0,\n",
       " 'output_tokens_max': 2048,\n",
       " 'calls': 1,\n",
       " 'model': 'gpt-3.5-turbo',\n",
       " 'simulated': True,\n",
       " 'input_string': \"HeyPersonInfoCorrectly extracted `PersonInfo` with all the required parameters with correct typesdict_values(['Name', 'string'])dict_values(['Age', 'integer'])\",\n",
       " 'output_string': None,\n",
       " 'description': None}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import instructor\n",
    "from pydantic import BaseModel\n",
    "from openai import OpenAI\n",
    "from instructor import Instructor\n",
    "from costly import costly, Costlog\n",
    "from costly.estimators.llm_api_estimation import LLM_API_Estimation\n",
    "\n",
    "\n",
    "@costly(\n",
    "    input_string=lambda kwargs: LLM_API_Estimation.get_raw_prompt_instructor(**kwargs),\n",
    ")\n",
    "def chatgpt_instructor(messages: str | list[dict[str, str]], model: str, client: Instructor, response_model: BaseModel) -> str:\n",
    "    if isinstance(messages, str):\n",
    "        messages = [{\"role\": \"user\", \"content\": messages}]\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        response_model=response_model,\n",
    "    )\n",
    "    return response\n",
    "\n",
    "class PersonInfo(BaseModel):\n",
    "    name: str\n",
    "    age: int\n",
    "\n",
    "cl = Costlog()\n",
    "chatgpt_instructor(\n",
    "    messages=\"Hey\",\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    response_model=PersonInfo,\n",
    "    client = instructor.from_openai(OpenAI()),\n",
    "    simulate=True,\n",
    "    cost_log=cl,\n",
    ")\n",
    "cl.items[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Costlog customizations\n",
    "\n",
    "The default [`costly.Costlog`](costly/costlog.py) class has two modes: `memory` and `jsonl`. The default is `memory`, but for large projects you may want to use `jsonl`: this dumps the cost log into a `.costly` folder in your working directory.\n",
    "\n",
    "The other thing that can be customized is the `totals_keys` parameter, which is a set of keys to aggregate costs by. By default it is `{\"cost_min\", \"cost_max\", \"time_min\", \"time_max\", \"calls\"}`, i.e. it tracks the range of possible costs and running times (`max` and `min` are usually only different when simulating because then you have to estimate). Out-of-the box you can customize it to also track `input_tokens`, `output_tokens_min`, `output_tokens_max`; any other customizations will only make sense if you are using your own estimator.\n",
    "\n",
    "### Simulator\n",
    "\n",
    "[`costly.simulators.llm_simulator_faker`](costly/simulators/llm_simulator_faker.py) has some examples of how to subclass it.\n",
    "\n",
    "One obvious reason to subclass it is to have custom simulating functions for the types you are interested in. Although the default class \"works\" for any Pydantic basemodel etc., you might want to have a custom function -- e.g. if a value needs to be within a certain range, or if its distribution is not uniform, or if you want to use examples from your data, etc.\n",
    "\n",
    "Also, the simulator has a very specific type signature:\n",
    "\n",
    "```python\n",
    "class LLM_Simulator_Faker:\n",
    "\n",
    "    @staticmethod\n",
    "    def simulate_llm_call(\n",
    "        input_string: str,\n",
    "        model: str = None,\n",
    "        response_model: type = str,\n",
    "        cost_log: Costlog = None,\n",
    "        description: list[str] = None,\n",
    "    ) -> str | Any:\n",
    "        ...\n",
    "```\n",
    "\n",
    "So it would make sense to subclass it to just change this function so you don't have to do that ridiculous lambda thing above and can just use `@costly()`.\n",
    "\n",
    "### Estimator\n",
    "\n",
    "Again [`costly.estimators.llm_api_estimation`](costly/estimators/llm_api_estimation.py) has some examples of how to subclass it. The most obvious reason would be to add prices for other models we don't have listed (right now it's just OpenAI and Anthropic). The `PRICES` dict is like this:\n",
    "\n",
    "```python\n",
    "class LLM_API_Estimation:\n",
    "\n",
    "    PRICES = {\n",
    "        \"gpt-4o\": {\n",
    "            \"input_tokens\": 5.0e-6,\n",
    "            \"output_tokens\": 15.0e-6,\n",
    "            \"time\": 18e-3,\n",
    "        },\n",
    "        \"gpt-4o-mini\": {\n",
    "            \"input_tokens\": 0.15e-6,\n",
    "            \"output_tokens\": 0.6e-6,\n",
    "            \"time\": 9e-3,\n",
    "        },\n",
    "    }\n",
    "```\n",
    "\n",
    "Something like this would be quite natural:\n",
    "\n",
    "```python\n",
    "class My_Estimation(LLM_API_Estimation):\n",
    "    PRICES = LLM_API_Estimation.PRICES | {\"my_model\": LLM_API_Estimation.PRICES[\"gpt-4o\"]}\n",
    "```\n",
    "\n",
    "Note that `LLM_API_Estimation` _can_ handle things like `gpt-4o-2024-05-13` etc. because it `LLM_API_Estimation.get_model()` gets the longest prefix matching model name in `PRICES`. \n",
    "\n",
    "### Some assumptions made\n",
    "\n",
    "`LLM_Simulator_Faker`, when producing text, produces text of about `600 * 4.5` characters.\n",
    "\n",
    "Generally we assume that 1 token is about 4.5 characters. Though actual token estimation does use `tiktoken` (unless you subclass `LLM_API_Estimation` and set `tokenize=_tokenize_rough`).\n",
    "\n",
    "Generally we assume, for cost estimation, that output tokens are in the range `[0, 2048]`, and the min and max are computed accordingly. As a rule of thumb for complex projects the true value tends to be about 1/3 the way through, and for projects that receive quite short responses it would be much lower.\n",
    "\n",
    "\n",
    "All of this can be overriden by subclassing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More examples\n",
    "\n",
    "## Example with `messages`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Misc stuff\n",
    "\n",
    "### messages, instructor etc.\n",
    "\n",
    "### local model support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unsupported type: <class 'float'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m costlog\u001b[38;5;241m.\u001b[39mnew_item() \u001b[38;5;28;01mas\u001b[39;00m (item, timer):\n\u001b[0;32m     11\u001b[0m     item\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHi\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHello\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[1;32m---> 13\u001b[0m x\u001b[38;5;241m=\u001b[39m\u001b[43mLLM_Simulator_Faker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimulate_llm_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_string\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHello\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-3.5-turbo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcost_log\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcostlog\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(x)\n",
      "File \u001b[1;32mc:\\Users\\abhim\\Google Drive\\Gittable\\Code\\quotoken\\costly\\simulators\\llm_simulator_faker.py:86\u001b[0m, in \u001b[0;36msimulate_llm_call\u001b[1;34m(input_string, model, response_model, cost_log, description)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;124;03mMain things you would likely subclass:\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;124;03m- FAKER_PARAMS: default is:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;124;03m    _fake_custom: fake a value of a custom type\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     75\u001b[0m FAKER \u001b[38;5;241m=\u001b[39m Faker()\n\u001b[0;32m     77\u001b[0m FAKER_PARAMS \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstr\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[0;32m     79\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_nb_chars\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m600\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m4.5\u001b[39m,\n\u001b[0;32m     80\u001b[0m     },\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[0;32m     82\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m     83\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m100\u001b[39m,\n\u001b[0;32m     84\u001b[0m     },\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m---> 86\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m20\u001b[39m,\n\u001b[0;32m     87\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m20\u001b[39m,\n\u001b[0;32m     88\u001b[0m     },\n\u001b[0;32m     89\u001b[0m }\n\u001b[0;32m     91\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msimulate_llm_call\u001b[39m(\n\u001b[0;32m     93\u001b[0m     input_string: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     97\u001b[0m     description: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     98\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m Any:\n\u001b[0;32m     99\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03m    Simulate an LLM call.\u001b[39;00m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\abhim\\Google Drive\\Gittable\\Code\\quotoken\\costly\\simulators\\llm_simulator_faker.py:110\u001b[0m, in \u001b[0;36mfake\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel is required for tracking costs\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    106\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m cost_log\u001b[38;5;241m.\u001b[39mnew_item() \u001b[38;5;28;01mas\u001b[39;00m (item, _):\n\u001b[0;32m    107\u001b[0m         cost_item \u001b[38;5;241m=\u001b[39m LLM_API_Estimation\u001b[38;5;241m.\u001b[39mget_cost_simulating(\n\u001b[0;32m    108\u001b[0m             input_string\u001b[38;5;241m=\u001b[39minput_string,\n\u001b[0;32m    109\u001b[0m             model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m--> 110\u001b[0m             description\u001b[38;5;241m=\u001b[39mdescription,\n\u001b[0;32m    111\u001b[0m             \u001b[38;5;66;03m# output_string=response, # not needed\u001b[39;00m\n\u001b[0;32m    112\u001b[0m         )\n\u001b[0;32m    113\u001b[0m         item\u001b[38;5;241m.\u001b[39mupdate(cost_item)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[1;31mValueError\u001b[0m: Unsupported type: <class 'float'>"
     ]
    }
   ],
   "source": [
    "from typing import Optional\n",
    "from costly.costlog import Costlog\n",
    "from costly.simulators.llm_simulator_faker import LLM_Simulator_Faker\n",
    "from costly.estimators.llm_api_estimation import LLM_API_Estimation\n",
    "from time import sleep\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any, Optional, Union\n",
    "costlog = Costlog()\n",
    "\n",
    "with costlog.new_item() as (item, timer):\n",
    "    item.update({\"Hi\": \"Hello\"})\n",
    "\n",
    "x=LLM_Simulator_Faker.simulate_llm_call(\n",
    "    input_string=\"Hello\",\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    response_model=float,\n",
    "    cost_log=costlog,\n",
    ")\n",
    "print(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
