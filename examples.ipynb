{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples\n",
    "\n",
    "[**(quick-start)**](#quick-start) Adding a `@costly()` decorator to your function automatically adds the arguments `simulate: bool` and `cost_log: costly.Costlog` to it (and also `description: list[str]` which is useful for tracing the sources of costs in the breakdown).\n",
    "\n",
    "By default, this uses [`LLM_Simulator_Faker.simulate_llm_call()`](costly/simulators/llm_simulator_faker.py) to simulate your function and [`LLM_API_Estimation.get_cost_real()`](costly/estimators/llm_api_estimation.py) as the estimator -- see [**(assumptions)**](#assumptions) for some brief comments on how these work by default. \n",
    "\n",
    "You can also use your own simulator and estimator, as shown in Example [**(customators)**](#customators). In particular you can subclass the existing simulator and estimator objects.\n",
    "\n",
    "The default simulator and estimator assume that your function takes arguments `input_string: str, model: str, response_model: str | BaseModel)`, and supplies these arguments to the simulator and estimator. Regardless of which simulator and estimator you use, if your function does not take the same arguments as them (e.g. if your function adds a system prompt to `input_string`, or you use a different naming system for `model`s from that used in `LLM_API_Estimation.PRICES`), you can pass parameter mappings as shown in Example [**(param-mappings)**](#param-mappings). \n",
    "\n",
    "In particular see the example parameter mappings for [**(prompt-and-instructor)**](#prompt-and-instructor).\n",
    "\n",
    "For more accurate cost tracking, you might want to calculate costs within the function, e.g. use info returned by the API call itself. For this, your function must response a `CostlyResponse` object as shown in Example [**(costly-response)**](#costly-response). In usage, your function will still return the normal output contained in `CostlyResponse.output`, and everything else will be passed to the simulator and estimator.\n",
    "\n",
    "Finally, see [**(costlog-notes)**](#costlog-notes) for some notes on the `Costlog` class itself.\n",
    "\n",
    "Oh and also see [**(invoice)**](#invoice) for how to use the `@invoice` decorator to functions to help `Costlog` track exactly what's using your money."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## quick-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abhim\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\requests\\__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.18) or chardet (5.2.0)/charset_normalizer (2.0.10) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n",
      "c:\\Users\\abhim\\Google Drive\\Gittable\\Code\\costly\\costly\\estimators\\llm_api_estimation.py:192: UserWarning: messages_to_input_tokens: gpt-4 may update over time. Returning num tokens assuming gpt-4-0613.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly! Here is the classic \"Lorem Ipsum\" text:\n",
      "\n",
      "```\n",
      "Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\n",
      "```\n",
      "\n",
      "If you need a longer version or something specific, just let me know! \n",
      "---\n",
      " Cultural scientist community care that.\n",
      "Imagine heavy she sing. Between goal available its then space final. Evidence another later stage.\n",
      "Agency account necessary pressure. Key southern each war involve who.\n",
      "Inside social training executive certainly close. Note old task size term condition loss. Claim chance crime type skill Congress book hair.\n",
      "Hard oil place. Its example responsibility author measure still success put. Arrive little final act.\n",
      "Congress eat ten must sure them. Suffer anything threat order speak light grow.\n",
      "Girl no crime deal at. Quality act actually meeting answer ability weight.\n",
      "Sit trial drive home. Ok responsibility even somebody. Center citizen it that your. Cover next rule sense would reveal world age.\n",
      "Everything green plant. Some answer rich.\n",
      "Left painting piece second fight.\n",
      "To enjoy modern. Ready cell child newspaper act share lawyer.\n",
      "Red third safe people service apply successful charge. Many concern race project meet.\n",
      "Find fine a scene better despite not. Fine may personal despite.\n",
      "Current relate respond. System appear glass money.\n",
      "Ability class process. Police her third later land front. Simply as crime doctor past.\n",
      "Little decide today all who. Pattern see clearly name husband activity go thank. Step group debate party according.\n",
      "Eat war dark. Call responsibility nor data.\n",
      "Office nature perhaps part. Minute any rest defense hour. Rich type take person quickly budget.\n",
      "Season available group deep determine man.\n",
      "Change social enjoy air message this. She age know necessary character listen line since. Life entire offer stage else world.\n",
      "Item smile TV recently total. Church accept with gun collection. Herself agreement support his radio your ten.\n",
      "Deal of run cell become. Party be oil nothing indicate research recognize. Information three offer member beat might.\n",
      "Else size top wish professional. Actually budget politics star forget most that.\n",
      "Attorney bill art ten. Goal owner born physical and cover have. Record together read same return two box song.\n",
      "A sense kid. Defense scene Democrat civil nothing bad I.\n",
      "Most trial wife its cell stage wind.\n",
      "Low subject do sound ago ten.\n",
      "Month name land tonight do watch maintain.\n",
      "Heavy dream whole light billion next. Act government south support. Serve dream whose two college edge school.\n",
      "Water summer beat general watch take. Analysis push impact peace. Investment professor institution resource.\n",
      "Where on then positive degree. Today agreement push.\n",
      "Crime treatment few popular world. Year every husband organization travel population. Operation cover end suddenly determine especially movement.\n",
      "Notice standard particular set see training. Bill use PM investment. \n",
      "---\n",
      " {'time_min': 1.596628100000089, 'time_max': 20.028628100000088, 'cost_min': 7.98e-05, 'cost_max': 0.0013085999999999998, 'calls': 2.0} \n",
      "---\n",
      " {'cost_min': 7.8e-05, 'cost_max': 7.8e-05, 'time_min': 1.596628100000089, 'time_max': 1.596628100000089, 'input_tokens': 12, 'output_tokens': 127, 'output_tokens_min': 127, 'output_tokens_max': 127, 'calls': 1, 'model': 'gpt-4o-mini', 'simulated': False, 'input_string': None, 'messages': [{'content': 'Write the Lorem ipsum text', 'role': 'user'}], 'output_string': 'Certainly! Here is the classic \"Lorem Ipsum\" text:\\n\\n```\\nLorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\\n```\\n\\nIf you need a longer version or something specific, just let me know!', 'description': None} \n",
      "---\n",
      " {'cost_min': 1.8e-06, 'cost_max': 0.0012305999999999999, 'time_min': 0.0, 'time_max': 18.432, 'input_tokens': 12, 'output_tokens_min': 0, 'output_tokens_max': 2048, 'calls': 1, 'model': 'gpt-4o-mini', 'simulated': True, 'input_string': None, 'messages': [{'content': 'Write the Lorem ipsum text', 'role': 'user'}], 'output_string': None, 'description': None}\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from costly import Costlog, costly\n",
    "\n",
    "\n",
    "@costly()\n",
    "def chatgpt(messages: dict, model: str) -> str:\n",
    "    from openai import OpenAI\n",
    "\n",
    "    client = OpenAI()\n",
    "    response = client.chat.completions.create(\n",
    "        model=model, messages=messages,\n",
    "    )\n",
    "    output_string = response.choices[0].message.content\n",
    "    return output_string\n",
    "\n",
    "\n",
    "cl = Costlog()\n",
    "x = chatgpt(\n",
    "    messages=[{\"content\": \"Write the Lorem ipsum text\", \"role\": \"user\"}],\n",
    "    model=\"gpt-4o-mini\",\n",
    "    cost_log=cl,\n",
    "    simulate=False,\n",
    ")\n",
    "y = chatgpt(\n",
    "    messages=[{\"content\": \"Write the Lorem ipsum text\", \"role\": \"user\"}],\n",
    "    model=\"gpt-4o-mini\",\n",
    "    cost_log=cl,\n",
    "    simulate=True,\n",
    ")\n",
    "print(\n",
    "    x,\n",
    "    \"\\n---\\n\",\n",
    "    y,\n",
    "    \"\\n---\\n\",\n",
    "    cl.totals,\n",
    "    \"\\n---\\n\",\n",
    "    cl.items[0],\n",
    "    \"\\n---\\n\",\n",
    "    cl.items[1],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's basically what's happening under the hood, courtesy of `costly.decorator.costly`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from costly.simulators.llm_simulator_faker import LLM_Simulator_Faker\n",
    "from costly.estimators.llm_api_estimation import LLM_API_Estimation\n",
    "\n",
    "def _chatgpt(messages: list[dict[str, str]], model: str) -> str:\n",
    "    from openai import OpenAI\n",
    "    client = OpenAI()\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages\n",
    "    )\n",
    "    output_string = response.choices[0].message.content\n",
    "    return output_string\n",
    "\n",
    "def chatgpt(messages: list[dict[str,str]], model: str, cost_log: Costlog=None, simulate: bool=False, description: list[str]=None) -> str:\n",
    "    if simulate:\n",
    "        return LLM_Simulator_Faker.simulate_llm_call(\n",
    "            messages=messages,\n",
    "            model=model,\n",
    "            response_model=str,\n",
    "            cost_log=cost_log,\n",
    "            description=description,\n",
    "        )\n",
    "    if cost_log is not None:\n",
    "        with cost_log.new_item() as (item, timer):\n",
    "            output_string = _chatgpt(messages, model)\n",
    "            cost_item = LLM_API_Estimation.get_cost_real(\n",
    "                model=model,\n",
    "                input_string=messages,\n",
    "                output_string=output_string,\n",
    "                description=description,\n",
    "                timer=timer(),\n",
    "            )\n",
    "            item.update(cost_item)\n",
    "    else:\n",
    "        output_string = _chatgpt(input_string, model)\n",
    "    return output_string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## assumptions\n",
    "\n",
    "`LLM_Simulator_Faker`, when producing text, produces text of about `600 * 4.5` characters.\n",
    "\n",
    "Generally we assume that 1 token is about 4.5 characters. Though actual token estimation does use `tiktoken` (unless you subclass `LLM_API_Estimation` and set `tokenize=_tokenize_rough`).\n",
    "\n",
    "Generally we assume, for cost estimation, that output tokens are in the range `[0, 2048]`, and the min and max are computed accordingly. As a rule of thumb for complex projects the true value tends to be about 1/3 the way through, and for projects that receive quite short responses it would be much lower.\n",
    "\n",
    "`LLM_API_Estimation.get_cost_real()` uses the `PRICES` dict to get the cost of a (real of simulated) API call, and to estimate time for a simulated API call. By default the standard OpenAI and Anthropic models are included; you can add more via subclassing.\n",
    "\n",
    "All of this can be overriden by subclassing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## customators\n",
    "\n",
    "The defualt decorator behaviour is\n",
    "\n",
    "```python\n",
    "@costly(\n",
    "    simulator=LLM_Simulator_Faker.simulate_llm_call,\n",
    "    estimator=LLM_API_Estimation.get_cost_real,\n",
    ")\n",
    "```\n",
    "\n",
    "These functions can be replaced by your own custom functions -- the best way to do this is probably to subclass the respective class.\n",
    "\n",
    "[`costly.simulators.llm_simulator_faker`](costly/simulators/llm_simulator_faker.py) has some examples of how to subclass it. One obvious reason to subclass it is to have custom simulating functions for the types you are interested in. Although the default class \"works\" for any Pydantic basemodel etc., you might want to have a custom function -- e.g. if a value needs to be within a certain range, or if its distribution is not uniform, or if you want to use examples from your data, etc.\n",
    "\n",
    "Again [`costly.estimators.llm_api_estimation`](costly/estimators/llm_api_estimation.py) has some examples of how to subclass it. The most obvious reason would be to add prices for other models we don't have listed (right now it's just OpenAI and Anthropic). The `PRICES` dict is like this:\n",
    "\n",
    "```python\n",
    "class LLM_API_Estimation:\n",
    "\n",
    "    PRICES = {\n",
    "        \"gpt-4o\": {\n",
    "            \"input_tokens\": 5.0e-6,\n",
    "            \"output_tokens\": 15.0e-6,\n",
    "            \"time\": 18e-3,\n",
    "        },\n",
    "        \"gpt-4o-mini\": {\n",
    "            \"input_tokens\": 0.15e-6,\n",
    "            \"output_tokens\": 0.6e-6,\n",
    "            \"time\": 9e-3,\n",
    "        },\n",
    "        ...\n",
    "    }\n",
    "```\n",
    "\n",
    "Something like this would be quite natural:\n",
    "\n",
    "```python\n",
    "class My_Estimation(LLM_API_Estimation):\n",
    "    PRICES = LLM_API_Estimation.PRICES | {\"my_model\": LLM_API_Estimation.PRICES[\"gpt-4o\"]}\n",
    "```\n",
    "\n",
    "Note that `LLM_API_Estimation` _can_ handle things like `gpt-4o-2024-05-13` etc. because it `LLM_API_Estimation.get_model()` gets the longest prefix matching model name in `PRICES`. \n",
    "\n",
    "\n",
    "The default simulator and estimator have type signatures:\n",
    "\n",
    "```python\n",
    "class LLM_Simulator_Faker:\n",
    "\n",
    "    @staticmethod\n",
    "    def simulate_llm_call(\n",
    "        input_string: str = None,  # must supply at least one of input_string, input_tokens, messages\n",
    "        input_tokens: int = None,\n",
    "        messages: list[dict[str, str]] = None,\n",
    "        model: str = None,\n",
    "        response_model: type = str,\n",
    "        cost_log: Costlog = None,\n",
    "        description: list[str] = None,\n",
    "    ) -> str | Any:\n",
    "        ...\n",
    "\n",
    "class LLM_API_Estimation:\n",
    "\n",
    "    @staticmethod\n",
    "    def get_cost_real(\n",
    "        model: str,\n",
    "        input_tokens: int = None,\n",
    "        output_tokens: int = None,\n",
    "        input_string: str = None,\n",
    "        messages: list[dict[str, str]] = None,\n",
    "        output_string: str = None,\n",
    "        timer: float = None,\n",
    "        **kwargs,\n",
    "    ) -> dict[str, float]:\n",
    "        ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## param-mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abhim\\Google Drive\\Gittable\\Code\\costly\\costly\\estimators\\llm_api_estimation.py:185: UserWarning: messages_to_input_tokens: gpt-3.5-turbo may update over time. Returning num tokens assuming gpt-3.5-turbo-0613.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cost_min': 4e-06, 'cost_max': 0.003076, 'time_min': 0.0, 'time_max': 73.728, 'input_tokens': 8, 'output_tokens_min': 0, 'output_tokens_max': 2048, 'calls': 1, 'model': 'gpt-3.5-turbo', 'simulated': True, 'input_string': None, 'messages': [{'role': 'user', 'content': 'Hello'}], 'output_string': None, 'description': None} \n",
      "---\n",
      " {'cost_min': 1.75e-05, 'cost_max': 1.75e-05, 'time_min': 0.5978679999825545, 'time_max': 0.5978679999825545, 'input_tokens': 8, 'output_tokens': 9, 'output_tokens_min': 9, 'output_tokens_max': 9, 'calls': 1, 'model': 'gpt-3.5-turbo', 'simulated': False, 'input_string': None, 'messages': [{'role': 'user', 'content': 'Hello'}], 'output_string': 'Hello! How can I help you today?', 'description': None}\n"
     ]
    }
   ],
   "source": [
    "from costly import Costlog, costly\n",
    "from costly.simulators.llm_simulator_faker import LLM_Simulator_Faker\n",
    "from costly.estimators.llm_api_estimation import LLM_API_Estimation\n",
    "\n",
    "\n",
    "@costly(\n",
    "    messages=(lambda kwargs: [{\"role\": \"user\", \"content\": kwargs[\"prompt\"]}]),\n",
    "    model=(lambda kwargs: kwargs[\"model_name\"]),\n",
    ")\n",
    "def chatgpt2(prompt: str, model_name: str) -> str:\n",
    "    from openai import OpenAI\n",
    "\n",
    "    client = OpenAI()\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name, messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    output_string = response.choices[0].message.content\n",
    "    return output_string\n",
    "\n",
    "\n",
    "cl = Costlog()\n",
    "chatgpt2(prompt=\"Hello\", model_name=\"gpt-3.5-turbo\", simulate=True, cost_log=cl)\n",
    "chatgpt2(prompt=\"Hello\", model_name=\"gpt-3.5-turbo\", simulate=False, cost_log=cl)\n",
    "print(cl.items[0],'\\n---\\n',cl.items[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually for the specific case of just remapping variables you can simply do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cost_min': 4e-06, 'cost_max': 0.003076, 'time_min': 0.0, 'time_max': 73.728, 'input_tokens': 8, 'output_tokens_min': 0, 'output_tokens_max': 2048, 'calls': 1, 'model': 'gpt-3.5-turbo', 'simulated': True, 'input_string': None, 'messages': [{'content': 'Hello', 'role': 'user'}], 'output_string': None, 'description': None} \n",
      "---\n",
      " {'cost_min': 1.75e-05, 'cost_max': 1.75e-05, 'time_min': 0.6519599999883212, 'time_max': 0.6519599999883212, 'input_tokens': 8, 'output_tokens': 9, 'output_tokens_min': 9, 'output_tokens_max': 9, 'calls': 1, 'model': 'gpt-3.5-turbo', 'simulated': False, 'input_string': None, 'messages': [{'content': 'Hello', 'role': 'user'}], 'output_string': 'Hello! How can I assist you today?', 'description': None}\n"
     ]
    }
   ],
   "source": [
    "from costly import Costlog, costly\n",
    "from costly.simulators.llm_simulator_faker import LLM_Simulator_Faker\n",
    "from costly.estimators.llm_api_estimation import LLM_API_Estimation\n",
    "\n",
    "\n",
    "@costly(messages=\"message_history\", model=\"model_name\")\n",
    "def chatgpt2(message_history: list[dict[str, str]], model_name: str) -> str:\n",
    "    from openai import OpenAI\n",
    "\n",
    "    client = OpenAI()\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name, messages=message_history\n",
    "    )\n",
    "    output_string = response.choices[0].message.content\n",
    "    return output_string\n",
    "\n",
    "\n",
    "cl = Costlog(\n",
    "    totals_keys={\n",
    "        \"cost_min\",\n",
    "        \"cost_max\",\n",
    "        \"time_min\",\n",
    "        \"time_max\",\n",
    "        \"calls\",\n",
    "        \"input_tokens\",\n",
    "        \"output_tokens_min\",\n",
    "        \"output_tokens_max\",\n",
    "    }\n",
    ")\n",
    "mh = [{\"content\": \"Hello\", \"role\": \"user\"}]\n",
    "chatgpt2(message_history=mh, model_name=\"gpt-3.5-turbo\", simulate=True, cost_log=cl)\n",
    "chatgpt2(message_history=mh, model_name=\"gpt-3.5-turbo\", simulate=False, cost_log=cl)\n",
    "print(cl.items[0],'\\n---\\n',cl.items[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prompt-and-instructor\n",
    "\n",
    "More examples of parameter mappings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abhim\\Google Drive\\Gittable\\Code\\costly\\costly\\estimators\\llm_api_estimation.py:192: UserWarning: messages_to_input_tokens: gpt-4 may update over time. Returning num tokens assuming gpt-4-0613.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cost_min': 2.7e-06, 'cost_max': 0.0012315, 'time_min': 0.0, 'time_max': 18.432, 'input_tokens': 18, 'output_tokens_min': 0, 'output_tokens_max': 2048, 'calls': 1, 'model': 'gpt-4o-mini', 'simulated': True, 'input_string': None, 'messages': None, 'output_string': None, 'description': None} \n",
      "---\n",
      " {'cost_min': 8.1e-06, 'cost_max': 8.1e-06, 'time_min': 0.8648557999986224, 'time_max': 0.8648557999986224, 'input_tokens': 18, 'output_tokens': 9, 'output_tokens_min': 9, 'output_tokens_max': 9, 'calls': 1, 'model': 'gpt-4o-mini', 'simulated': False, 'input_string': None, 'messages': None, 'output_string': 'Hello! How can I assist you today?', 'description': None}\n"
     ]
    }
   ],
   "source": [
    "from costly import Costlog, costly\n",
    "from costly.simulators.llm_simulator_faker import LLM_Simulator_Faker\n",
    "from costly.estimators.llm_api_estimation import LLM_API_Estimation\n",
    "\n",
    "\n",
    "@costly(\n",
    "    input_tokens=lambda kwargs: LLM_API_Estimation.prompt_to_input_tokens(**kwargs),\n",
    ")\n",
    "def chatgpt_prompt(\n",
    "    prompt: str, model: str, system_prompt: str = \"You are a helpful assistant.\"\n",
    ") -> str:\n",
    "    from openai import OpenAI\n",
    "\n",
    "    client = OpenAI()\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"content\": prompt, \"role\": \"user\"},\n",
    "            {\"content\": system_prompt, \"role\": \"system\"},\n",
    "        ],\n",
    "    )\n",
    "    output_string = response.choices[0].message.content\n",
    "    return output_string\n",
    "\n",
    "\n",
    "cl = Costlog()\n",
    "chatgpt_prompt(\n",
    "    prompt=\"Hey\",\n",
    "    model=\"gpt-4o-mini\",\n",
    "    simulate=True,\n",
    "    cost_log=cl,\n",
    ")\n",
    "chatgpt_prompt(\n",
    "    prompt=\"Hey\",\n",
    "    model=\"gpt-4o-mini\",\n",
    "    simulate=False,\n",
    "    cost_log=cl,\n",
    ")\n",
    "print(cl.items[0], \"\\n---\\n\", cl.items[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-05 23:00:11,977 DEBUG instructor: Instructor Request: mode.value='tool_call', response_model=<class '__main__.PersonInfo'>, new_kwargs={'messages': [{'content': 'Hey', 'role': 'user'}], 'model': 'gpt-3.5-turbo', 'tools': [{'type': 'function', 'function': {'name': 'PersonInfo', 'description': 'Correctly extracted `PersonInfo` with all the required parameters with correct types', 'parameters': {'properties': {'name': {'title': 'Name', 'type': 'string'}, 'age': {'title': 'Age', 'type': 'integer'}}, 'required': ['age', 'name'], 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'PersonInfo'}}}\n",
      "2024-09-05 23:00:11,992 DEBUG instructor: max_retries: 3\n",
      "2024-09-05 23:00:12,015 DEBUG instructor: Patching `client.chat.completions.create` with mode=<Mode.TOOLS: 'tool_call'>\n",
      "2024-09-05 23:00:12,015 DEBUG instructor: Instructor Request: mode.value='tool_call', response_model=<class '__main__.PersonInfo'>, new_kwargs={'messages': [{'content': 'Hey', 'role': 'user'}], 'model': 'gpt-3.5-turbo', 'tools': [{'type': 'function', 'function': {'name': 'PersonInfo', 'description': 'Correctly extracted `PersonInfo` with all the required parameters with correct types', 'parameters': {'properties': {'name': {'title': 'Name', 'type': 'string'}, 'age': {'title': 'Age', 'type': 'integer'}}, 'required': ['age', 'name'], 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'PersonInfo'}}}\n",
      "2024-09-05 23:00:12,015 DEBUG instructor: max_retries: 3\n",
      "2024-09-05 23:00:12,031 DEBUG instructor: Instructor Request: mode.value='tool_call', response_model=<class '__main__.PersonInfo'>, new_kwargs={'messages': [{'role': 'user', 'content': 'Hey'}], 'model': 'gpt-3.5-turbo', 'tools': [{'type': 'function', 'function': {'name': 'PersonInfo', 'description': 'Correctly extracted `PersonInfo` with all the required parameters with correct types', 'parameters': {'properties': {'name': {'title': 'Name', 'type': 'string'}, 'age': {'title': 'Age', 'type': 'integer'}}, 'required': ['age', 'name'], 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'PersonInfo'}}}\n",
      "2024-09-05 23:00:12,031 DEBUG instructor: max_retries: 3\n",
      "2024-09-05 23:00:12,654 DEBUG instructor: Instructor Raw Response: ChatCompletion(id='chatcmpl-A4EoOdBIMN5TnJ0FvNpMW4NQZ3exj', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_BcZKqrQYMNgvNwIPmxAKxMca', function=Function(arguments='{\"name\":\"Alice\",\"age\":30}', name='PersonInfo'), type='function')], refusal=None))], created=1725573612, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=9, prompt_tokens=75, total_tokens=84))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cost_min': 1.7e-05, 'cost_max': 0.003089, 'time_min': 0.0, 'time_max': 73.728, 'input_tokens': 34, 'output_tokens_min': 0, 'output_tokens_max': 2048, 'calls': 1, 'model': 'gpt-3.5-turbo', 'simulated': True, 'input_string': \"HeyPersonInfoCorrectly extracted `PersonInfo` with all the required parameters with correct typesdict_values(['Name', 'string'])dict_values(['Age', 'integer'])\", 'messages': 'Hey', 'output_string': None, 'description': None} \n",
      "---\n",
      " {'cost_min': 1.7e-05, 'cost_max': 1.7e-05, 'time_min': 0.6298766999971122, 'time_max': 0.6298766999971122, 'input_tokens': 34, 'output_tokens': 0, 'output_tokens_min': 0, 'output_tokens_max': 0, 'calls': 1, 'model': 'gpt-3.5-turbo', 'simulated': False, 'input_string': \"HeyPersonInfoCorrectly extracted `PersonInfo` with all the required parameters with correct typesdict_values(['Name', 'string'])dict_values(['Age', 'integer'])\", 'messages': 'Hey', 'output_string': PersonInfo(name='Alice', age=30), 'description': None}\n"
     ]
    }
   ],
   "source": [
    "import instructor\n",
    "from pydantic import BaseModel\n",
    "from openai import OpenAI\n",
    "from instructor import Instructor\n",
    "from costly import costly, Costlog\n",
    "from costly.estimators.llm_api_estimation import LLM_API_Estimation\n",
    "\n",
    "\n",
    "@costly(\n",
    "    input_string=lambda kwargs: LLM_API_Estimation._get_raw_messages_instructor(**kwargs),\n",
    ")\n",
    "def chatgpt_instructor(\n",
    "    messages: list[dict[str, str]],\n",
    "    model: str,\n",
    "    client: Instructor,\n",
    "    response_model: BaseModel,\n",
    ") -> str:\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        response_model=response_model,\n",
    "    )\n",
    "    return response\n",
    "\n",
    "\n",
    "class PersonInfo(BaseModel):\n",
    "    name: str\n",
    "    age: int\n",
    "\n",
    "\n",
    "cl = Costlog()\n",
    "chatgpt_instructor(\n",
    "    messages={\"content\": \"Hey\", \"role\": \"user\"},\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    response_model=PersonInfo,\n",
    "    client=instructor.from_openai(OpenAI()),\n",
    "    simulate=True,\n",
    "    cost_log=cl,\n",
    ")\n",
    "chatgpt_instructor(\n",
    "    messages={\"content\": \"Hey\", \"role\": \"user\"},\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    response_model=PersonInfo,\n",
    "    client=instructor.from_openai(OpenAI()),\n",
    "    simulate=False,\n",
    "    cost_log=cl,\n",
    ")\n",
    "print(cl.items[0], '\\n---\\n', cl.items[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## costly-response\n",
    "\n",
    "For more accurate estimation of _real_ (not simulated) costs, we might want to calculate costs within the function.\n",
    "\n",
    "To do this, your function must return a `CostlyResponse` object.\n",
    "\n",
    "```python\n",
    "@dataclass\n",
    "class CostlyResponse:\n",
    "    output: Any\n",
    "    cost_info: dict[str, Any]\n",
    "```\n",
    "\n",
    " In actual usage, your function will only return the `output` field -- the `@costly` decorator will take care of this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abhim\\Google Drive\\Gittable\\Code\\costly\\costly\\estimators\\llm_api_estimation.py:192: UserWarning: messages_to_input_tokens: gpt-4 may update over time. Returning num tokens assuming gpt-4-0613.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language set out air. Alone sense physical agent national spend. Form school out benefit represent.\n",
      "Action concern reach direction. Alone image culture along.\n",
      "Each head deal officer environment time east. Near drug economic his claim put.\n",
      "Church participant star lay fish score.\n",
      "Brother parent want first. However that writer visit do often. Over left former control myself audience.\n",
      "Fine father seem professional note you. Begin thousand approach authority he bit only.\n",
      "Both young itself nation scene western here easy. Couple decision law management scene. Customer mean fast.\n",
      "Million television voice toward there specific. Dog improve cold they us. Research sport expect it bill.\n",
      "Only debate sport join word them want.\n",
      "Staff although effect model significant skill pattern area. Always like resource certainly office another.\n",
      "Remain mention high easy stuff movement themselves. Piece newspaper nice grow. Spring girl approach yet century two force. Nice address wife early pass.\n",
      "Book scientist officer him next bag fall. Buy newspaper there value future those. While notice my laugh.\n",
      "Activity pay become perhaps. Consumer rest not wrong protect require station artist.\n",
      "Natural difficult control manager western. Once dinner wear development member entire economy.\n",
      "Who ever white. Party month why. Article long human amount agree rule.\n",
      "Loss yard approach fear teach leave.\n",
      "Generation commercial with official anything teacher now. Try worry reality hot own oil. You role deep professional play pretty light.\n",
      "Her number significant none. Music there few a.\n",
      "Protect personal on those road election reach. Begin pattern building audience at. Grow market speak rate wish. Perhaps wonder bank us chair official.\n",
      "Improve man decide window. Matter movement soldier whole hair. History good share factor.\n",
      "Reach executive other source economic. Cup according best spring but more.\n",
      "Network fire health side time over successful send. Generation play institution meeting voice. Opportunity firm describe southern.\n",
      "After score article. Fish trip not effort. By peace subject whatever bad drug. Decision those send society authority cause subject.\n",
      "Assume onto list however everybody. Prevent in home red star front. Radio exactly can stuff assume week.\n",
      "Past design best.\n",
      "Past leave that fall. Certainly example need difference morning. Fish bed order enter nothing example.\n",
      "Few something necessary nice. Southern may listen son. From article probably foot.\n",
      "Factor floor sport.\n",
      "But create peace seem second field. Share ready which goal.\n",
      "Quality key name near media other manager stay.\n",
      "Appear huge any moment everybody note. Probably their interesting side. \n",
      "---\n",
      " \"Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\" \n",
      "---\n",
      " {'cost_min': 0.00036, 'cost_max': 0.12324, 'time_min': 0.0, 'time_max': 73.728, 'input_tokens': 12, 'output_tokens_min': 0, 'output_tokens_max': 2048, 'calls': 1, 'model': 'gpt-4', 'simulated': True, 'input_string': None, 'messages': [{'content': 'Write the Lorem ipsum text', 'role': 'user'}], 'output_string': None, 'description': None} \n",
      "---\n",
      " {'cost_min': 0.006180000000000001, 'cost_max': 0.006180000000000001, 'time_min': 7.134455200022785, 'time_max': 7.134455200022785, 'input_tokens': 12, 'output_tokens': 97, 'output_tokens_min': 97, 'output_tokens_max': 97, 'calls': 1, 'model': 'gpt-4', 'simulated': False, 'input_string': None, 'messages': [{'content': 'Write the Lorem ipsum text', 'role': 'user'}], 'output_string': '\"Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\"', 'description': None}\n"
     ]
    }
   ],
   "source": [
    "from costly import Costlog, costly, CostlyResponse\n",
    "\n",
    "\n",
    "@costly()\n",
    "def chatgpt(messages: list[dict[str, str]], model: str) -> str:\n",
    "    from openai import OpenAI\n",
    "\n",
    "    client = OpenAI()\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "    )\n",
    "\n",
    "    return CostlyResponse(\n",
    "        output=response.choices[0].message.content,\n",
    "        cost_info={\n",
    "            \"input_tokens\": response.usage.prompt_tokens,\n",
    "            \"output_tokens\": response.usage.completion_tokens,\n",
    "        },\n",
    "    )\n",
    "\n",
    "\n",
    "cl = Costlog()\n",
    "x = chatgpt(\n",
    "    messages=[{\"content\": \"Write the Lorem ipsum text\", \"role\": \"user\"}],\n",
    "    model=\"gpt-4\",\n",
    "    cost_log=cl,\n",
    "    simulate=True,\n",
    ")\n",
    "y = chatgpt(\n",
    "    messages=[{\"content\": \"Write the Lorem ipsum text\", \"role\": \"user\"}],\n",
    "    model=\"gpt-4\",\n",
    "    cost_log=cl,\n",
    "    simulate=False,\n",
    ")\n",
    "print(x, \"\\n---\\n\", y, \"\\n---\\n\", cl.items[0], \"\\n---\\n\", cl.items[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-06 19:06:24,864 DEBUG instructor: Patching `client.chat.completions.create` with mode=<Mode.TOOLS: 'tool_call'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-06 19:06:24,867 DEBUG instructor: Instructor Request: mode.value='tool_call', response_model=<class '__main__.FOOMODEL'>, new_kwargs={'messages': [{'content': 'Hey', 'role': 'user'}], 'model': 'gpt-4', 'tools': [{'type': 'function', 'function': {'name': 'FOOMODEL', 'description': 'Correctly extracted `FOOMODEL` with all the required parameters with correct types', 'parameters': {'properties': {'name': {'title': 'Name', 'type': 'string'}, 'age': {'title': 'Age', 'type': 'integer'}, 'bmi': {'title': 'Bmi', 'type': 'number'}, 'metadata': {'anyOf': [{'type': 'object'}, {'type': 'null'}], 'default': None, 'title': 'Metadata'}}, 'required': ['age', 'bmi', 'name'], 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'FOOMODEL'}}}\n",
      "2024-09-06 19:06:24,875 DEBUG instructor: max_retries: 3\n",
      "c:\\Users\\abhim\\Google Drive\\Gittable\\Code\\costly\\costly\\estimators\\llm_api_estimation.py:287: UserWarning: Field title not found in tokenization function\n",
      "  warnings.warn(\n",
      "c:\\Users\\abhim\\Google Drive\\Gittable\\Code\\costly\\costly\\estimators\\llm_api_estimation.py:287: UserWarning: Field anyOf not found in tokenization function\n",
      "  warnings.warn(\n",
      "c:\\Users\\abhim\\Google Drive\\Gittable\\Code\\costly\\costly\\estimators\\llm_api_estimation.py:287: UserWarning: Field default not found in tokenization function\n",
      "  warnings.warn(\n",
      "c:\\Users\\abhim\\Google Drive\\Gittable\\Code\\costly\\costly\\estimators\\llm_api_estimation.py:196: UserWarning: messages_to_input_tokens: gpt-4 may update over time. Returning num tokens assuming gpt-4-0613.\n",
      "  warnings.warn(\n",
      "2024-09-06 19:06:24,929 DEBUG instructor: Patching `client.chat.completions.create` with mode=<Mode.TOOLS: 'tool_call'>\n",
      "2024-09-06 19:06:24,936 DEBUG instructor: Instructor Request: mode.value='tool_call', response_model=<class '__main__.FOOMODEL'>, new_kwargs={'messages': [{'content': 'Hey', 'role': 'user'}], 'model': 'gpt-4', 'tools': [{'type': 'function', 'function': {'name': 'FOOMODEL', 'description': 'Correctly extracted `FOOMODEL` with all the required parameters with correct types', 'parameters': {'properties': {'name': {'title': 'Name', 'type': 'string'}, 'age': {'title': 'Age', 'type': 'integer'}, 'bmi': {'title': 'Bmi', 'type': 'number'}, 'metadata': {'anyOf': [{'type': 'object'}, {'type': 'null'}], 'default': None, 'title': 'Metadata'}}, 'required': ['age', 'bmi', 'name'], 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'FOOMODEL'}}}\n",
      "2024-09-06 19:06:24,936 DEBUG instructor: max_retries: 3\n",
      "2024-09-06 19:06:24,965 DEBUG instructor: Instructor Request: mode.value='tool_call', response_model=<class '__main__.FOOMODEL'>, new_kwargs={'messages': [{'content': 'Hey', 'role': 'user'}], 'model': 'gpt-4', 'tools': [{'type': 'function', 'function': {'name': 'FOOMODEL', 'description': 'Correctly extracted `FOOMODEL` with all the required parameters with correct types', 'parameters': {'properties': {'name': {'title': 'Name', 'type': 'string'}, 'age': {'title': 'Age', 'type': 'integer'}, 'bmi': {'title': 'Bmi', 'type': 'number'}, 'metadata': {'anyOf': [{'type': 'object'}, {'type': 'null'}], 'default': None, 'title': 'Metadata'}}, 'required': ['age', 'bmi', 'name'], 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'FOOMODEL'}}}\n",
      "2024-09-06 19:06:24,965 DEBUG instructor: max_retries: 3\n",
      "2024-09-06 19:06:27,330 DEBUG instructor: Instructor Raw Response: ChatCompletion(id='chatcmpl-A4Xdid3ERaJ0uKFsjXWdsqF6AD9lW', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_i9EMPcF3JSU3tJZSsQ5Fo7sE', function=Function(arguments='{\\n  \"name\": \"Jack\",\\n  \"age\": 25,\\n  \"bmi\": 23.5\\n}', name='FOOMODEL'), type='function')], refusal=None))], created=1725645986, model='gpt-4-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=26, prompt_tokens=81, total_tokens=107))\n",
      "2024-09-06 19:06:27,346 DEBUG instructor: Instructor Request: mode.value='tool_call', response_model=<class '__main__.FOOMODEL'>, new_kwargs={'messages': [{'content': 'Hey', 'role': 'user'}], 'model': 'gpt-4', 'tools': [{'type': 'function', 'function': {'name': 'FOOMODEL', 'description': 'Correctly extracted `FOOMODEL` with all the required parameters with correct types', 'parameters': {'properties': {'name': {'title': 'Name', 'type': 'string'}, 'age': {'title': 'Age', 'type': 'integer'}, 'bmi': {'title': 'Bmi', 'type': 'number'}, 'metadata': {'anyOf': [{'type': 'object'}, {'type': 'null'}], 'default': None, 'title': 'Metadata'}}, 'required': ['age', 'bmi', 'name'], 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'FOOMODEL'}}}\n",
      "2024-09-06 19:06:27,346 DEBUG instructor: max_retries: 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode.value='tool_call', response_model=<class '__main__.FOOMODEL'>, new_kwargs={'messages': [{'content': 'Hey', 'role': 'user'}], 'model': 'gpt-4', 'tools': [{'type': 'function', 'function': {'name': 'FOOMODEL', 'description': 'Correctly extracted `FOOMODEL` with all the required parameters with correct types', 'parameters': {'properties': {'name': {'title': 'Name', 'type': 'string'}, 'age': {'title': 'Age', 'type': 'integer'}, 'bmi': {'title': 'Bmi', 'type': 'number'}, 'metadata': {'anyOf': [{'type': 'object'}, {'type': 'null'}], 'default': None, 'title': 'Metadata'}}, 'required': ['age', 'bmi', 'name'], 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'FOOMODEL'}}}\n",
      "name='Beautiful successful family Mr. Its yourself scientist.\\nArt attack phone walk. Office factor civil try appear single.\\nCourse southern media local among. Everyone hear Republican white cold yourself perhaps after. Professional away heavy yes kid certain throughout.\\nAmong story race them sing back. Owner help media once possible north.\\nAs site yourself strategy down history. Record animal reveal season education.\\nPerformance area over goal seat organization probably. Serve officer president behavior away body. Small magazine remain gas image space four. Where account protect fill card establish each.\\nGrowth show catch be certain hit general. Feel military tough any entire fight field. Join statement series plan.\\nScience in present these. Brother eat I.\\nMrs fear record south nothing store take. Today nearly edge rise.\\nClear interview truth candidate. Either sure need machine toward information human its.\\nCertain say way should thought toward. Somebody billion agency scientist letter group determine.\\nTheory value return notice page mouth. Hope stuff talk represent. Shake appear travel which as forward agree.\\nOrder life manager suggest election. Probably sell medical nothing beautiful push paper culture.\\nTry nothing increase have any feeling field new. Identify firm line plant family great season. Star risk his garden task suggest north audience. Power so stage only.\\nTree common window. Today firm involve effort national water billion day. Control role fear behavior history next avoid you.\\nLive lose end place exactly. Begin table tend bag range.\\nFire hot partner least.\\nAsk whatever board impact. Ready affect east suffer state.\\nPlay after oil sometimes century reality through. Pressure region ability full person suggest when.\\nEye president maintain wind. Never key method. Test southern care scene recently media third.\\nReally million food where role beyond film. Show idea college fill. Never already research for risk run here not.\\nWalk accept citizen expect see. General although none draw.\\nStock return pass member. Court player he.\\nCut teacher compare around sport. Inside loss rich son pattern north already structure.\\nNewspaper bit risk specific real she five. Because responsibility across whatever about game. Visit seat owner agency.\\nElse nothing fly method business bill member. Reflect new energy change long. Certainly mean than young home.\\nAbout short sign week so. Myself decide movement.\\nMust good system traditional recognize huge. Relate sport film name eye.\\nArrive central sing goal right many. Current street very. Daughter individual technology bank head.\\nMoment let water system. Health open sort of.' age=45 bmi=6.97027831098843 metadata=None \n",
      "---\n",
      " name='Jack' age=25 bmi=23.5 metadata=None \n",
      "---\n",
      " {'cost_min': 0.00198, 'cost_max': 0.12486, 'time_min': 0.0, 'time_max': 73.728, 'input_tokens': 66, 'output_tokens_min': 0, 'output_tokens_max': 2048, 'calls': 1, 'model': 'gpt-4', 'simulated': True, 'input_string': None, 'messages': [{'content': 'Hey', 'role': 'user'}], 'output_string': None, 'description': None} \n",
      "---\n",
      " {'cost_min': 0.00399, 'cost_max': 0.00399, 'time_min': 2.413704999984475, 'time_max': 2.413704999984475, 'input_tokens': 81, 'output_tokens': 26, 'output_tokens_min': 26, 'output_tokens_max': 26, 'calls': 1, 'model': 'gpt-4', 'simulated': False, 'input_string': None, 'messages': [{'content': 'Hey', 'role': 'user'}], 'output_string': FOOMODEL(name='Jack', age=25, bmi=23.5, metadata=None), 'description': None}\n"
     ]
    }
   ],
   "source": [
    "import instructor\n",
    "from pydantic import BaseModel\n",
    "from instructor import Instructor\n",
    "from typing import Any\n",
    "from openai import OpenAI\n",
    "from costly import Costlog, costly, CostlyResponse\n",
    "from costly.estimators.llm_api_estimation import LLM_API_Estimation\n",
    "\n",
    "\n",
    "@costly(\n",
    "    input_tokens=lambda kwargs: LLM_API_Estimation.get_input_tokens_instructor(\n",
    "        **kwargs\n",
    "    ),\n",
    ")\n",
    "def chatgpt_instructor(\n",
    "    messages: list[dict[str, str]],\n",
    "    model: str,\n",
    "    client: Instructor,\n",
    "    response_model: BaseModel,\n",
    ") -> str:\n",
    "    response = client.chat.completions.create_with_completion(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        response_model=response_model,\n",
    "    )\n",
    "    # print(\n",
    "    #     LLM_API_Estimation._get_raw_messages_instructor(\n",
    "    #         messages=messages,\n",
    "    #         model=model,\n",
    "    #         client=client,\n",
    "    #         response_model=response_model,\n",
    "    #         process=False,\n",
    "    #     )\n",
    "    # )\n",
    "    output_string, cost_info = response\n",
    "    return CostlyResponse(\n",
    "        output=output_string,\n",
    "        cost_info={\n",
    "            \"input_tokens\": cost_info.usage.prompt_tokens,\n",
    "            \"output_tokens\": cost_info.usage.completion_tokens,\n",
    "        },\n",
    "    )\n",
    "\n",
    "\n",
    "class PersonInfo(BaseModel):\n",
    "    name: str\n",
    "    age: int\n",
    "\n",
    "class FOOMODEL(BaseModel):\n",
    "    name: str\n",
    "    age: int\n",
    "    bmi: float\n",
    "    metadata: dict[str, Any] | None = None\n",
    "\n",
    "\n",
    "class BARMODEL(BaseModel):\n",
    "    foo: FOOMODEL\n",
    "    fookids: list[FOOMODEL]\n",
    "\n",
    "cl = Costlog()\n",
    "x = chatgpt_instructor(\n",
    "    messages=[{\"content\": \"Hey\", \"role\": \"user\"}],\n",
    "    model=\"gpt-4\",\n",
    "    response_model=FOOMODEL,\n",
    "    client=instructor.from_openai(OpenAI()),\n",
    "    simulate=True,\n",
    "    cost_log=cl,\n",
    ")\n",
    "y = chatgpt_instructor(\n",
    "    messages=[{\"content\": \"Hey\", \"role\": \"user\"}],\n",
    "    model=\"gpt-4\",\n",
    "    response_model=FOOMODEL,\n",
    "    client=instructor.from_openai(OpenAI()),\n",
    "    simulate=False,\n",
    "    cost_log=cl,\n",
    ")\n",
    "print(x, \"\\n---\\n\", y, \"\\n---\\n\", cl.items[0], \"\\n---\\n\", cl.items[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## costlog-notes\n",
    "\n",
    "The default [`costly.Costlog`](costly/costlog.py) class has two modes: `memory` and `jsonl`. The default is `memory`, but for large projects you may want to use `jsonl`: this dumps the cost log into a `.costly` folder in your working directory.\n",
    "\n",
    "The other thing that can be customized is the `totals_keys` parameter, which is a set of keys to aggregate costs by. By default it is `{\"cost_min\", \"cost_max\", \"time_min\", \"time_max\", \"calls\"}`, i.e. it tracks the range of possible costs and running times (`max` and `min` are usually only different when simulating because then you have to estimate). Out-of-the box you can customize it to also track `input_tokens`, `output_tokens_min`, `output_tokens_max`; any other customizations will only make sense if you are using your own estimator.\n",
    "\n",
    "You will want to change `totals_keys` if you want to use this package for things other than LLM costs, or if you want to track something other than `min` and `max`, e.g. some estimate of the average, or percentiles or whatever."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## invoice\n",
    "\n",
    "While the `@costly()` decorator goes on your \"load-bearing\" functions (e.g. your LLM wrapper), the `@invoice()` decorator goes on your \"high-level\" functions whose usage you want to be reported in your final `Costlog` to see a breakdown. This adds a `description` argument to your function, and as long as this argument is passed unbroken to your load-bearing functions (e.g. via `**kwargs`), you Costlog items will contain a `description` field tracing where they came from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'func': 'manufacture_cups', 'args': [], 'kwargs': {}}, 'manufacturing steel', 'mining iron']\n",
      "[{'func': 'manufacture_cups', 'args': [], 'kwargs': {}}, 'educating potters', 'capturing enemies']\n"
     ]
    }
   ],
   "source": [
    "from costly import invoice\n",
    "\n",
    "@invoice()\n",
    "def manufacture_cups(**kwargs):\n",
    "    manufacture_steel(**kwargs)\n",
    "    educate_potters(**kwargs)\n",
    "    return\n",
    "\n",
    "@invoice(\"educating potters\")\n",
    "def educate_potters(**kwargs):\n",
    "    capture_enemies(**kwargs)\n",
    "    return\n",
    "\n",
    "@invoice(\"manufacturing steel\")\n",
    "def manufacture_steel(**kwargs):\n",
    "    mine_iron(**kwargs)\n",
    "    return\n",
    "\n",
    "@invoice(\"mining iron\")\n",
    "def mine_iron(**kwargs):\n",
    "    print(kwargs[\"description\"])\n",
    "    return\n",
    "\n",
    "@invoice(\"capturing enemies\")\n",
    "def capture_enemies(**kwargs):\n",
    "    print(kwargs[\"description\"])\n",
    "    return\n",
    "\n",
    "manufacture_cups()\n",
    "### prints:\n",
    "# [{'func': 'manufacture_cups', 'args': [], 'kwargs': {}}, 'manufacturing steel', 'mining iron']\n",
    "# [{'func': 'manufacture_cups', 'args': [], 'kwargs': {}}, 'educating potters', 'capturing enemies']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If description is not provided, it will default to the function's name and arguments like above.\n",
    "\n",
    "`@invoice` uses the more general decorator [`@updatable_default`](costly/updatable_default.py); see the documentation therein it's kinda cool."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
