{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Costly\n",
    "\n",
    "Costly adds a `simulate` argument and a cost logger to your function. The idea is this cost logger logs the cost of your function calls if `simulate=False`, and reasonably estimates it if `simulate=True`.\n",
    "\n",
    "The default behaviour is for LLM API calls, and uses `LLM_Simulator_Faker.simulate_llm_call()` as the simulator and `LLM_API_Estimation.get_cost_real()`, `LLM_API_Estimation.get_cost_simulating()` as the estimator.\n",
    "\n",
    "## Quick start\n",
    "\n",
    "Just mark the function responsible for your costs with `@costly()` decorator, as follows.\n",
    "\n",
    "This will only work sensibly out of the box if your function is doing an LLM API call, and takes the arguments `input_string`, `model` (and optionally `response_model` if the response is expected to be a complex object e.g. a Pydantic model) and returns a string (or a `response_model` object if you specified one).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Here is the traditional Lorem Ipsum text:\n",
      "\n",
      "```\n",
      "Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\n",
      "```\n",
      "\n",
      "If you need a longer or different variation, just let me know!\n",
      "Two the return sure age might. As attorney maybe especially million movie. Realize cut everything.\n",
      "Determine event car cup begin over nation. Sound build itself attention finally.\n",
      "Case hotel with. Control receive film safe tonight blood.\n",
      "Skin national play friend return. Section enter hundred need hundred. Himself health image he leave.\n",
      "Market though family really clear ahead almost. Gas political customer phone.\n",
      "Challenge themselves by. Interest possible particular major.\n",
      "Include defense then physical information reveal suddenly fine. Cause piece brother professor its buy throughout they. Detail rate key national let name somebody.\n",
      "Reflect window rock several side. Analysis there city serious girl culture could science. Leg speak school still.\n",
      "Edge send have whether. Rule response bed mean movie natural effect.\n",
      "Line in player mean tonight outside. Outside might simple house structure office assume. From might eat stuff.\n",
      "Much wind necessary option owner line hold. Only recognize relate effort action. Rich sure among bank from.\n",
      "Still stop thank positive. Of white name major analysis condition understand.\n",
      "Major blue still nature design miss. Half show nothing power bed state interview.\n",
      "Throw return land. Baby culture truth already. Resource sense summer yourself accept eat.\n",
      "Center deal have. Around social amount.\n",
      "Another look table it. Reveal law bill dog when commercial reduce.\n",
      "Note culture behavior health serious bit. Weight able traditional. List model clear again set bag.\n",
      "Happy different fight see hit else certainly both. Because cover camera economic successful west cold. Develop anyone walk story network.\n",
      "Goal discover blood. Environment stay account soon control field throughout too. Suffer condition commercial live out blood.\n",
      "Something adult list service team. Yourself station movement what. Financial history person door party staff charge visit.\n",
      "Magazine product worry theory. Thing imagine third that. Already actually occur man.\n",
      "Include defense significant its computer appear everybody. President race wear book.\n",
      "Them certainly heavy section. Probably ago past throw. Break water knowledge adult while account concern.\n",
      "Blood call agree system age. Enter news one open.\n",
      "Throw center behind thus think determine theory. Language bring finally traditional leg step charge determine. Forget remain above green tonight teacher.\n",
      "Interesting forward local name after beautiful cultural. Place pay look law garden.\n",
      "Parent bank move alone raise law general. Paper along vote involve. Mr form Mrs television street.\n",
      "Among attention deep you fish manager. Language so evidence hour adult computer country.\n",
      "{'cost_min': 7.59e-05, 'time_max': 20.293083100026006, 'cost_max': 0.0013047, 'time_min': 1.8610831000260077, 'calls': 2.0}\n",
      "{'cost_min': 7.515e-05, 'cost_max': 7.515e-05, 'time_min': 1.8610831000260077, 'time_max': 1.8610831000260077, 'input_tokens': 5, 'output_tokens': 124, 'output_tokens_min': 124, 'output_tokens_max': 124, 'calls': 1, 'model': 'gpt-4o-mini', 'simulated': False, 'input_string': 'Write the Lorem ipsum text', 'output_string': 'Sure! Here is the traditional Lorem Ipsum text:\\n\\n```\\nLorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\\n```\\n\\nIf you need a longer or different variation, just let me know!', 'description': ['chatgpt call']}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'cost_min': 7.5e-07,\n",
       " 'cost_max': 0.00122955,\n",
       " 'time_min': 0.0,\n",
       " 'time_max': 18.432,\n",
       " 'input_tokens': 5,\n",
       " 'output_tokens_min': 0,\n",
       " 'output_tokens_max': 2048,\n",
       " 'calls': 1,\n",
       " 'model': 'gpt-4o-mini',\n",
       " 'simulated': True,\n",
       " 'input_string': 'Write the Lorem ipsum text',\n",
       " 'output_string': None,\n",
       " 'description': ['chatgpt call']}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from costly import Costlog, costly\n",
    "\n",
    "@costly()\n",
    "def chatgpt(input_string: str, model: str) -> str:\n",
    "    from openai import OpenAI\n",
    "    client = OpenAI()\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": input_string}]\n",
    "    )\n",
    "    output_string = response.choices[0].message.content\n",
    "    return output_string\n",
    "\n",
    "\n",
    "    \n",
    "cl = Costlog()\n",
    "x = chatgpt(input_string=\"Write the Lorem ipsum text\", model = \"gpt-4o-mini\", cost_log=cl, simulate=False, description=[\"chatgpt call\"])\n",
    "y = chatgpt(input_string=\"Write the Lorem ipsum text\", model = \"gpt-4o-mini\", cost_log=cl, simulate=True, description=[\"chatgpt call\"])\n",
    "print(x)\n",
    "print(y)\n",
    "print(cl.totals)\n",
    "print(cl.items[0])\n",
    "cl.items[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's basically what's happening under the hood, courtesy of `costly.decorator.costly`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from costly.simulators.llm_simulator_faker import LLM_Simulator_Faker\n",
    "from costly.estimators.llm_api_estimation import LLM_API_Estimation\n",
    "\n",
    "def _chatgpt(input_string: str, model: str) -> str:\n",
    "    from openai import OpenAI\n",
    "    client = OpenAI()\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": input_string}]\n",
    "    )\n",
    "    output_string = response.choices[0].message.content\n",
    "    return output_string\n",
    "\n",
    "def chatgpt(input_string: str, model: str, cost_log: Costlog=None, simulate: bool=False, description: list[str]=None) -> str:\n",
    "    if simulate:\n",
    "        return LLM_Simulator_Faker.simulate_llm_call(\n",
    "            input_string=input_string,\n",
    "            model=model,\n",
    "            response_model=str,\n",
    "            cost_log=cost_log,\n",
    "            description=description,\n",
    "        )\n",
    "    if cost_log is not None:\n",
    "        with cost_log.new_item() as (item, timer):\n",
    "            output_string = _chatgpt(input_string, model)\n",
    "            cost_item = LLM_API_Estimation.get_cost_real(\n",
    "                model=model,\n",
    "                input_string=input_string,\n",
    "                output_string=output_string,\n",
    "                description=description,\n",
    "                timer=timer(),\n",
    "            )\n",
    "            item.update(cost_item)\n",
    "    else:\n",
    "        output_string = _chatgpt(input_string, model)\n",
    "    return output_string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customizations\n",
    "\n",
    "Although the library is designed for LLM API calls, it can be extended to estimating other types of costs with some customization and subclassing.\n",
    "\n",
    "Customizations you can do:\n",
    "\n",
    "### different simulators and estimators\n",
    "\n",
    "The defualt decorator behaviour is\n",
    "\n",
    "```python\n",
    "@costly(\n",
    "    simulator=LLM_Simulator_Faker.simulate_llm_call,\n",
    "    estimator=LLM_API_Estimation.get_cost_real,\n",
    ")\n",
    "```\n",
    "\n",
    "These functions can be replaced by your own custom functions. For reference, you can see how the default ones are implemented in [`costly.simulators.llm_simulator_faker`](costly/simulators/llm_simulator_faker.py) and [`costly.estimators.llm_api_estimation`](costly/estimators/llm_api_estimation.py).\n",
    "\n",
    "Also, the simulator and the estimator both have very specific type signatures:\n",
    "\n",
    "```python\n",
    "class LLM_Simulator_Faker:\n",
    "\n",
    "    @staticmethod\n",
    "    def simulate_llm_call(\n",
    "        input_string: str,\n",
    "        model: str = None,\n",
    "        response_model: type = str,\n",
    "        cost_log: Costlog = None,\n",
    "        description: list[str] = None,\n",
    "    ) -> str | Any:\n",
    "        ...\n",
    "\n",
    "class LLM_API_Estimation:\n",
    "\n",
    "    @staticmethod\n",
    "    def get_cost_real(\n",
    "        model: str,\n",
    "        input_tokens: int = None,\n",
    "        output_tokens_min: int = None,\n",
    "        output_tokens_max: int = None,\n",
    "        input_string: str = None,\n",
    "        output_string: str = None,\n",
    "        timer: float = None,\n",
    "        **kwargs,\n",
    "    ) -> dict[str, float]:\n",
    "        ...\n",
    "```\n",
    "\n",
    "So e.g. if your function takes different argument names -- say `prompt`, `model_name` and `response_type` -- you can change the decorator to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cost_min': 1.55e-05,\n",
       " 'cost_max': 1.55e-05,\n",
       " 'time_min': 0.6242734999977984,\n",
       " 'time_max': 0.6242734999977984,\n",
       " 'input_tokens': 1,\n",
       " 'output_tokens': 10,\n",
       " 'output_tokens_min': 10,\n",
       " 'output_tokens_max': 10,\n",
       " 'calls': 1,\n",
       " 'model': 'gpt-3.5-turbo',\n",
       " 'simulated': False,\n",
       " 'input_string': 'Hello',\n",
       " 'output_string': 'Hi there! How can I assist you today?',\n",
       " 'description': None}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from costly.simulators.llm_simulator_faker import LLM_Simulator_Faker\n",
    "from costly.estimators.llm_api_estimation import LLM_API_Estimation\n",
    "\n",
    "@costly(\n",
    "    simulator=lambda prompt, model_name, response_type=str, cost_log=None, description=None: LLM_Simulator_Faker.simulate_llm_call(\n",
    "        input_string=prompt,\n",
    "        model=model_name,\n",
    "        response_model=response_type,\n",
    "        cost_log=cost_log,\n",
    "        description=description,\n",
    "    ),\n",
    "    estimator=lambda model_name, prompt, output_string, description, timer: LLM_API_Estimation.get_cost_real(\n",
    "        model=model_name,\n",
    "        input_string=prompt,\n",
    "        output_string=output_string,\n",
    "        description=description,\n",
    "        timer=timer,\n",
    "    ),\n",
    ")\n",
    "def chatgpt2(prompt: str, model_name: str) -> str:\n",
    "    from openai import OpenAI\n",
    "\n",
    "    client = OpenAI()\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name, messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    output_string = response.choices[0].message.content\n",
    "    return output_string\n",
    "\n",
    "\n",
    "cl = Costlog(totals_keys={\"cost_min\", \"cost_max\", \"time_min\", \"time_max\", \"calls\", \"input_tokens\", \"output_tokens_min\", \"output_tokens_max\"})\n",
    "chatgpt2(prompt=\"Hello\", model_name=\"gpt-3.5-turbo\", simulate=False, cost_log=cl)\n",
    "cl.items[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Costlog customizations\n",
    "\n",
    "The default [`costly.Costlog`](costly/costlog.py) class has two modes: `memory` and `jsonl`. The default is `memory`, but for large projects you may want to use `jsonl`: this dumps the cost log into a `.costly` folder in your working directory.\n",
    "\n",
    "The other thing that can be customized is the `totals_keys` parameter, which is a set of keys to aggregate costs by. By default it is `{\"cost_min\", \"cost_max\", \"time_min\", \"time_max\", \"calls\"}`, i.e. it tracks the range of possible costs and running times (`max` and `min` are usually only different when simulating because then you have to estimate). Out-of-the box you can customize it to also track `input_tokens`, `output_tokens_min`, `output_tokens_max`; any other customizations will only make sense if you are using your own estimator.\n",
    "\n",
    "### Simulator\n",
    "\n",
    "[`costly.simulators.llm_simulator_faker`](costly/simulators/llm_simulator_faker.py) has some examples of how to subclass it.\n",
    "\n",
    "One obvious reason to subclass it is to have custom simulating functions for the types you are interested in. Although the default class \"works\" for any Pydantic basemodel etc., you might want to have a custom function -- e.g. if a value needs to be within a certain range, or if its distribution is not uniform, or if you want to use examples from your data, etc.\n",
    "\n",
    "Also, the simulator has a very specific type signature:\n",
    "\n",
    "```python\n",
    "class LLM_Simulator_Faker:\n",
    "\n",
    "    @staticmethod\n",
    "    def simulate_llm_call(\n",
    "        input_string: str,\n",
    "        model: str = None,\n",
    "        response_model: type = str,\n",
    "        cost_log: Costlog = None,\n",
    "        description: list[str] = None,\n",
    "    ) -> str | Any:\n",
    "        ...\n",
    "```\n",
    "\n",
    "So it would make sense to subclass it to just change this function so you don't have to do that ridiculous lambda thing above and can just use `@costly()`.\n",
    "\n",
    "### Estimator\n",
    "\n",
    "Again [`costly.estimators.llm_api_estimation`](costly/estimators/llm_api_estimation.py) has some examples of how to subclass it. The most obvious reason would be to add prices for other models we don't have listed (right now it's just OpenAI and Anthropic). The `PRICES` dict is like this:\n",
    "\n",
    "```python\n",
    "class LLM_API_Estimation:\n",
    "\n",
    "    PRICES = {\n",
    "        \"gpt-4o\": {\n",
    "            \"input_tokens\": 5.0e-6,\n",
    "            \"output_tokens\": 15.0e-6,\n",
    "            \"time\": 18e-3,\n",
    "        },\n",
    "        \"gpt-4o-mini\": {\n",
    "            \"input_tokens\": 0.15e-6,\n",
    "            \"output_tokens\": 0.6e-6,\n",
    "            \"time\": 9e-3,\n",
    "        },\n",
    "    }\n",
    "```\n",
    "\n",
    "Something like this would be quite natural:\n",
    "\n",
    "```python\n",
    "class My_Estimation(LLM_API_Estimation):\n",
    "    PRICES = LLM_API_Estimation.PRICES | {\"my_model\": LLM_API_Estimation.PRICES[\"gpt-4o\"]}\n",
    "```\n",
    "\n",
    "Note that `LLM_API_Estimation` _can_ handle things like `gpt-4o-2024-05-13` etc. because it `LLM_API_Estimation.get_model()` gets the longest prefix matching model name in `PRICES`. \n",
    "\n",
    "### Some assumptions made\n",
    "\n",
    "`LLM_Simulator_Faker`, when producing text, produces text of about `600 * 4.5` characters.\n",
    "\n",
    "Generally we assume that 1 token is about 4.5 characters. Though actual token estimation does use `tiktoken` (unless you subclass `LLM_API_Estimation` and set `tokenize=_tokenize_rough`).\n",
    "\n",
    "Generally we assume, for cost estimation, that output tokens are in the range `[0, 2048]`, and the min and max are computed accordingly. As a rule of thumb for complex projects the true value tends to be about 1/3 the way through, and for projects that receive quite short responses it would be much lower.\n",
    "\n",
    "\n",
    "All of this can be overriden by subclassing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Misc stuff\n",
    "\n",
    "### messages, instructor etc.\n",
    "\n",
    "### local model support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whether need I manage this unit.\n",
      "Within bag truth suggest every turn. Easy inside along professor act structure degree. Lose music Republican safe. Base remember professor partner.\n",
      "Be near more stuff. Single method government. Should compare fear window collection so. Difference like decade family describe strategy second alone.\n",
      "Action job yard science never quality. Cause contain though.\n",
      "Rest loss capital score check. Southern American community safe community nor. Road national artist reduce step seven.\n",
      "Another whole blue table today may. Cost long capital piece.\n",
      "Three car rate hair. Subject painting continue executive throw network.\n",
      "Six attention cup city at add. Several continue ask evidence. Detail responsibility guy lay occur president investment.\n",
      "Model often outside behavior agreement or forget. Building star customer certainly.\n",
      "Player fear national tonight of. Style by before strategy action sing. Turn ground return current.\n",
      "Interview concern home doctor. Hear hundred method leave.\n",
      "Oil example bill institution radio. Treatment must mother usually bar center sing. Everyone another important dream.\n",
      "Among hand machine month. Everything like individual whole close bank media.\n",
      "Company individual rest wide term now level pull. Important government hundred when kitchen fund.\n",
      "Improve window popular necessary environment region war. Test institution everything three.\n",
      "See public candidate too.\n",
      "Sign establish leader another challenge. Strategy expert or since hair.\n",
      "Player force worker reach evening such. Wind father certain Mrs.\n",
      "Specific again certain matter. Child that leg benefit fund rate.\n",
      "Foot almost woman after build partner. Personal capital tonight staff kitchen camera.\n",
      "Act different mission partner teacher effort. Over agree administration stuff bit ever article. Congress later area radio. Hair medical mind class pay baby.\n",
      "Never indicate phone leave follow chair land. Stage difference office range city and project. Rich which while ok many.\n",
      "International sell question. Reflect responsibility course himself society her three street.\n",
      "Live room close write time side to image. Technology production much keep could talk.\n",
      "Consider member executive environmental.\n",
      "Future door create. Join loss wrong past recent both. Significant turn far past.\n",
      "Task moment that positive key. Discover moment anyone city play. Room adult member certainly story. Yeah door indeed feel end air sometimes.\n",
      "Stand lose that find assume character.\n",
      "Together pass outside dinner usually plan majority. Care ready political exactly. Member soldier forward from important keep sport.\n",
      "Finish purpose carry good you.\n",
      "Election capital law somebody exactly find. Any garden environment.\n"
     ]
    }
   ],
   "source": [
    "from typing import Optional\n",
    "from costly.costlog import Costlog\n",
    "from costly.simulators.llm_simulator_faker import LLM_Simulator_Faker\n",
    "from costly.estimators.llm_api_estimation import LLM_API_Estimation\n",
    "from time import sleep\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any, Optional, Union\n",
    "costlog = Costlog()\n",
    "\n",
    "with costlog.new_item() as (item, timer):\n",
    "    item.update({\"Hi\": \"Hello\"})\n",
    "\n",
    "x=LLM_Simulator_Faker.simulate_llm_call(\n",
    "    input_string=\"Hello\",\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    response_model=dict[str, Optional[list]] | str | None,\n",
    "    cost_log=costlog,\n",
    ")\n",
    "print(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
