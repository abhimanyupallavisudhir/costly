{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Costly\n",
    "\n",
    "Costly adds a `simulate` argument and a cost logger to your function. The idea is this cost logger logs the cost of your function calls if `simulate=False`, and reasonably estimates it if `simulate=True`.\n",
    "\n",
    "The default behaviour is for LLM API calls, and uses `LLM_Simulator_Faker.simulate_llm_call()` as the simulator and `LLM_API_Estimation.get_cost_real()`, `LLM_API_Estimation.get_cost_simulating()` as the estimator.\n",
    "\n",
    "**Table of contents**\n",
    "\n",
    "- [Quick start](#quick-start)\n",
    "- [Customizations](#customizations)\n",
    "  - [different simulators and estimators](#different-simulators-and-estimators)\n",
    "  - [Costlog customizations](#costlog-customizations)\n",
    "  - [Simulator](#simulator)\n",
    "  - [Estimator](#estimator)\n",
    "- [Some assumptions made](#some-assumptions-made)\n",
    "- [Calculating costs within a function](#calculating-costs-within-a-function)\n",
    "\n",
    "## Quick start\n",
    "\n",
    "Just mark the function responsible for your costs with `@costly()` decorator, as follows.\n",
    "\n",
    "This will only work sensibly out of the box if your function is doing an LLM API call, and takes the arguments `input_string`, `model` (and optionally `response_model` if the response is expected to be a complex object e.g. a Pydantic model) and returns a string (or a `response_model` object if you specified one).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abhim\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\requests\\__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.18) or chardet (5.2.0)/charset_normalizer (2.0.10) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly! Here is the classic \"Lorem Ipsum\" text:\n",
      "\n",
      "```\n",
      "Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\n",
      "```\n",
      "\n",
      "Let me know if you need more text or any variations!\n",
      "Quite rather later challenge parent research important. Recent worry thought of nation past.\n",
      "Knowledge become I together. Analysis series share past here quality. Cell south TV water upon fall.\n",
      "Institution daughter available into collection family. Government lose notice finish less life.\n",
      "Professional executive window whatever. Degree garden collection important each. Environment probably reason very rise pass. President make write believe conference fish quality.\n",
      "Leg executive everyone similar hold every or. About every generation at box page where receive. Kind part hit case growth ten run.\n",
      "There else benefit police. Interview trial appear voice sit identify finish. Worker rock feeling week that.\n",
      "Medical region coach anything perhaps compare guess. It why stand bring drop get machine. Enjoy car opportunity threat training forget space.\n",
      "Military decide cup. Support mouth morning.\n",
      "Who lawyer wonder fine character. Fight play early lead network. Their Congress fund Mrs nearly but organization build.\n",
      "Mother daughter hot computer travel. Call drive send everybody number computer radio.\n",
      "Cover above quite development game eight. Single dark camera alone company cell establish. Job still drop right myself.\n",
      "Impact build be particular interest world add. Station author development order mouth for.\n",
      "Collection indicate president actually candidate gun. Next election college spend government hear. Blue another course American challenge seven real.\n",
      "Reality water section go peace. Charge tax part thing.\n",
      "Base position pretty manager. Which that any drive. Quality debate big management think true.\n",
      "Defense audience production animal. Meet act hair unit risk check. Worker gun three drug every.\n",
      "Word responsibility let own. Fly behavior leg only consider friend.\n",
      "Grow me throughout. Claim race director assume member break least. End husband necessary office.\n",
      "Hand air suddenly suffer degree American. Statement if agreement factor challenge dark.\n",
      "Interview base open surface player follow job. Production city name force population. Understand fall safe military may story whatever. Less enjoy serious war rate her heart.\n",
      "Generation own test military none practice price he. Player travel growth data provide.\n",
      "Agree wall course performance look of chair. Another matter hold occur. Practice course term various customer raise.\n",
      "Way who high always try security maybe drug. Service what eat.\n",
      "Method although my commercial. Group central chance party that those shake. Large example its.\n",
      "Very president because traditional alone although carry check. Car right oil late friend. Bar head technology control.\n",
      "{'time_max': 20.40452720000688, 'cost_min': 7.59e-05, 'time_min': 1.9725272000068799, 'calls': 2.0, 'cost_max': 0.0013047}\n",
      "{'cost_min': 7.515e-05, 'cost_max': 7.515e-05, 'time_min': 1.9725272000068799, 'time_max': 1.9725272000068799, 'input_tokens': 5, 'output_tokens': 124, 'output_tokens_min': 124, 'output_tokens_max': 124, 'calls': 1, 'model': 'gpt-4o-mini', 'simulated': False, 'input_string': 'Write the Lorem ipsum text', 'output_string': 'Certainly! Here is the classic \"Lorem Ipsum\" text:\\n\\n```\\nLorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\\n```\\n\\nLet me know if you need more text or any variations!', 'description': ['chatgpt call']}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'cost_min': 7.5e-07,\n",
       " 'cost_max': 0.00122955,\n",
       " 'time_min': 0.0,\n",
       " 'time_max': 18.432,\n",
       " 'input_tokens': 5,\n",
       " 'output_tokens_min': 0,\n",
       " 'output_tokens_max': 2048,\n",
       " 'calls': 1,\n",
       " 'model': 'gpt-4o-mini',\n",
       " 'simulated': True,\n",
       " 'input_string': 'Write the Lorem ipsum text',\n",
       " 'output_string': None,\n",
       " 'description': ['chatgpt call']}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from costly import Costlog, costly\n",
    "\n",
    "@costly()\n",
    "def chatgpt(input_string: str, model: str) -> str:\n",
    "    from openai import OpenAI\n",
    "    client = OpenAI()\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": input_string}]\n",
    "    )\n",
    "    output_string = response.choices[0].message.content\n",
    "    return output_string\n",
    "\n",
    "\n",
    "    \n",
    "cl = Costlog()\n",
    "x = chatgpt(input_string=\"Write the Lorem ipsum text\", model = \"gpt-4o-mini\", cost_log=cl, simulate=False, description=[\"chatgpt call\"])\n",
    "y = chatgpt(input_string=\"Write the Lorem ipsum text\", model = \"gpt-4o-mini\", cost_log=cl, simulate=True, description=[\"chatgpt call\"])\n",
    "print(x)\n",
    "print(y)\n",
    "print(cl.totals)\n",
    "print(cl.items[0])\n",
    "cl.items[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's basically what's happening under the hood, courtesy of `costly.decorator.costly`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from costly.simulators.llm_simulator_faker import LLM_Simulator_Faker\n",
    "from costly.estimators.llm_api_estimation import LLM_API_Estimation\n",
    "\n",
    "def _chatgpt(input_string: str, model: str) -> str:\n",
    "    from openai import OpenAI\n",
    "    client = OpenAI()\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": input_string}]\n",
    "    )\n",
    "    output_string = response.choices[0].message.content\n",
    "    return output_string\n",
    "\n",
    "def chatgpt(input_string: str, model: str, cost_log: Costlog=None, simulate: bool=False, description: list[str]=None) -> str:\n",
    "    if simulate:\n",
    "        return LLM_Simulator_Faker.simulate_llm_call(\n",
    "            input_string=input_string,\n",
    "            model=model,\n",
    "            response_model=str,\n",
    "            cost_log=cost_log,\n",
    "            description=description,\n",
    "        )\n",
    "    if cost_log is not None:\n",
    "        with cost_log.new_item() as (item, timer):\n",
    "            output_string = _chatgpt(input_string, model)\n",
    "            cost_item = LLM_API_Estimation.get_cost_real(\n",
    "                model=model,\n",
    "                input_string=input_string,\n",
    "                output_string=output_string,\n",
    "                description=description,\n",
    "                timer=timer(),\n",
    "            )\n",
    "            item.update(cost_item)\n",
    "    else:\n",
    "        output_string = _chatgpt(input_string, model)\n",
    "    return output_string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customizations\n",
    "\n",
    "Although the library is designed for LLM API calls, it can be extended to estimating other types of costs with some customization and subclassing.\n",
    "\n",
    "Customizations you can do:\n",
    "\n",
    "### different simulators and estimators\n",
    "\n",
    "The defualt decorator behaviour is\n",
    "\n",
    "```python\n",
    "@costly(\n",
    "    simulator=LLM_Simulator_Faker.simulate_llm_call,\n",
    "    estimator=LLM_API_Estimation.get_cost_real,\n",
    ")\n",
    "```\n",
    "\n",
    "These functions can be replaced by your own custom functions. For reference, you can see how the default ones are implemented in [`costly.simulators.llm_simulator_faker`](costly/simulators/llm_simulator_faker.py) and [`costly.estimators.llm_api_estimation`](costly/estimators/llm_api_estimation.py).\n",
    "\n",
    "Also, the simulator and the estimator both have very specific type signatures:\n",
    "\n",
    "```python\n",
    "class LLM_Simulator_Faker:\n",
    "\n",
    "    @staticmethod\n",
    "    def simulate_llm_call(\n",
    "        input_string: str,\n",
    "        model: str = None,\n",
    "        response_model: type = str,\n",
    "        cost_log: Costlog = None,\n",
    "        description: list[str] = None,\n",
    "    ) -> str | Any:\n",
    "        ...\n",
    "\n",
    "class LLM_API_Estimation:\n",
    "\n",
    "    @staticmethod\n",
    "    def get_cost_real(\n",
    "        model: str,\n",
    "        input_tokens: int = None,\n",
    "        output_tokens_min: int = None,\n",
    "        output_tokens_max: int = None,\n",
    "        input_string: str = None,\n",
    "        output_string: str = None,\n",
    "        timer: float = None,\n",
    "        **kwargs,\n",
    "    ) -> dict[str, float]:\n",
    "        ...\n",
    "```\n",
    "\n",
    "So e.g. if your function takes different argument names -- say `prompt`, `model_name` and `response_type` -- you can change the decorator to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cost_min': 1.4e-05,\n",
       " 'cost_max': 1.4e-05,\n",
       " 'time_min': 0.5816699999850243,\n",
       " 'time_max': 0.5816699999850243,\n",
       " 'input_tokens': 1,\n",
       " 'output_tokens': 9,\n",
       " 'output_tokens_min': 9,\n",
       " 'output_tokens_max': 9,\n",
       " 'calls': 1,\n",
       " 'model': 'gpt-3.5-turbo',\n",
       " 'simulated': False,\n",
       " 'input_string': 'Hello',\n",
       " 'output_string': 'Hello! How can I assist you today?',\n",
       " 'description': None}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from costly.simulators.llm_simulator_faker import LLM_Simulator_Faker\n",
    "from costly.estimators.llm_api_estimation import LLM_API_Estimation\n",
    "\n",
    "@costly(\n",
    "    simulator=lambda prompt, model_name, response_type=str, cost_log=None, description=None: LLM_Simulator_Faker.simulate_llm_call(\n",
    "        input_string=prompt,\n",
    "        model=model_name,\n",
    "        response_model=response_type,\n",
    "        cost_log=cost_log,\n",
    "        description=description,\n",
    "    ),\n",
    "    estimator=lambda model_name, prompt, output_string, description, timer: LLM_API_Estimation.get_cost_real(\n",
    "        model=model_name,\n",
    "        input_string=prompt,\n",
    "        output_string=output_string,\n",
    "        description=description,\n",
    "        timer=timer,\n",
    "    ),\n",
    ")\n",
    "def chatgpt2(prompt: str, model_name: str) -> str:\n",
    "    from openai import OpenAI\n",
    "\n",
    "    client = OpenAI()\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name, messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    output_string = response.choices[0].message.content\n",
    "    return output_string\n",
    "\n",
    "\n",
    "cl = Costlog(totals_keys={\"cost_min\", \"cost_max\", \"time_min\", \"time_max\", \"calls\", \"input_tokens\", \"output_tokens_min\", \"output_tokens_max\"})\n",
    "chatgpt2(prompt=\"Hello\", model_name=\"gpt-3.5-turbo\", simulate=False, cost_log=cl)\n",
    "cl.items[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately, life isn't so hard, and there is a less cumbersome way of doing this in the case of remapping/computing variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cost_min': 1.4e-05,\n",
       " 'cost_max': 1.4e-05,\n",
       " 'time_min': 2.4095481999684125,\n",
       " 'time_max': 2.4095481999684125,\n",
       " 'input_tokens': 1,\n",
       " 'output_tokens': 9,\n",
       " 'output_tokens_min': 9,\n",
       " 'output_tokens_max': 9,\n",
       " 'calls': 1,\n",
       " 'model': 'gpt-3.5-turbo',\n",
       " 'simulated': False,\n",
       " 'input_string': 'Hello',\n",
       " 'output_string': 'Hello! How can I assist you today?',\n",
       " 'description': None}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from costly import Costlog, costly\n",
    "from costly.simulators.llm_simulator_faker import LLM_Simulator_Faker\n",
    "from costly.estimators.llm_api_estimation import LLM_API_Estimation\n",
    "\n",
    "\n",
    "@costly(\n",
    "    input_string=(lambda kwargs: kwargs[\"prompt\"]),\n",
    "    model=(lambda kwargs: kwargs[\"model_name\"]),\n",
    ")\n",
    "def chatgpt2(prompt: str, model_name: str) -> str:\n",
    "    from openai import OpenAI\n",
    "\n",
    "    client = OpenAI()\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name, messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    output_string = response.choices[0].message.content\n",
    "    return output_string\n",
    "\n",
    "\n",
    "cl = Costlog(\n",
    "    totals_keys={\n",
    "        \"cost_min\",\n",
    "        \"cost_max\",\n",
    "        \"time_min\",\n",
    "        \"time_max\",\n",
    "        \"calls\",\n",
    "        \"input_tokens\",\n",
    "        \"output_tokens_min\",\n",
    "        \"output_tokens_max\",\n",
    "    }\n",
    ")\n",
    "chatgpt2(prompt=\"Hello\", model_name=\"gpt-3.5-turbo\", simulate=False, cost_log=cl)\n",
    "cl.items[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually for the specific case of just remapping variables you can simply do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cost_min': 5e-07,\n",
       " 'cost_max': 0.0030725,\n",
       " 'time_min': 0.0,\n",
       " 'time_max': 73.728,\n",
       " 'input_tokens': 1,\n",
       " 'output_tokens_min': 0,\n",
       " 'output_tokens_max': 2048,\n",
       " 'calls': 1,\n",
       " 'model': 'gpt-3.5-turbo',\n",
       " 'simulated': True,\n",
       " 'input_string': 'Hello',\n",
       " 'output_string': None,\n",
       " 'description': None}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from costly import Costlog, costly\n",
    "from costly.simulators.llm_simulator_faker import LLM_Simulator_Faker\n",
    "from costly.estimators.llm_api_estimation import LLM_API_Estimation\n",
    "\n",
    "\n",
    "@costly(input_string=\"prompt\", model=\"model_name\")\n",
    "def chatgpt2(prompt: str, model_name: str) -> str:\n",
    "    from openai import OpenAI\n",
    "\n",
    "    client = OpenAI()\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name, messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    output_string = response.choices[0].message.content\n",
    "    return output_string\n",
    "\n",
    "\n",
    "cl = Costlog(\n",
    "    totals_keys={\n",
    "        \"cost_min\",\n",
    "        \"cost_max\",\n",
    "        \"time_min\",\n",
    "        \"time_max\",\n",
    "        \"calls\",\n",
    "        \"input_tokens\",\n",
    "        \"output_tokens_min\",\n",
    "        \"output_tokens_max\",\n",
    "    }\n",
    ")\n",
    "chatgpt2(prompt=\"Hello\", model_name=\"gpt-3.5-turbo\", simulate=True, cost_log=cl)\n",
    "cl.items[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These hacks will still be supported if you swap `simulator` and `estimator` for something else: just provide a way to calculate whatever parameters your new functions take!\n",
    "\n",
    "Some more examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cost_min': 1.4e-05,\n",
       " 'cost_max': 1.4e-05,\n",
       " 'time_min': 0.5815050000092015,\n",
       " 'time_max': 0.5815050000092015,\n",
       " 'input_tokens': 1,\n",
       " 'output_tokens': 9,\n",
       " 'output_tokens_min': 9,\n",
       " 'output_tokens_max': 9,\n",
       " 'calls': 1,\n",
       " 'model': 'gpt-3.5-turbo',\n",
       " 'simulated': False,\n",
       " 'input_string': 'Hey',\n",
       " 'output_string': 'Hello! How can I assist you today?',\n",
       " 'description': None}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from costly import Costlog, costly\n",
    "from costly.simulators.llm_simulator_faker import LLM_Simulator_Faker\n",
    "from costly.estimators.llm_api_estimation import LLM_API_Estimation\n",
    "\n",
    "\n",
    "@costly(\n",
    "    input_string=lambda kwargs: LLM_API_Estimation.messages_to_input_string(\n",
    "        kwargs[\"messages\"]\n",
    "    ),\n",
    ")\n",
    "def chatgpt_messages(messages: list[dict[str, str]], model: str) -> str:\n",
    "    from openai import OpenAI\n",
    "\n",
    "    client = OpenAI()\n",
    "    response = client.chat.completions.create(model=model, messages=messages)\n",
    "    output_string = response.choices[0].message.content\n",
    "    return output_string\n",
    "\n",
    "\n",
    "cl = Costlog(\n",
    "    totals_keys={\n",
    "        \"cost_min\",\n",
    "        \"cost_max\",\n",
    "        \"time_min\",\n",
    "        \"time_max\",\n",
    "        \"calls\",\n",
    "        \"input_tokens\",\n",
    "        \"output_tokens_min\",\n",
    "        \"output_tokens_max\",\n",
    "    }\n",
    ")\n",
    "chatgpt_messages(\n",
    "    messages=LLM_API_Estimation._input_string_to_messages(\"Hey\"),\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    simulate=False,\n",
    "    cost_log=cl,\n",
    ")\n",
    "cl.items[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-30 03:56:35,564 DEBUG instructor: Patching `client.chat.completions.create` with mode=<Mode.TOOLS: 'tool_call'>\n",
      "2024-08-30 03:56:35,572 DEBUG instructor: Instructor Request: mode.value='tool_call', response_model=<class '__main__.PersonInfo'>, new_kwargs={'messages': [{'content': 'Hey', 'role': 'user'}], 'model': 'gpt-3.5-turbo', 'tools': [{'type': 'function', 'function': {'name': 'PersonInfo', 'description': 'Correctly extracted `PersonInfo` with all the required parameters with correct types', 'parameters': {'properties': {'name': {'title': 'Name', 'type': 'string'}, 'age': {'title': 'Age', 'type': 'integer'}}, 'required': ['age', 'name'], 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'PersonInfo'}}}\n",
      "2024-08-30 03:56:35,574 DEBUG instructor: max_retries: 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'cost_min': 1.7e-05,\n",
       " 'cost_max': 0.003089,\n",
       " 'time_min': 0.0,\n",
       " 'time_max': 73.728,\n",
       " 'input_tokens': 34,\n",
       " 'output_tokens_min': 0,\n",
       " 'output_tokens_max': 2048,\n",
       " 'calls': 1,\n",
       " 'model': 'gpt-3.5-turbo',\n",
       " 'simulated': True,\n",
       " 'input_string': \"HeyPersonInfoCorrectly extracted `PersonInfo` with all the required parameters with correct typesdict_values(['Name', 'string'])dict_values(['Age', 'integer'])\",\n",
       " 'output_string': None,\n",
       " 'description': None}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import instructor\n",
    "from pydantic import BaseModel\n",
    "from openai import OpenAI\n",
    "from instructor import Instructor\n",
    "from costly import costly, Costlog\n",
    "from costly.estimators.llm_api_estimation import LLM_API_Estimation\n",
    "\n",
    "\n",
    "@costly(\n",
    "    input_string=lambda kwargs: LLM_API_Estimation.get_raw_prompt_instructor(**kwargs),\n",
    ")\n",
    "def chatgpt_instructor(\n",
    "    messages: str | list[dict[str, str]],\n",
    "    model: str,\n",
    "    client: Instructor,\n",
    "    response_model: BaseModel,\n",
    ") -> str:\n",
    "    if isinstance(messages, str):\n",
    "        messages = [{\"role\": \"user\", \"content\": messages}]\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        response_model=response_model,\n",
    "    )\n",
    "    return response\n",
    "\n",
    "\n",
    "class PersonInfo(BaseModel):\n",
    "    name: str\n",
    "    age: int\n",
    "\n",
    "\n",
    "cl = Costlog()\n",
    "chatgpt_instructor(\n",
    "    messages=\"Hey\",\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    response_model=PersonInfo,\n",
    "    client=instructor.from_openai(OpenAI()),\n",
    "    simulate=True,\n",
    "    cost_log=cl,\n",
    ")\n",
    "cl.items[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Costlog customizations\n",
    "\n",
    "The default [`costly.Costlog`](costly/costlog.py) class has two modes: `memory` and `jsonl`. The default is `memory`, but for large projects you may want to use `jsonl`: this dumps the cost log into a `.costly` folder in your working directory.\n",
    "\n",
    "The other thing that can be customized is the `totals_keys` parameter, which is a set of keys to aggregate costs by. By default it is `{\"cost_min\", \"cost_max\", \"time_min\", \"time_max\", \"calls\"}`, i.e. it tracks the range of possible costs and running times (`max` and `min` are usually only different when simulating because then you have to estimate). Out-of-the box you can customize it to also track `input_tokens`, `output_tokens_min`, `output_tokens_max`; any other customizations will only make sense if you are using your own estimator.\n",
    "\n",
    "### Simulator\n",
    "\n",
    "[`costly.simulators.llm_simulator_faker`](costly/simulators/llm_simulator_faker.py) has some examples of how to subclass it.\n",
    "\n",
    "One obvious reason to subclass it is to have custom simulating functions for the types you are interested in. Although the default class \"works\" for any Pydantic basemodel etc., you might want to have a custom function -- e.g. if a value needs to be within a certain range, or if its distribution is not uniform, or if you want to use examples from your data, etc.\n",
    "\n",
    "Also, the simulator has a very specific type signature:\n",
    "\n",
    "```python\n",
    "class LLM_Simulator_Faker:\n",
    "\n",
    "    @staticmethod\n",
    "    def simulate_llm_call(\n",
    "        input_string: str,\n",
    "        model: str = None,\n",
    "        response_model: type = str,\n",
    "        cost_log: Costlog = None,\n",
    "        description: list[str] = None,\n",
    "    ) -> str | Any:\n",
    "        ...\n",
    "```\n",
    "\n",
    "So it would make sense to subclass it to just change this function so you don't have to do that ridiculous lambda thing above and can just use `@costly()`.\n",
    "\n",
    "### Estimator\n",
    "\n",
    "Again [`costly.estimators.llm_api_estimation`](costly/estimators/llm_api_estimation.py) has some examples of how to subclass it. The most obvious reason would be to add prices for other models we don't have listed (right now it's just OpenAI and Anthropic). The `PRICES` dict is like this:\n",
    "\n",
    "```python\n",
    "class LLM_API_Estimation:\n",
    "\n",
    "    PRICES = {\n",
    "        \"gpt-4o\": {\n",
    "            \"input_tokens\": 5.0e-6,\n",
    "            \"output_tokens\": 15.0e-6,\n",
    "            \"time\": 18e-3,\n",
    "        },\n",
    "        \"gpt-4o-mini\": {\n",
    "            \"input_tokens\": 0.15e-6,\n",
    "            \"output_tokens\": 0.6e-6,\n",
    "            \"time\": 9e-3,\n",
    "        },\n",
    "        ...\n",
    "    }\n",
    "```\n",
    "\n",
    "Something like this would be quite natural:\n",
    "\n",
    "```python\n",
    "class My_Estimation(LLM_API_Estimation):\n",
    "    PRICES = LLM_API_Estimation.PRICES | {\"my_model\": LLM_API_Estimation.PRICES[\"gpt-4o\"]}\n",
    "```\n",
    "\n",
    "Note that `LLM_API_Estimation` _can_ handle things like `gpt-4o-2024-05-13` etc. because it `LLM_API_Estimation.get_model()` gets the longest prefix matching model name in `PRICES`. \n",
    "\n",
    "### Some assumptions made\n",
    "\n",
    "`LLM_Simulator_Faker`, when producing text, produces text of about `600 * 4.5` characters.\n",
    "\n",
    "Generally we assume that 1 token is about 4.5 characters. Though actual token estimation does use `tiktoken` (unless you subclass `LLM_API_Estimation` and set `tokenize=_tokenize_rough`).\n",
    "\n",
    "Generally we assume, for cost estimation, that output tokens are in the range `[0, 2048]`, and the min and max are computed accordingly. As a rule of thumb for complex projects the true value tends to be about 1/3 the way through, and for projects that receive quite short responses it would be much lower.\n",
    "\n",
    "\n",
    "All of this can be overriden by subclassing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating costs within a function\n",
    "\n",
    "For more accurate estimation of _real_ (not simulated) costs, we might want to calculate costs within the function, as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\n",
      "Important listen apply scene school us morning.\n",
      "Image hold go worry large. Want study enough indeed then against sense. Congress politics become lot close.\n",
      "Worry lose western southern. Type purpose push though nearly.\n",
      "Others north goal article. Amount entire game national inside share game.\n",
      "Certain minute spring support car.\n",
      "Everything while those assume yeah we much. Really wall write population bar nation it.\n",
      "Our customer cold smile born price another. Sport next such. Spend administration wind apply.\n",
      "Husband issue camera serve fight. Three already fact visit sign everything. But entire school plant sit tell hot.\n",
      "Argue seat white card.\n",
      "Past success gas yes. You sound big hour add. Could represent direction policy teach.\n",
      "And career wide whatever. Sister mind ball discuss election open. Bill senior wonder from dream task.\n",
      "Travel hotel class two whom style democratic.\n",
      "Product set drop fact. Strong ten my conference. Middle turn while though.\n",
      "Company hear low clear service possible.\n",
      "Foreign tell message man. Important another eight tax. Up modern give.\n",
      "Practice common board human keep nature upon. Assume respond strategy section great your language. Suggest those laugh others deep read walk.\n",
      "Board space region institution. List your kind on. Both memory card nation physical right international.\n",
      "With assume tough commercial century sound enjoy. Theory view wait red forward mean keep. Ground deal plant rather how old stock. Generation somebody candidate address scientist.\n",
      "Tonight bill next build we also word best. Anyone together although cell land. Way character single challenge page. Close allow far despite structure look raise.\n",
      "Interest quality can rich thousand mean change. Strategy ready street decide seat attorney.\n",
      "During cause opportunity speech building.\n",
      "Artist future body reason from program. Small back put politics surface sense benefit. Where value defense try win tree.\n",
      "Kitchen difficult hit board current any. Single seek matter mission. Model military apply lay agreement against.\n",
      "He physical outside lawyer be treatment other. Decide happy college.\n",
      "Design prove believe hold door radio. There prevent glass second move from born. Indeed detail network will benefit.\n",
      "Compare interesting here who institution alone. Time authority set administration. Special heavy management face million book.\n",
      "Argue feeling table price. Great door beautiful on respond weight. Opportunity letter consumer over here individual. Since most wide light.\n",
      "Enough day laugh significant. Open summer race magazine hear. Choice us give.\n",
      "Fire source cost dinner right magazine despite. Work when force.\n",
      "{'time_min': 4.097308799973689, 'calls': 2.0, 'time_max': 77.82530879997368, 'cost_min': 0.00627, 'cost_max': 0.12915}\n",
      "{'cost_min': 0.0061200000000000004, 'cost_max': 0.0061200000000000004, 'time_min': 4.097308799973689, 'time_max': 4.097308799973689, 'input_tokens': 12, 'output_tokens': 96, 'output_tokens_min': 96, 'output_tokens_max': 96, 'calls': 1, 'model': 'gpt-4', 'simulated': False, 'input_string': 'Write the Lorem ipsum text', 'output_string': 'Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.', 'description': ['chatgpt call']}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'cost_min': 0.00015000000000000001,\n",
       " 'cost_max': 0.12303,\n",
       " 'time_min': 0.0,\n",
       " 'time_max': 73.728,\n",
       " 'input_tokens': 5,\n",
       " 'output_tokens_min': 0,\n",
       " 'output_tokens_max': 2048,\n",
       " 'calls': 1,\n",
       " 'model': 'gpt-4',\n",
       " 'simulated': True,\n",
       " 'input_string': 'Write the Lorem ipsum text',\n",
       " 'output_string': None,\n",
       " 'description': ['chatgpt call']}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from costly import Costlog, costly, CostlyResponse\n",
    "\n",
    "\n",
    "@costly()\n",
    "def chatgpt(input_string: str, model: str) -> str:\n",
    "    from openai import OpenAI\n",
    "\n",
    "    client = OpenAI()\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": input_string},\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    return CostlyResponse(\n",
    "        output=response.choices[0].message.content,\n",
    "        cost_info={\n",
    "            \"input_tokens\": response.usage.prompt_tokens,\n",
    "            \"output_tokens\": response.usage.completion_tokens,\n",
    "        },\n",
    "    )\n",
    "\n",
    "\n",
    "cl = Costlog()\n",
    "x = chatgpt(\n",
    "    input_string=\"Write the Lorem ipsum text\",\n",
    "    model=\"gpt-4\",\n",
    "    cost_log=cl,\n",
    "    simulate=True,\n",
    "    description=[\"chatgpt call\"],\n",
    ")\n",
    "y = chatgpt(\n",
    "    input_string=\"Write the Lorem ipsum text\",\n",
    "    model=\"gpt-4\",\n",
    "    cost_log=cl,\n",
    "    simulate=False,\n",
    "    description=[\"chatgpt call\"],\n",
    ")\n",
    "print(x)\n",
    "print(y)\n",
    "print(cl.totals)\n",
    "print(cl.items[0])\n",
    "cl.items[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-31 13:33:49,019 DEBUG instructor: Patching `client.chat.completions.create` with mode=<Mode.TOOLS: 'tool_call'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-31 13:33:49,026 DEBUG instructor: Instructor Request: mode.value='tool_call', response_model=<class '__main__.PersonInfo'>, new_kwargs={'messages': [{'content': 'Hey', 'role': 'user'}], 'model': 'gpt-3.5-turbo', 'tools': [{'type': 'function', 'function': {'name': 'PersonInfo', 'description': 'Correctly extracted `PersonInfo` with all the required parameters with correct types', 'parameters': {'properties': {'name': {'title': 'Name', 'type': 'string'}, 'age': {'title': 'Age', 'type': 'integer'}}, 'required': ['age', 'name'], 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'PersonInfo'}}}\n",
      "2024-08-31 13:33:49,028 DEBUG instructor: max_retries: 3\n",
      "2024-08-31 13:33:49,060 DEBUG instructor: Patching `client.chat.completions.create` with mode=<Mode.TOOLS: 'tool_call'>\n",
      "2024-08-31 13:33:49,075 DEBUG instructor: Instructor Request: mode.value='tool_call', response_model=<class '__main__.PersonInfo'>, new_kwargs={'messages': [{'content': 'Hey', 'role': 'user'}], 'model': 'gpt-3.5-turbo', 'tools': [{'type': 'function', 'function': {'name': 'PersonInfo', 'description': 'Correctly extracted `PersonInfo` with all the required parameters with correct types', 'parameters': {'properties': {'name': {'title': 'Name', 'type': 'string'}, 'age': {'title': 'Age', 'type': 'integer'}}, 'required': ['age', 'name'], 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'PersonInfo'}}}\n",
      "2024-08-31 13:33:49,075 DEBUG instructor: max_retries: 3\n",
      "2024-08-31 13:33:49,093 DEBUG instructor: Instructor Request: mode.value='tool_call', response_model=<class '__main__.PersonInfo'>, new_kwargs={'messages': [{'role': 'user', 'content': 'Hey'}], 'model': 'gpt-3.5-turbo', 'tools': [{'type': 'function', 'function': {'name': 'PersonInfo', 'description': 'Correctly extracted `PersonInfo` with all the required parameters with correct types', 'parameters': {'properties': {'name': {'title': 'Name', 'type': 'string'}, 'age': {'title': 'Age', 'type': 'integer'}}, 'required': ['age', 'name'], 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'PersonInfo'}}}\n",
      "2024-08-31 13:33:49,093 DEBUG instructor: max_retries: 3\n",
      "2024-08-31 13:33:49,680 DEBUG instructor: Instructor Raw Response: ChatCompletion(id='chatcmpl-A2HaYJBt4a5EguUjKD6N1nhLv6vcQ', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_2u3FjfLthP5bjK2YaEV7kd9V', function=Function(arguments='{\"name\":\"Alice\",\"age\":30}', name='PersonInfo'), type='function')], refusal=None))], created=1725107630, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=9, prompt_tokens=75, total_tokens=84))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cost_min': 1.7e-05, 'cost_max': 0.003089, 'time_min': 0.0, 'time_max': 73.728, 'input_tokens': 34, 'output_tokens_min': 0, 'output_tokens_max': 2048, 'calls': 1, 'model': 'gpt-3.5-turbo', 'simulated': True, 'input_string': \"HeyPersonInfoCorrectly extracted `PersonInfo` with all the required parameters with correct typesdict_values(['Name', 'string'])dict_values(['Age', 'integer'])\", 'output_string': None, 'description': None}\n",
      "{'cost_min': 5.099999999999999e-05, 'cost_max': 5.099999999999999e-05, 'time_min': 0.5973313000285998, 'time_max': 0.5973313000285998, 'input_tokens': 75, 'output_tokens': 9, 'output_tokens_min': 9, 'output_tokens_max': 9, 'calls': 1, 'model': 'gpt-3.5-turbo', 'simulated': False, 'input_string': \"HeyPersonInfoCorrectly extracted `PersonInfo` with all the required parameters with correct typesdict_values(['Name', 'string'])dict_values(['Age', 'integer'])\", 'output_string': PersonInfo(name='Alice', age=30), 'description': None}\n"
     ]
    }
   ],
   "source": [
    "import instructor\n",
    "from pydantic import BaseModel\n",
    "from instructor import Instructor\n",
    "from openai import OpenAI\n",
    "from costly import Costlog, costly, CostlyResponse\n",
    "from costly.estimators.llm_api_estimation import LLM_API_Estimation\n",
    "\n",
    "\n",
    "@costly(\n",
    "    input_string=lambda kwargs: LLM_API_Estimation.get_raw_prompt_instructor(**kwargs),\n",
    ")\n",
    "def chatgpt_instructor(\n",
    "    messages: str | list[dict[str, str]],\n",
    "    model: str,\n",
    "    client: Instructor,\n",
    "    response_model: BaseModel,\n",
    ") -> str:\n",
    "    if isinstance(messages, str):\n",
    "        messages = [{\"role\": \"user\", \"content\": messages}]\n",
    "    response = client.chat.completions.create_with_completion(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        response_model=response_model,\n",
    "    )\n",
    "    output_string, cost_info = response\n",
    "    return CostlyResponse(\n",
    "        output=output_string,\n",
    "        cost_info={\n",
    "            \"input_tokens\": cost_info.usage.prompt_tokens,\n",
    "            \"output_tokens\": cost_info.usage.completion_tokens\n",
    "        }\n",
    "    )\n",
    "    \n",
    "class PersonInfo(BaseModel):\n",
    "    name: str\n",
    "    age: int\n",
    "\n",
    "\n",
    "cl = Costlog()\n",
    "chatgpt_instructor(\n",
    "    messages=\"Hey\",\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    response_model=PersonInfo,\n",
    "    client=instructor.from_openai(OpenAI()),\n",
    "    simulate=True,\n",
    "    cost_log=cl,\n",
    ")\n",
    "chatgpt_instructor(\n",
    "    messages=\"Hey\",\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    response_model=PersonInfo,\n",
    "    client=instructor.from_openai(OpenAI()),\n",
    "    simulate=False,\n",
    "    cost_log=cl,\n",
    ")\n",
    "print(cl.items[0])\n",
    "print(cl.items[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
