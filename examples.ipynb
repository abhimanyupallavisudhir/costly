{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples\n",
    "\n",
    "[**(quick-start)**](#quick-start) Adding a `@costly()` decorator to your function automatically adds the arguments `simulate: bool` and `cost_log: costly.Costlog` to it (and also `description: list[str]` which is useful for tracing the sources of costs in the breakdown).\n",
    "\n",
    "By default, this uses [`LLM_Simulator_Faker.simulate_llm_call()`](costly/simulators/llm_simulator_faker.py) to simulate your function and [`LLM_API_Estimation.get_cost_real()`](costly/estimators/llm_api_estimation.py) as the estimator -- see [**(assumptions)**](#assumptions) for some brief comments on how these work by default. \n",
    "\n",
    "You can also use your own simulator and estimator, as shown in Example [**(customators)**](#customators). In particular you can subclass the existing simulator and estimator objects.\n",
    "\n",
    "The default simulator and estimator assume that your function takes arguments `input_string: str, model: str, response_model: str | BaseModel)`, and supplies these arguments to the simulator and estimator. Regardless of which simulator and estimator you use, if your function does not take the same arguments as them (e.g. if your function adds a system prompt to `input_string`, or you use a different naming system for `model`s from that used in `LLM_API_Estimation.PRICES`), you can pass parameter mappings as shown in Example [**(param-mappings)**](#param-mappings). \n",
    "\n",
    "In particular see the example parameter mappings for [**(messages-and-instructor)**](#messages-and-instructor).\n",
    "\n",
    "For more accurate cost tracking, you might want to calculate costs within the function, e.g. use info returned by the API call itself. For this, your function must response a `CostlyResponse` object as shown in Example [**(costly-response)**](#costly-response). In usage, your function will still return the normal output contained in `CostlyResponse.output`, and everything else will be passed to the simulator and estimator.\n",
    "\n",
    "Finally, see [**(costlog-notes)**](#costlog-notes) for some notes on the `Costlog` class itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## quick-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly! Here is the classic \"Lorem Ipsum\" placeholder text:\n",
      "\n",
      "```\n",
      "Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\n",
      "```\n",
      "\n",
      "Feel free to ask if you need a different variation or more text! \n",
      "---\n",
      " Official trial contain them test first. Sea management run realize education now detail. Billion mission space nothing go lose. Voice together stand other make person fast.\n",
      "Particular list family machine rate contain. Under drug prepare inside. Force human everybody bag but discuss age.\n",
      "Perform rate study top card. Population yes consumer. Section her line our claim.\n",
      "Former account available its rich look wish. Party full girl buy behavior approach.\n",
      "Open language choose style author medical. Mention order continue enter.\n",
      "Get economic relate right eight mother. Could lot year face several right marriage onto. Improve learn nor toward work.\n",
      "Benefit summer skill become. Both big make daughter. Mr turn live tonight apply.\n",
      "Husband parent high nearly share what kitchen include. Soldier idea eat less research over.\n",
      "Car really hundred public admit interest more should. Another story alone watch move term try.\n",
      "Health cost five opportunity when wife. Trade choose plan.\n",
      "Certain law power above treatment still letter. Hundred beat great base attorney political. Up there pass field appear second ago.\n",
      "Because weight other source pressure paper phone dark. She dark exactly cup. It voice debate oil long nothing.\n",
      "Cover news very establish star guess task. Live thousand paper speak reach pattern. Discover argue receive.\n",
      "Across because hospital PM claim. Perhaps set want speech everyone. International available its follow yourself own.\n",
      "Be sign require city third open fly. Easy many both section indicate possible. Health hear law write establish subject theory.\n",
      "Ability including lawyer answer read. Garden outside structure. Computer peace watch girl expect.\n",
      "Force education successful become effort. Rich why purpose probably. Account seat miss center.\n",
      "Could share idea. Scene very stand have send.\n",
      "Stock about explain despite leave necessary guy. Itself item work network. Fact day together leave impact similar parent.\n",
      "Morning management level leader back sort increase yeah. Claim yeah produce sound success large star.\n",
      "Paper discussion recently piece. Election current evening way company likely.\n",
      "Anything not one. Develop good pay trade kind high style. Ever color nation agreement while foot executive.\n",
      "Phone begin history lead improve break conference. Defense through fund on discover. Exactly attention own year.\n",
      "Accept know such total. Personal school despite crime change. Word need onto entire.\n",
      "Method why goal peace country. Ten difficult worry family whom own. Moment could usually safe. White serve fill financial teach run.\n",
      "Benefit race yet measure life require. Goal vote pick enough half learn operation talk. Third determine factor two central. \n",
      "---\n",
      " {'time_min': 1.550557899987325, 'calls': 2.0, 'time_max': 19.982557899987324, 'cost_min': 7.77e-05, 'cost_max': 0.0013065} \n",
      "---\n",
      " {'cost_min': 7.695e-05, 'cost_max': 7.695e-05, 'time_min': 1.550557899987325, 'time_max': 1.550557899987325, 'input_tokens': 5, 'output_tokens': 127, 'output_tokens_min': 127, 'output_tokens_max': 127, 'calls': 1, 'model': 'gpt-4o-mini', 'simulated': False, 'input_string': 'Write the Lorem ipsum text', 'output_string': 'Certainly! Here is the classic \"Lorem Ipsum\" placeholder text:\\n\\n```\\nLorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\\n```\\n\\nFeel free to ask if you need a different variation or more text!', 'description': ['chatgpt call']} \n",
      "---\n",
      " {'cost_min': 7.5e-07, 'cost_max': 0.00122955, 'time_min': 0.0, 'time_max': 18.432, 'input_tokens': 5, 'output_tokens_min': 0, 'output_tokens_max': 2048, 'calls': 1, 'model': 'gpt-4o-mini', 'simulated': True, 'input_string': 'Write the Lorem ipsum text', 'output_string': None, 'description': ['chatgpt call']}\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from costly import Costlog, costly\n",
    "\n",
    "\n",
    "@costly()\n",
    "def chatgpt(input_string: str, model: str) -> str:\n",
    "    from openai import OpenAI\n",
    "\n",
    "    client = OpenAI()\n",
    "    response = client.chat.completions.create(\n",
    "        model=model, messages=[{\"role\": \"user\", \"content\": input_string}]\n",
    "    )\n",
    "    output_string = response.choices[0].message.content\n",
    "    return output_string\n",
    "\n",
    "\n",
    "cl = Costlog()\n",
    "x = chatgpt(\n",
    "    input_string=\"Write the Lorem ipsum text\",\n",
    "    model=\"gpt-4o-mini\",\n",
    "    cost_log=cl,\n",
    "    simulate=False,\n",
    "    description=[\"chatgpt call\"],\n",
    ")\n",
    "y = chatgpt(\n",
    "    input_string=\"Write the Lorem ipsum text\",\n",
    "    model=\"gpt-4o-mini\",\n",
    "    cost_log=cl,\n",
    "    simulate=True,\n",
    "    description=[\"chatgpt call\"],\n",
    ")\n",
    "print(\n",
    "    x,\n",
    "    \"\\n---\\n\",\n",
    "    y,\n",
    "    \"\\n---\\n\",\n",
    "    cl.totals,\n",
    "    \"\\n---\\n\",\n",
    "    cl.items[0],\n",
    "    \"\\n---\\n\",\n",
    "    cl.items[1],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's basically what's happening under the hood, courtesy of `costly.decorator.costly`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from costly.simulators.llm_simulator_faker import LLM_Simulator_Faker\n",
    "from costly.estimators.llm_api_estimation import LLM_API_Estimation\n",
    "\n",
    "def _chatgpt(input_string: str, model: str) -> str:\n",
    "    from openai import OpenAI\n",
    "    client = OpenAI()\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": input_string}]\n",
    "    )\n",
    "    output_string = response.choices[0].message.content\n",
    "    return output_string\n",
    "\n",
    "def chatgpt(input_string: str, model: str, cost_log: Costlog=None, simulate: bool=False, description: list[str]=None) -> str:\n",
    "    if simulate:\n",
    "        return LLM_Simulator_Faker.simulate_llm_call(\n",
    "            input_string=input_string,\n",
    "            model=model,\n",
    "            response_model=str,\n",
    "            cost_log=cost_log,\n",
    "            description=description,\n",
    "        )\n",
    "    if cost_log is not None:\n",
    "        with cost_log.new_item() as (item, timer):\n",
    "            output_string = _chatgpt(input_string, model)\n",
    "            cost_item = LLM_API_Estimation.get_cost_real(\n",
    "                model=model,\n",
    "                input_string=input_string,\n",
    "                output_string=output_string,\n",
    "                description=description,\n",
    "                timer=timer(),\n",
    "            )\n",
    "            item.update(cost_item)\n",
    "    else:\n",
    "        output_string = _chatgpt(input_string, model)\n",
    "    return output_string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## assumptions\n",
    "\n",
    "`LLM_Simulator_Faker`, when producing text, produces text of about `600 * 4.5` characters.\n",
    "\n",
    "Generally we assume that 1 token is about 4.5 characters. Though actual token estimation does use `tiktoken` (unless you subclass `LLM_API_Estimation` and set `tokenize=_tokenize_rough`).\n",
    "\n",
    "Generally we assume, for cost estimation, that output tokens are in the range `[0, 2048]`, and the min and max are computed accordingly. As a rule of thumb for complex projects the true value tends to be about 1/3 the way through, and for projects that receive quite short responses it would be much lower.\n",
    "\n",
    "`LLM_API_Estimation.get_cost_real()` uses the `PRICES` dict to get the cost of a (real of simulated) API call, and to estimate time for a simulated API call. By default the standard OpenAI and Anthropic models are included; you can add more via subclassing.\n",
    "\n",
    "All of this can be overriden by subclassing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## customators\n",
    "\n",
    "The defualt decorator behaviour is\n",
    "\n",
    "```python\n",
    "@costly(\n",
    "    simulator=LLM_Simulator_Faker.simulate_llm_call,\n",
    "    estimator=LLM_API_Estimation.get_cost_real,\n",
    ")\n",
    "```\n",
    "\n",
    "These functions can be replaced by your own custom functions -- the best way to do this is probably to subclass the respective class.\n",
    "\n",
    "[`costly.simulators.llm_simulator_faker`](costly/simulators/llm_simulator_faker.py) has some examples of how to subclass it. One obvious reason to subclass it is to have custom simulating functions for the types you are interested in. Although the default class \"works\" for any Pydantic basemodel etc., you might want to have a custom function -- e.g. if a value needs to be within a certain range, or if its distribution is not uniform, or if you want to use examples from your data, etc.\n",
    "\n",
    "Again [`costly.estimators.llm_api_estimation`](costly/estimators/llm_api_estimation.py) has some examples of how to subclass it. The most obvious reason would be to add prices for other models we don't have listed (right now it's just OpenAI and Anthropic). The `PRICES` dict is like this:\n",
    "\n",
    "```python\n",
    "class LLM_API_Estimation:\n",
    "\n",
    "    PRICES = {\n",
    "        \"gpt-4o\": {\n",
    "            \"input_tokens\": 5.0e-6,\n",
    "            \"output_tokens\": 15.0e-6,\n",
    "            \"time\": 18e-3,\n",
    "        },\n",
    "        \"gpt-4o-mini\": {\n",
    "            \"input_tokens\": 0.15e-6,\n",
    "            \"output_tokens\": 0.6e-6,\n",
    "            \"time\": 9e-3,\n",
    "        },\n",
    "        ...\n",
    "    }\n",
    "```\n",
    "\n",
    "Something like this would be quite natural:\n",
    "\n",
    "```python\n",
    "class My_Estimation(LLM_API_Estimation):\n",
    "    PRICES = LLM_API_Estimation.PRICES | {\"my_model\": LLM_API_Estimation.PRICES[\"gpt-4o\"]}\n",
    "```\n",
    "\n",
    "Note that `LLM_API_Estimation` _can_ handle things like `gpt-4o-2024-05-13` etc. because it `LLM_API_Estimation.get_model()` gets the longest prefix matching model name in `PRICES`. \n",
    "\n",
    "\n",
    "The default simulator and estimator have type signatures:\n",
    "\n",
    "```python\n",
    "class LLM_Simulator_Faker:\n",
    "\n",
    "    @staticmethod\n",
    "    def simulate_llm_call(\n",
    "        input_string: str,\n",
    "        model: str = None,\n",
    "        response_model: type = str,\n",
    "        cost_log: Costlog = None,\n",
    "        description: list[str] = None,\n",
    "    ) -> str | Any:\n",
    "        ...\n",
    "\n",
    "class LLM_API_Estimation:\n",
    "\n",
    "    @staticmethod\n",
    "    def get_cost_real(\n",
    "        model: str,\n",
    "        input_tokens: int = None,\n",
    "        output_tokens_min: int = None,\n",
    "        output_tokens_max: int = None,\n",
    "        input_string: str = None,\n",
    "        output_string: str = None,\n",
    "        timer: float = None,\n",
    "        **kwargs,\n",
    "    ) -> dict[str, float]:\n",
    "        ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## param-mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cost_min': 5e-07, 'cost_max': 0.0030725, 'time_min': 0.0, 'time_max': 73.728, 'input_tokens': 1, 'output_tokens_min': 0, 'output_tokens_max': 2048, 'calls': 1, 'model': 'gpt-3.5-turbo', 'simulated': True, 'input_string': 'Hello', 'output_string': None, 'description': None} \n",
      "---\n",
      " {'cost_min': 1.4e-05, 'cost_max': 1.4e-05, 'time_min': 0.9808458000188693, 'time_max': 0.9808458000188693, 'input_tokens': 1, 'output_tokens': 9, 'output_tokens_min': 9, 'output_tokens_max': 9, 'calls': 1, 'model': 'gpt-3.5-turbo', 'simulated': False, 'input_string': 'Hello', 'output_string': 'Hello! How can I help you today?', 'description': None}\n"
     ]
    }
   ],
   "source": [
    "from costly import Costlog, costly\n",
    "from costly.simulators.llm_simulator_faker import LLM_Simulator_Faker\n",
    "from costly.estimators.llm_api_estimation import LLM_API_Estimation\n",
    "\n",
    "\n",
    "@costly(\n",
    "    input_string=(lambda kwargs: kwargs[\"prompt\"]),\n",
    "    model=(lambda kwargs: kwargs[\"model_name\"]),\n",
    ")\n",
    "def chatgpt2(prompt: str, model_name: str) -> str:\n",
    "    from openai import OpenAI\n",
    "\n",
    "    client = OpenAI()\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name, messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    output_string = response.choices[0].message.content\n",
    "    return output_string\n",
    "\n",
    "\n",
    "cl = Costlog()\n",
    "chatgpt2(prompt=\"Hello\", model_name=\"gpt-3.5-turbo\", simulate=True, cost_log=cl)\n",
    "chatgpt2(prompt=\"Hello\", model_name=\"gpt-3.5-turbo\", simulate=False, cost_log=cl)\n",
    "print(cl.items[0],'\\n---\\n',cl.items[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually for the specific case of just remapping variables you can simply do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cost_min': 5e-07, 'cost_max': 0.0030725, 'time_min': 0.0, 'time_max': 73.728, 'input_tokens': 1, 'output_tokens_min': 0, 'output_tokens_max': 2048, 'calls': 1, 'model': 'gpt-3.5-turbo', 'simulated': True, 'input_string': 'Hello', 'output_string': None, 'description': None} \n",
      "---\n",
      " {'cost_min': 1.4e-05, 'cost_max': 1.4e-05, 'time_min': 0.6241316000232473, 'time_max': 0.6241316000232473, 'input_tokens': 1, 'output_tokens': 9, 'output_tokens_min': 9, 'output_tokens_max': 9, 'calls': 1, 'model': 'gpt-3.5-turbo', 'simulated': False, 'input_string': 'Hello', 'output_string': 'Hello! How can I assist you today?', 'description': None}\n"
     ]
    }
   ],
   "source": [
    "from costly import Costlog, costly\n",
    "from costly.simulators.llm_simulator_faker import LLM_Simulator_Faker\n",
    "from costly.estimators.llm_api_estimation import LLM_API_Estimation\n",
    "\n",
    "\n",
    "@costly(input_string=\"prompt\", model=\"model_name\")\n",
    "def chatgpt2(prompt: str, model_name: str) -> str:\n",
    "    from openai import OpenAI\n",
    "\n",
    "    client = OpenAI()\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name, messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    output_string = response.choices[0].message.content\n",
    "    return output_string\n",
    "\n",
    "\n",
    "cl = Costlog(\n",
    "    totals_keys={\n",
    "        \"cost_min\",\n",
    "        \"cost_max\",\n",
    "        \"time_min\",\n",
    "        \"time_max\",\n",
    "        \"calls\",\n",
    "        \"input_tokens\",\n",
    "        \"output_tokens_min\",\n",
    "        \"output_tokens_max\",\n",
    "    }\n",
    ")\n",
    "chatgpt2(prompt=\"Hello\", model_name=\"gpt-3.5-turbo\", simulate=True, cost_log=cl)\n",
    "chatgpt2(prompt=\"Hello\", model_name=\"gpt-3.5-turbo\", simulate=False, cost_log=cl)\n",
    "print(cl.items[0],'\\n---\\n',cl.items[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## messages-and-instructor\n",
    "\n",
    "More examples of parameter mappings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cost_min': 1.5e-07, 'cost_max': 0.00122895, 'time_min': 0.0, 'time_max': 18.432, 'input_tokens': 1, 'output_tokens_min': 0, 'output_tokens_max': 2048, 'calls': 1, 'model': 'gpt-4o-mini', 'simulated': True, 'input_string': 'Hey', 'output_string': None, 'description': None} \n",
      "---\n",
      " {'cost_min': 5.55e-06, 'cost_max': 5.55e-06, 'time_min': 0.7071941000176594, 'time_max': 0.7071941000176594, 'input_tokens': 1, 'output_tokens': 9, 'output_tokens_min': 9, 'output_tokens_max': 9, 'calls': 1, 'model': 'gpt-4o-mini', 'simulated': False, 'input_string': 'Hey', 'output_string': 'Hello! How can I assist you today?', 'description': None}\n"
     ]
    }
   ],
   "source": [
    "from costly import Costlog, costly\n",
    "from costly.simulators.llm_simulator_faker import LLM_Simulator_Faker\n",
    "from costly.estimators.llm_api_estimation import LLM_API_Estimation\n",
    "\n",
    "\n",
    "@costly(\n",
    "    input_string=lambda kwargs: LLM_API_Estimation.messages_to_input_string(\n",
    "        kwargs[\"messages\"]\n",
    "    ),\n",
    ")\n",
    "def chatgpt_messages(messages: list[dict[str, str]], model: str) -> str:\n",
    "    from openai import OpenAI\n",
    "\n",
    "    client = OpenAI()\n",
    "    response = client.chat.completions.create(model=model, messages=messages)\n",
    "    output_string = response.choices[0].message.content\n",
    "    return output_string\n",
    "\n",
    "\n",
    "cl = Costlog()\n",
    "chatgpt_messages(\n",
    "    messages=LLM_API_Estimation._input_string_to_messages(\"Hey\"),\n",
    "    model=\"gpt-4o-mini\",\n",
    "    simulate=True,\n",
    "    cost_log=cl,\n",
    ")\n",
    "chatgpt_messages(\n",
    "    messages=LLM_API_Estimation._input_string_to_messages(\"Hey\"),\n",
    "    model=\"gpt-4o-mini\",\n",
    "    simulate=False,\n",
    "    cost_log=cl,\n",
    ")\n",
    "print(cl.items[0], '\\n---\\n', cl.items[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-31 21:23:19,515 DEBUG instructor: Patching `client.chat.completions.create` with mode=<Mode.TOOLS: 'tool_call'>\n",
      "2024-08-31 21:23:19,583 DEBUG instructor: Instructor Request: mode.value='tool_call', response_model=<class '__main__.PersonInfo'>, new_kwargs={'messages': [{'content': 'Hey', 'role': 'user'}], 'model': 'gpt-3.5-turbo', 'tools': [{'type': 'function', 'function': {'name': 'PersonInfo', 'description': 'Correctly extracted `PersonInfo` with all the required parameters with correct types', 'parameters': {'properties': {'name': {'title': 'Name', 'type': 'string'}, 'age': {'title': 'Age', 'type': 'integer'}}, 'required': ['age', 'name'], 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'PersonInfo'}}}\n",
      "2024-08-31 21:23:19,590 DEBUG instructor: max_retries: 3\n",
      "2024-08-31 21:23:19,667 DEBUG instructor: Patching `client.chat.completions.create` with mode=<Mode.TOOLS: 'tool_call'>\n",
      "2024-08-31 21:23:19,683 DEBUG instructor: Instructor Request: mode.value='tool_call', response_model=<class '__main__.PersonInfo'>, new_kwargs={'messages': [{'content': 'Hey', 'role': 'user'}], 'model': 'gpt-3.5-turbo', 'tools': [{'type': 'function', 'function': {'name': 'PersonInfo', 'description': 'Correctly extracted `PersonInfo` with all the required parameters with correct types', 'parameters': {'properties': {'name': {'title': 'Name', 'type': 'string'}, 'age': {'title': 'Age', 'type': 'integer'}}, 'required': ['age', 'name'], 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'PersonInfo'}}}\n",
      "2024-08-31 21:23:19,685 DEBUG instructor: max_retries: 3\n",
      "2024-08-31 21:23:19,719 DEBUG instructor: Instructor Request: mode.value='tool_call', response_model=<class '__main__.PersonInfo'>, new_kwargs={'messages': [{'role': 'user', 'content': 'Hey'}], 'model': 'gpt-3.5-turbo', 'tools': [{'type': 'function', 'function': {'name': 'PersonInfo', 'description': 'Correctly extracted `PersonInfo` with all the required parameters with correct types', 'parameters': {'properties': {'name': {'title': 'Name', 'type': 'string'}, 'age': {'title': 'Age', 'type': 'integer'}}, 'required': ['age', 'name'], 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'PersonInfo'}}}\n",
      "2024-08-31 21:23:19,719 DEBUG instructor: max_retries: 3\n",
      "2024-08-31 21:23:20,317 DEBUG instructor: Instructor Raw Response: ChatCompletion(id='chatcmpl-A2OuunLU9dVZYjgAIbzP2cyqV1IHh', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_HnqrqzG4ORHCcfx8pOpU2srD', function=Function(arguments='{\"name\":\"Alice\",\"age\":30}', name='PersonInfo'), type='function')], refusal=None))], created=1725135800, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=9, prompt_tokens=75, total_tokens=84))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cost_min': 1.7e-05, 'cost_max': 0.003089, 'time_min': 0.0, 'time_max': 73.728, 'input_tokens': 34, 'output_tokens_min': 0, 'output_tokens_max': 2048, 'calls': 1, 'model': 'gpt-3.5-turbo', 'simulated': True, 'input_string': \"HeyPersonInfoCorrectly extracted `PersonInfo` with all the required parameters with correct typesdict_values(['Name', 'string'])dict_values(['Age', 'integer'])\", 'output_string': None, 'description': None} \n",
      "---\n",
      " {'cost_min': 1.7e-05, 'cost_max': 1.7e-05, 'time_min': 0.6167464000172913, 'time_max': 0.6167464000172913, 'input_tokens': 34, 'output_tokens': 0, 'output_tokens_min': 0, 'output_tokens_max': 0, 'calls': 1, 'model': 'gpt-3.5-turbo', 'simulated': False, 'input_string': \"HeyPersonInfoCorrectly extracted `PersonInfo` with all the required parameters with correct typesdict_values(['Name', 'string'])dict_values(['Age', 'integer'])\", 'output_string': PersonInfo(name='Alice', age=30), 'description': None}\n"
     ]
    }
   ],
   "source": [
    "import instructor\n",
    "from pydantic import BaseModel\n",
    "from openai import OpenAI\n",
    "from instructor import Instructor\n",
    "from costly import costly, Costlog\n",
    "from costly.estimators.llm_api_estimation import LLM_API_Estimation\n",
    "\n",
    "\n",
    "@costly(\n",
    "    input_string=lambda kwargs: LLM_API_Estimation.get_raw_prompt_instructor(**kwargs),\n",
    ")\n",
    "def chatgpt_instructor(\n",
    "    messages: str | list[dict[str, str]],\n",
    "    model: str,\n",
    "    client: Instructor,\n",
    "    response_model: BaseModel,\n",
    ") -> str:\n",
    "    if isinstance(messages, str):\n",
    "        messages = [{\"role\": \"user\", \"content\": messages}]\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        response_model=response_model,\n",
    "    )\n",
    "    return response\n",
    "\n",
    "\n",
    "class PersonInfo(BaseModel):\n",
    "    name: str\n",
    "    age: int\n",
    "\n",
    "\n",
    "cl = Costlog()\n",
    "chatgpt_instructor(\n",
    "    messages=\"Hey\",\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    response_model=PersonInfo,\n",
    "    client=instructor.from_openai(OpenAI()),\n",
    "    simulate=True,\n",
    "    cost_log=cl,\n",
    ")\n",
    "chatgpt_instructor(\n",
    "    messages=\"Hey\",\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    response_model=PersonInfo,\n",
    "    client=instructor.from_openai(OpenAI()),\n",
    "    simulate=False,\n",
    "    cost_log=cl,\n",
    ")\n",
    "print(cl.items[0], '\\n---\\n', cl.items[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## costly-response\n",
    "\n",
    "For more accurate estimation of _real_ (not simulated) costs, we might want to calculate costs within the function.\n",
    "\n",
    "To do this, your function must return a `CostlyResponse` object.\n",
    "\n",
    "```python\n",
    "@dataclass\n",
    "class CostlyResponse:\n",
    "    output: Any\n",
    "    cost_info: dict[str, Any]\n",
    "```\n",
    "\n",
    " In actual usage, your function will only return the `output` field -- the `@costly` decorator will take care of this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Little break party executive probably. Article act address director author. Professor into moment couple recognize.\n",
      "Level same international describe finally avoid. Receive sense various learn lot second animal. Ready check local body blood country.\n",
      "Success toward only.\n",
      "Population top career professor next avoid between. Pull I he big into. Today by direction.\n",
      "Own morning its writer big.\n",
      "Really her item year table yourself must. Although argue coach design bad. Still section bed finally as minute often.\n",
      "Concern manager either throughout public sister including. Guy cup save trial film yeah natural focus.\n",
      "Yourself act foreign outside dinner.\n",
      "Member decade nation if behind. Quickly give TV be give. Measure against begin understand case.\n",
      "Different us positive why look. Us start culture cover especially. Key yard south pretty marriage tend. Others certainly however control.\n",
      "International always produce yet face dream pay.\n",
      "Media bring book theory pick color.\n",
      "Yourself the fact continue feel into short. Argue fight by upon.\n",
      "Myself old start tend smile. Common allow budget industry yet may. Identify those beat south person free.\n",
      "Why big fire American million create type. Happen authority clearly sound already student offer beautiful. Rest about seat experience none loss agent gun.\n",
      "Most enjoy rather employee international because hour. Performance church language focus report memory save.\n",
      "Field art memory sing TV wind market. Popular yourself since cost new suddenly. Pretty use marriage watch expect discover early memory.\n",
      "Threat report about raise other environment owner. Catch type hundred recent buy. Use must lay benefit year prove candidate energy.\n",
      "Indicate school agent teacher late huge head. Weight financial happy mind father baby.\n",
      "Ago teacher parent back customer guy. Mean option opportunity upon.\n",
      "Situation power table tend either security. Movie near run interview. Down everybody Mrs nation.\n",
      "Explain try simple chance subject three. Let man they drive garden guess. Cup military be white poor some measure.\n",
      "However huge ago religious because future. Option against maybe research. Space protect hit eat any difficult.\n",
      "Understand place through national. Like degree receive mention.\n",
      "And now sometimes meeting. Yes card they although. Letter idea forget appear require.\n",
      "Hard argue tell citizen seat. You spring unit like.\n",
      "Nothing gas civil medical style finally. Court authority ask chance.\n",
      "Catch mission public. Step focus again agent.\n",
      "Article past long. Majority four speech great study hard present. Product company almost.\n",
      "Word may employee task. Source hospital situation note summer ready while. Rule itself their discover worry street. \n",
      "---\n",
      " Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum. \n",
      "---\n",
      " {'cost_min': 0.00015000000000000001, 'cost_max': 0.12303, 'time_min': 0.0, 'time_max': 73.728, 'input_tokens': 5, 'output_tokens_min': 0, 'output_tokens_max': 2048, 'calls': 1, 'model': 'gpt-4', 'simulated': True, 'input_string': 'Write the Lorem ipsum text', 'output_string': None, 'description': None} \n",
      "---\n",
      " {'cost_min': 0.0061200000000000004, 'cost_max': 0.0061200000000000004, 'time_min': 4.206619099946693, 'time_max': 4.206619099946693, 'input_tokens': 12, 'output_tokens': 96, 'output_tokens_min': 96, 'output_tokens_max': 96, 'calls': 1, 'model': 'gpt-4', 'simulated': False, 'input_string': 'Write the Lorem ipsum text', 'output_string': 'Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.', 'description': None}\n"
     ]
    }
   ],
   "source": [
    "from costly import Costlog, costly, CostlyResponse\n",
    "\n",
    "\n",
    "@costly()\n",
    "def chatgpt(input_string: str, model: str) -> str:\n",
    "    from openai import OpenAI\n",
    "\n",
    "    client = OpenAI()\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": input_string},\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    return CostlyResponse(\n",
    "        output=response.choices[0].message.content,\n",
    "        cost_info={\n",
    "            \"input_tokens\": response.usage.prompt_tokens,\n",
    "            \"output_tokens\": response.usage.completion_tokens,\n",
    "        },\n",
    "    )\n",
    "\n",
    "\n",
    "cl = Costlog()\n",
    "x = chatgpt(\n",
    "    input_string=\"Write the Lorem ipsum text\",\n",
    "    model=\"gpt-4\",\n",
    "    cost_log=cl,\n",
    "    simulate=True,\n",
    ")\n",
    "y = chatgpt(\n",
    "    input_string=\"Write the Lorem ipsum text\",\n",
    "    model=\"gpt-4\",\n",
    "    cost_log=cl,\n",
    "    simulate=False,\n",
    ")\n",
    "print(x, \"\\n---\\n\", y, \"\\n---\\n\", cl.items[0], \"\\n---\\n\", cl.items[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-31 21:31:28,447 DEBUG instructor: Patching `client.chat.completions.create` with mode=<Mode.TOOLS: 'tool_call'>\n",
      "2024-08-31 21:31:28,463 DEBUG instructor: Instructor Request: mode.value='tool_call', response_model=<class '__main__.PersonInfo'>, new_kwargs={'messages': [{'content': 'Hey', 'role': 'user'}], 'model': 'gpt-4', 'tools': [{'type': 'function', 'function': {'name': 'PersonInfo', 'description': 'Correctly extracted `PersonInfo` with all the required parameters with correct types', 'parameters': {'properties': {'name': {'title': 'Name', 'type': 'string'}, 'age': {'title': 'Age', 'type': 'integer'}}, 'required': ['age', 'name'], 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'PersonInfo'}}}\n",
      "2024-08-31 21:31:28,463 DEBUG instructor: max_retries: 3\n",
      "2024-08-31 21:31:28,497 DEBUG instructor: Patching `client.chat.completions.create` with mode=<Mode.TOOLS: 'tool_call'>\n",
      "2024-08-31 21:31:28,516 DEBUG instructor: Instructor Request: mode.value='tool_call', response_model=<class '__main__.PersonInfo'>, new_kwargs={'messages': [{'content': 'Hey', 'role': 'user'}], 'model': 'gpt-4', 'tools': [{'type': 'function', 'function': {'name': 'PersonInfo', 'description': 'Correctly extracted `PersonInfo` with all the required parameters with correct types', 'parameters': {'properties': {'name': {'title': 'Name', 'type': 'string'}, 'age': {'title': 'Age', 'type': 'integer'}}, 'required': ['age', 'name'], 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'PersonInfo'}}}\n",
      "2024-08-31 21:31:28,516 DEBUG instructor: max_retries: 3\n",
      "2024-08-31 21:31:28,532 DEBUG instructor: Instructor Request: mode.value='tool_call', response_model=<class '__main__.PersonInfo'>, new_kwargs={'messages': [{'role': 'user', 'content': 'Hey'}], 'model': 'gpt-4', 'tools': [{'type': 'function', 'function': {'name': 'PersonInfo', 'description': 'Correctly extracted `PersonInfo` with all the required parameters with correct types', 'parameters': {'properties': {'name': {'title': 'Name', 'type': 'string'}, 'age': {'title': 'Age', 'type': 'integer'}}, 'required': ['age', 'name'], 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'PersonInfo'}}}\n",
      "2024-08-31 21:31:28,539 DEBUG instructor: max_retries: 3\n",
      "2024-08-31 21:31:29,435 DEBUG instructor: Instructor Raw Response: ChatCompletion(id='chatcmpl-A2P2n9alDuJTvrQQn9ZETGxQvJWRj', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_cQ9Zr4ZzZ0GkpzMxpYKBwFpM', function=Function(arguments='{ \"name\": \"John\", \"age\": 24 }', name='PersonInfo'), type='function')], refusal=None))], created=1725136289, model='gpt-4-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=13, prompt_tokens=66, total_tokens=79))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Little break party executive probably. Article act address director author. Professor into moment couple recognize.\n",
      "Level same international describe finally avoid. Receive sense various learn lot second animal. Ready check local body blood country.\n",
      "Success toward only.\n",
      "Population top career professor next avoid between. Pull I he big into. Today by direction.\n",
      "Own morning its writer big.\n",
      "Really her item year table yourself must. Although argue coach design bad. Still section bed finally as minute often.\n",
      "Concern manager either throughout public sister including. Guy cup save trial film yeah natural focus.\n",
      "Yourself act foreign outside dinner.\n",
      "Member decade nation if behind. Quickly give TV be give. Measure against begin understand case.\n",
      "Different us positive why look. Us start culture cover especially. Key yard south pretty marriage tend. Others certainly however control.\n",
      "International always produce yet face dream pay.\n",
      "Media bring book theory pick color.\n",
      "Yourself the fact continue feel into short. Argue fight by upon.\n",
      "Myself old start tend smile. Common allow budget industry yet may. Identify those beat south person free.\n",
      "Why big fire American million create type. Happen authority clearly sound already student offer beautiful. Rest about seat experience none loss agent gun.\n",
      "Most enjoy rather employee international because hour. Performance church language focus report memory save.\n",
      "Field art memory sing TV wind market. Popular yourself since cost new suddenly. Pretty use marriage watch expect discover early memory.\n",
      "Threat report about raise other environment owner. Catch type hundred recent buy. Use must lay benefit year prove candidate energy.\n",
      "Indicate school agent teacher late huge head. Weight financial happy mind father baby.\n",
      "Ago teacher parent back customer guy. Mean option opportunity upon.\n",
      "Situation power table tend either security. Movie near run interview. Down everybody Mrs nation.\n",
      "Explain try simple chance subject three. Let man they drive garden guess. Cup military be white poor some measure.\n",
      "However huge ago religious because future. Option against maybe research. Space protect hit eat any difficult.\n",
      "Understand place through national. Like degree receive mention.\n",
      "And now sometimes meeting. Yes card they although. Letter idea forget appear require.\n",
      "Hard argue tell citizen seat. You spring unit like.\n",
      "Nothing gas civil medical style finally. Court authority ask chance.\n",
      "Catch mission public. Step focus again agent.\n",
      "Article past long. Majority four speech great study hard present. Product company almost.\n",
      "Word may employee task. Source hospital situation note summer ready while. Rule itself their discover worry street. \n",
      "---\n",
      " Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum. \n",
      "---\n",
      " {'cost_min': 0.00102, 'cost_max': 0.12390000000000001, 'time_min': 0.0, 'time_max': 73.728, 'input_tokens': 34, 'output_tokens_min': 0, 'output_tokens_max': 2048, 'calls': 1, 'model': 'gpt-4', 'simulated': True, 'input_string': \"HeyPersonInfoCorrectly extracted `PersonInfo` with all the required parameters with correct typesdict_values(['Name', 'string'])dict_values(['Age', 'integer'])\", 'output_string': None, 'description': None} \n",
      "---\n",
      " {'cost_min': 0.00276, 'cost_max': 0.00276, 'time_min': 0.905726799974218, 'time_max': 0.905726799974218, 'input_tokens': 66, 'output_tokens': 13, 'output_tokens_min': 13, 'output_tokens_max': 13, 'calls': 1, 'model': 'gpt-4', 'simulated': False, 'input_string': \"HeyPersonInfoCorrectly extracted `PersonInfo` with all the required parameters with correct typesdict_values(['Name', 'string'])dict_values(['Age', 'integer'])\", 'output_string': PersonInfo(name='John', age=24), 'description': None}\n"
     ]
    }
   ],
   "source": [
    "import instructor\n",
    "from pydantic import BaseModel\n",
    "from instructor import Instructor\n",
    "from openai import OpenAI\n",
    "from costly import Costlog, costly, CostlyResponse\n",
    "from costly.estimators.llm_api_estimation import LLM_API_Estimation\n",
    "\n",
    "\n",
    "@costly(\n",
    "    input_string=lambda kwargs: LLM_API_Estimation.get_raw_prompt_instructor(**kwargs),\n",
    ")\n",
    "def chatgpt_instructor(\n",
    "    messages: str | list[dict[str, str]],\n",
    "    model: str,\n",
    "    client: Instructor,\n",
    "    response_model: BaseModel,\n",
    ") -> str:\n",
    "    if isinstance(messages, str):\n",
    "        messages = [{\"role\": \"user\", \"content\": messages}]\n",
    "    response = client.chat.completions.create_with_completion(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        response_model=response_model,\n",
    "    )\n",
    "    output_string, cost_info = response\n",
    "    return CostlyResponse(\n",
    "        output=output_string,\n",
    "        cost_info={\n",
    "            \"input_tokens\": cost_info.usage.prompt_tokens,\n",
    "            \"output_tokens\": cost_info.usage.completion_tokens\n",
    "        }\n",
    "    )\n",
    "    \n",
    "class PersonInfo(BaseModel):\n",
    "    name: str\n",
    "    age: int\n",
    "\n",
    "\n",
    "cl = Costlog()\n",
    "chatgpt_instructor(\n",
    "    messages=\"Hey\",\n",
    "    model=\"gpt-4\",\n",
    "    response_model=PersonInfo,\n",
    "    client=instructor.from_openai(OpenAI()),\n",
    "    simulate=True,\n",
    "    cost_log=cl,\n",
    ")\n",
    "chatgpt_instructor(\n",
    "    messages=\"Hey\",\n",
    "    model=\"gpt-4\",\n",
    "    response_model=PersonInfo,\n",
    "    client=instructor.from_openai(OpenAI()),\n",
    "    simulate=False,\n",
    "    cost_log=cl,\n",
    ")\n",
    "print(x, '\\n---\\n', y, '\\n---\\n', cl.items[0], '\\n---\\n', cl.items[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## costlog-notes\n",
    "\n",
    "The default [`costly.Costlog`](costly/costlog.py) class has two modes: `memory` and `jsonl`. The default is `memory`, but for large projects you may want to use `jsonl`: this dumps the cost log into a `.costly` folder in your working directory.\n",
    "\n",
    "The other thing that can be customized is the `totals_keys` parameter, which is a set of keys to aggregate costs by. By default it is `{\"cost_min\", \"cost_max\", \"time_min\", \"time_max\", \"calls\"}`, i.e. it tracks the range of possible costs and running times (`max` and `min` are usually only different when simulating because then you have to estimate). Out-of-the box you can customize it to also track `input_tokens`, `output_tokens_min`, `output_tokens_max`; any other customizations will only make sense if you are using your own estimator.\n",
    "\n",
    "You will want to change `totals_keys` if you want to use this package for things other than LLM costs, or if you want to track something other than `min` and `max`, e.g. some estimate of the average, or percentiles or whatever."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
